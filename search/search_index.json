{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"tfbpmodeling \u00b6 A Python package for Transcription Factor Binding and Perturbation (TFBP) modeling that analyzes relationships between transcription factor binding and gene expression perturbations using machine learning techniques. What is tfbpmodeling? \u00b6 tfbpmodeling provides a comprehensive workflow for modeling the relationship between transcription factor binding data and gene expression perturbation data. The package uses bootstrap resampling and regularized regression (LassoCV) to identify significant binding-perturbation relationships while controlling for confounding factors. Key Features \u00b6 Bootstrap Resampling : Robust statistical inference through bootstrap confidence intervals Regularized Regression : LassoCV for feature selection and model fitting Multi-stage Workflow : Sequential modeling with increasing specificity Interaction Analysis : Statistical evaluation of interaction terms vs main effects Flexible Input : Support for various data formats and feature engineering options Comprehensive Output : Detailed results with confidence intervals and diagnostics Workflow Overview \u00b6 The package implements a 4-stage sequential workflow: graph TD A[Input Data] --> B[Stage 1: All Data Modeling] B --> C[Stage 2: Top-N Modeling] C --> D[Stage 3: Interactor Significance] D --> E[Stage 4: Final Results] B --> F[Bootstrap Resampling<br/>LassoCV Fitting<br/>Confidence Intervals] C --> G[Significant Predictors<br/>Top-N Data Subset<br/>Refined Modeling] D --> H[Interaction vs Main Effects<br/>Statistical Significance<br/>Final Selection] All Data Modeling : Bootstrap resampling with LassoCV on complete dataset Top-N Modeling : Secondary modeling on most significant predictors from top-performing data Interactor Significance : Evaluation of interaction terms against main effects Results Generation : Comprehensive output with confidence intervals and statistics Quick Start \u00b6 Installation \u00b6 python -m pip install git+https://github.com/BrentLab/tfbpmodeling.git Basic Usage \u00b6 python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file expression_data.csv \\ --predictors_file binding_data.csv \\ --perturbed_tf YourTF Documentation Structure \u00b6 Getting Started : Installation and setup instructions CLI Reference : Complete command-line interface documentation Tutorials : Step-by-step guides and examples API Reference : Detailed API documentation for all modules Development : Contributing guidelines and development setup Use Cases \u00b6 tfbpmodeling is designed for researchers working with: Transcription Factor Studies : Analyzing TF binding patterns and their effects on gene expression Gene Expression Analysis : Understanding perturbation effects in transcriptional networks Regulatory Network Modeling : Building predictive models of transcriptional regulation Functional Genomics : Integrating binding and expression data for biological insights Citation \u00b6 If you use tfbpmodeling in your research, please cite: @software { tfbpmodeling , title = {tfbpmodeling: Transcription Factor Binding and Perturbation Modeling} , author = {Mateusiak, Chase and Eric Jia and Erdenebaatar, Zolboo and Mueller, Ben and Liu, Chenxing and Brent, Michael} , url = {https://github.com/BrentLab/tfbpmodeling} , year = {2024} } Support \u00b6 Issues : Report bugs and request features on GitHub Issues Discussions : Ask questions on GitHub Discussions Documentation : Browse the complete documentation at https://brentlab.github.io/tfbpmodeling/","title":"Home"},{"location":"#tfbpmodeling","text":"A Python package for Transcription Factor Binding and Perturbation (TFBP) modeling that analyzes relationships between transcription factor binding and gene expression perturbations using machine learning techniques.","title":"tfbpmodeling"},{"location":"#what-is-tfbpmodeling","text":"tfbpmodeling provides a comprehensive workflow for modeling the relationship between transcription factor binding data and gene expression perturbation data. The package uses bootstrap resampling and regularized regression (LassoCV) to identify significant binding-perturbation relationships while controlling for confounding factors.","title":"What is tfbpmodeling?"},{"location":"#key-features","text":"Bootstrap Resampling : Robust statistical inference through bootstrap confidence intervals Regularized Regression : LassoCV for feature selection and model fitting Multi-stage Workflow : Sequential modeling with increasing specificity Interaction Analysis : Statistical evaluation of interaction terms vs main effects Flexible Input : Support for various data formats and feature engineering options Comprehensive Output : Detailed results with confidence intervals and diagnostics","title":"Key Features"},{"location":"#workflow-overview","text":"The package implements a 4-stage sequential workflow: graph TD A[Input Data] --> B[Stage 1: All Data Modeling] B --> C[Stage 2: Top-N Modeling] C --> D[Stage 3: Interactor Significance] D --> E[Stage 4: Final Results] B --> F[Bootstrap Resampling<br/>LassoCV Fitting<br/>Confidence Intervals] C --> G[Significant Predictors<br/>Top-N Data Subset<br/>Refined Modeling] D --> H[Interaction vs Main Effects<br/>Statistical Significance<br/>Final Selection] All Data Modeling : Bootstrap resampling with LassoCV on complete dataset Top-N Modeling : Secondary modeling on most significant predictors from top-performing data Interactor Significance : Evaluation of interaction terms against main effects Results Generation : Comprehensive output with confidence intervals and statistics","title":"Workflow Overview"},{"location":"#quick-start","text":"","title":"Quick Start"},{"location":"#installation","text":"python -m pip install git+https://github.com/BrentLab/tfbpmodeling.git","title":"Installation"},{"location":"#basic-usage","text":"python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file expression_data.csv \\ --predictors_file binding_data.csv \\ --perturbed_tf YourTF","title":"Basic Usage"},{"location":"#documentation-structure","text":"Getting Started : Installation and setup instructions CLI Reference : Complete command-line interface documentation Tutorials : Step-by-step guides and examples API Reference : Detailed API documentation for all modules Development : Contributing guidelines and development setup","title":"Documentation Structure"},{"location":"#use-cases","text":"tfbpmodeling is designed for researchers working with: Transcription Factor Studies : Analyzing TF binding patterns and their effects on gene expression Gene Expression Analysis : Understanding perturbation effects in transcriptional networks Regulatory Network Modeling : Building predictive models of transcriptional regulation Functional Genomics : Integrating binding and expression data for biological insights","title":"Use Cases"},{"location":"#citation","text":"If you use tfbpmodeling in your research, please cite: @software { tfbpmodeling , title = {tfbpmodeling: Transcription Factor Binding and Perturbation Modeling} , author = {Mateusiak, Chase and Eric Jia and Erdenebaatar, Zolboo and Mueller, Ben and Liu, Chenxing and Brent, Michael} , url = {https://github.com/BrentLab/tfbpmodeling} , year = {2024} }","title":"Citation"},{"location":"#support","text":"Issues : Report bugs and request features on GitHub Issues Discussions : Ask questions on GitHub Discussions Documentation : Browse the complete documentation at https://brentlab.github.io/tfbpmodeling/","title":"Support"},{"location":"api/bootstrap_model_results/","text":"bootstrap_model_results \u00b6 Results aggregation and statistical analysis for bootstrap modeling. tfbpmodeling.bootstrap_model_results \u00b6 BootstrapModelResults \u00b6 BootstrapModelResults ( ci_dict , bootstrap_coefs_df , alpha_list , alpha_df = pd . DataFrame (), ) Encapsulates the results from bootstrapped stratified cross-validation modeling. This includes: - Confidence intervals for model coefficients across bootstrap iterations - Raw coefficient estimates from each iteration - Alpha values (regularization strengths) selected during each iteration - Methods for extracting statistically significant coefficients - Visualization utilities - Serialization and deserialization support Initialize BootstrapModelResults. Parameters: ci_dict ( dict [ str , dict [ str , tuple [ float , float ]]] ) \u2013 Nested dictionary mapping confidence levels to (low, high) confidence intervals for each coefficient. bootstrap_coefs_df ( DataFrame ) \u2013 DataFrame of shape (n_bootstraps, n_features) containing coefficient values from each bootstrap sample. alpha_list ( list [ float ] ) \u2013 List of alpha values (regularization strength) selected during each bootstrap iteration. alpha_df ( DataFrame , default: DataFrame () ) \u2013 a dataframe with the columns 'bootstrap_idx', 'alpha', 'fold', and 'mse' Source code in tfbpmodeling/bootstrap_model_results.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def __init__ ( self , ci_dict : dict [ str , dict [ str , tuple [ float , float ]]], bootstrap_coefs_df : pd . DataFrame , alpha_list : list [ float ], alpha_df : pd . DataFrame = pd . DataFrame (), ): \"\"\" Initialize BootstrapModelResults. :param ci_dict: Nested dictionary mapping confidence levels to (low, high) confidence intervals for each coefficient. :param bootstrap_coefs_df: DataFrame of shape (n_bootstraps, n_features) containing coefficient values from each bootstrap sample. :param alpha_list: List of alpha values (regularization strength) selected during each bootstrap iteration. :param alpha_df: a dataframe with the columns 'bootstrap_idx', 'alpha', 'fold', and 'mse' \"\"\" self . ci_dict = ci_dict self . bootstrap_coefs_df = bootstrap_coefs_df self . alpha_list = alpha_list self . alpha_df = alpha_df deserialize classmethod \u00b6 deserialize ( ci_dict_json , coefs_alphas_pkl ) Load model results from disk. Parameters: ci_dict_json ( str ) \u2013 Path to the JSON file with confidence intervals. coefs_alphas_pkl ( str ) \u2013 Path to the Pickle file with coefficient matrix and alpha list. Returns: BootstrapModelResults \u2013 A new BootstrapModelResults instance. Raises: FileNotFoundError \u2013 If either file is missing. ValueError \u2013 If the pickle file contents are invalid. Source code in tfbpmodeling/bootstrap_model_results.py 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 @classmethod def deserialize ( cls , ci_dict_json : str , coefs_alphas_pkl : str ) -> \"BootstrapModelResults\" : \"\"\" Load model results from disk. :param ci_dict_json: Path to the JSON file with confidence intervals. :param coefs_alphas_pkl: Path to the Pickle file with coefficient matrix and alpha list. :return: A new BootstrapModelResults instance. :raises FileNotFoundError: If either file is missing. :raises ValueError: If the pickle file contents are invalid. \"\"\" # Ensure both files exist before proceeding if not os . path . exists ( ci_dict_json ): raise FileNotFoundError ( f \"Confidence intervals file ' { ci_dict_json } ' not found.\" ) if not os . path . exists ( coefs_alphas_pkl ): raise FileNotFoundError ( f \"Pickle file ' { coefs_alphas_pkl } ' not found.\" ) # Load confidence intervals from JSON with open ( ci_dict_json ) as f : ci_dict = json . load ( f ) # Load DataFrame and alpha_list from Pickle with open ( coefs_alphas_pkl , \"rb\" ) as f : loaded_data = pickle . load ( f ) # Validate loaded data if not isinstance ( loaded_data , tuple ) or len ( loaded_data ) != 2 : raise ValueError ( \"Pickle file does not contain expected (DataFrame, list) format.\" ) bootstrap_coefs_df , alpha_list = loaded_data return cls ( ci_dict , bootstrap_coefs_df , alpha_list ) extract_significant_coefficients \u00b6 extract_significant_coefficients ( ci_level = \"95.0\" , threshold = 0.0 ) Extract coefficients that are statistically significant based on their bootstrap confidence intervals. A coefficient is considered significant if its entire confidence interval lies above threshold or below -threshold . Parameters: ci_level ( str , default: '95.0' ) \u2013 Confidence interval level (e.g., \"95.0\"). threshold ( float , default: 0.0 ) \u2013 Minimum effect size for significance. Returns: dict [ str , tuple [ float , float ]] \u2013 Dictionary mapping coefficient names to their (low, high) CI bounds. Source code in tfbpmodeling/bootstrap_model_results.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def extract_significant_coefficients ( self , ci_level : str = \"95.0\" , threshold : float = 0.0 ) -> dict [ str , tuple [ float , float ]]: \"\"\" Extract coefficients that are statistically significant based on their bootstrap confidence intervals. A coefficient is considered significant if its entire confidence interval lies above `threshold` or below `-threshold`. :param ci_level: Confidence interval level (e.g., \"95.0\"). :param threshold: Minimum effect size for significance. :return: Dictionary mapping coefficient names to their (low, high) CI bounds. \"\"\" ci_dict_local = self . ci_dict . copy () # If CI level is not precomputed, calculate it if ci_level not in ci_dict_local : ci_level_numeric = float ( ci_level ) # log that the ci_level is not in the ci_dict logger . debug ( f \"Generating confidence intervals for ci level: { ci_level_numeric } \" ) ci_dict_local [ ci_level ] = { colname : ( np . percentile ( self . bootstrap_coefs_df [ colname ], ( 100 - ci_level_numeric ) / 2 ), np . percentile ( self . bootstrap_coefs_df [ colname ], 100 - ( 100 - ci_level_numeric ) / 2 , ), ) for colname in self . bootstrap_coefs_df . columns } # Select significant coefficients based on the confidence interval threshold significant_coefs_dict = { coef : bounds for coef , bounds in ci_dict_local [ ci_level ] . items () if bounds [ 0 ] > threshold or bounds [ 1 ] < - threshold } # remove the following terms from ci_dict: keys_to_remove = [ \"bootstrap_idx\" , \"final_training_score\" , \"alpha\" , \"left_asymptote\" , \"right_asymptote\" , \"Intercept\" , ] for key in keys_to_remove : significant_coefs_dict . pop ( key , None ) return significant_coefs_dict from_jsonl classmethod \u00b6 from_jsonl ( db_path , bootstrap_results_table_name = \"bootstrap_results\" , mse_table_name = \"mse_path\" , ) Load bootstrap results from JSONL files. This is intended to be used with the sigmoid bootstrap results. Parameters: db_path ( str ) \u2013 Path to the directory containing the JSONL files for a given regulator bootstrap_results_table_name ( str , default: 'bootstrap_results' ) \u2013 Name of the JSONL file containing bootstrap coefficient/final model results mse_table_name ( str , default: 'mse_path' ) \u2013 Name of the JSONL file containing fold-wise MSE results by bootstrap_idx/alpha Returns: BootstrapModelResults \u2013 An instance of BootstrapModelResults Raises: FileNotFoundError \u2013 If the JSONL files do not exist. Source code in tfbpmodeling/bootstrap_model_results.py 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 @classmethod def from_jsonl ( cls , db_path : str , bootstrap_results_table_name : str = \"bootstrap_results\" , mse_table_name : str = \"mse_path\" , ) -> \"BootstrapModelResults\" : \"\"\" Load bootstrap results from JSONL files. This is intended to be used with the sigmoid bootstrap results. :param db_path: Path to the directory containing the JSONL files for a given regulator :param bootstrap_results_table_name: Name of the JSONL file containing bootstrap coefficient/final model results :param mse_table_name: Name of the JSONL file containing fold-wise MSE results by bootstrap_idx/alpha :return: An instance of BootstrapModelResults :raises FileNotFoundError: If the JSONL files do not exist. \"\"\" bootstrap_coef_results_path = os . path . join ( db_path , f \" { bootstrap_results_table_name } .jsonl\" ) mse_path = os . path . join ( db_path , f \" { mse_table_name } .jsonl\" ) if not os . path . isfile ( bootstrap_coef_results_path ): raise FileNotFoundError ( f \"Results file not found: { bootstrap_coef_results_path } \" ) if not os . path . isfile ( mse_path ): raise FileNotFoundError ( f \"Results file not found: { mse_path } \" ) results_rows = [] with open ( bootstrap_coef_results_path ) as f : for line in f : try : results_rows . append ( json . loads ( line )) except json . JSONDecodeError : continue if not results_rows : raise ValueError ( \"No valid records found in the results JSONL file.\" ) bootstrap_coef_results_df = pd . DataFrame ( results_rows ) # Handle optional MSE file mse_rows = [] with open ( mse_path ) as f : for line in f : try : mse_rows . append ( json . loads ( line )) except json . JSONDecodeError : continue alpha_df = pd . DataFrame ( mse_rows ) if mse_rows else pd . DataFrame () return cls ( ci_dict = {}, bootstrap_coefs_df = bootstrap_coef_results_df , alpha_list = [], alpha_df = alpha_df , ) serialize \u00b6 serialize ( filename , output_dir = None ) Save the results to disk. Creates two files: - {filename}.json : confidence intervals - {filename}.pkl : tuple of (bootstrap_coefs_df, alpha_list) Parameters: filename ( str ) \u2013 Base filename (without extension). output_dir ( str | None , default: None ) \u2013 Optional directory to write files into. Uses current directory if not specified. Raises: FileNotFoundError \u2013 If the specified directory does not exist. Source code in tfbpmodeling/bootstrap_model_results.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 def serialize ( self , filename : str , output_dir : str | None = None ) -> None : \"\"\" Save the results to disk. Creates two files: - `{filename}.json`: confidence intervals - `{filename}.pkl`: tuple of (bootstrap_coefs_df, alpha_list) :param filename: Base filename (without extension). :param output_dir: Optional directory to write files into. Uses current directory if not specified. :raises FileNotFoundError: If the specified directory does not exist. \"\"\" # Validate that the output directory exists if output_dir : if not os . path . isdir ( output_dir ): raise FileNotFoundError ( f \"The output directory ' { output_dir } ' does not exist. \" \"Please create it before saving.\" ) filepath_json = os . path . join ( output_dir , f \" { filename } .json\" ) filepath_pkl = os . path . join ( output_dir , f \" { filename } .pkl\" ) else : filepath_json = f \" { filename } .json\" filepath_pkl = f \" { filename } .pkl\" # Save confidence intervals as JSON with open ( filepath_json , \"w\" ) as f : json . dump ( self . ci_dict , f , indent = 4 ) # Save DataFrame and alpha_list as a Pickle file with open ( filepath_pkl , \"wb\" ) as f : pickle . dump (( self . bootstrap_coefs_df , self . alpha_list ), f ) visualize_significant_coefficients \u00b6 visualize_significant_coefficients ( ci_level = \"95.0\" , threshold = 0.0 ) Visualize the distribution of coefficients that are significant at the specified confidence level. Parameters: ci_level ( str , default: '95.0' ) \u2013 Confidence interval level (e.g., \"95.0\"). threshold ( float , default: 0.0 ) \u2013 Minimum absolute value for significance. Returns: Figure | None \u2013 Matplotlib figure, or None if no significant coefficients are found. Source code in tfbpmodeling/bootstrap_model_results.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 def visualize_significant_coefficients ( self , ci_level : str = \"95.0\" , threshold : float = 0.0 ) -> plt . Figure | None : \"\"\" Visualize the distribution of coefficients that are significant at the specified confidence level. :param ci_level: Confidence interval level (e.g., \"95.0\"). :param threshold: Minimum absolute value for significance. :return: Matplotlib figure, or None if no significant coefficients are found. \"\"\" significant_coefs = self . extract_significant_coefficients ( ci_level , threshold ) if not significant_coefs : print ( f \"No significant coefficients found for CI { ci_level } \" \"at threshold {threshold} .\" ) return None # Extract relevant coefficients for plotting df_extracted = self . bootstrap_coefs_df [ list ( significant_coefs . keys ())] # Create the boxplot fig = plt . figure ( figsize = ( 10 , 6 )) sns . boxplot ( data = df_extracted , orient = \"h\" ) plt . axvline ( x = 0 , linestyle = \"--\" , color = \"black\" ) plt . xlabel ( \"Coefficient Values\" ) plt . title ( f \"Coefficients with { ci_level } % CI outside \u00b1 { threshold } \" ) return fig Overview \u00b6 The bootstrap_model_results module provides classes and functions for aggregating and analyzing results from bootstrap modeling. It handles the statistical analysis of coefficient distributions, confidence intervals, and significance testing. Key Features \u00b6 Coefficient Aggregation : Combines results from multiple bootstrap samples Confidence Interval Calculation : Computes percentile-based confidence intervals Statistical Significance : Determines feature significance based on CI bounds Result Export : Saves results in multiple formats for analysis Usage Examples \u00b6 Result Aggregation \u00b6 from tfbpmodeling.bootstrap_model_results import BootstrapModelResults # Create results aggregator results = BootstrapModelResults ( bootstrap_coefficients = coef_matrix , feature_names = feature_list , confidence_level = 95.0 ) # Get confidence intervals ci_results = results . get_confidence_intervals () # Get significant features significant_features = results . get_significant_features () Statistical Analysis \u00b6 # Calculate summary statistics summary_stats = results . get_summary_statistics () # Export results results . save_results ( output_dir = './results/' ) # Generate diagnostic plots results . plot_coefficient_distributions () Related Modules \u00b6 bootstrapped_input_data : Bootstrap data generation interface : Main workflow integration interactor_significance_results : Interaction analysis","title":"bootstrap_model_results"},{"location":"api/bootstrap_model_results/#bootstrap_model_results","text":"Results aggregation and statistical analysis for bootstrap modeling.","title":"bootstrap_model_results"},{"location":"api/bootstrap_model_results/#tfbpmodeling.bootstrap_model_results","text":"","title":"bootstrap_model_results"},{"location":"api/bootstrap_model_results/#tfbpmodeling.bootstrap_model_results.BootstrapModelResults","text":"BootstrapModelResults ( ci_dict , bootstrap_coefs_df , alpha_list , alpha_df = pd . DataFrame (), ) Encapsulates the results from bootstrapped stratified cross-validation modeling. This includes: - Confidence intervals for model coefficients across bootstrap iterations - Raw coefficient estimates from each iteration - Alpha values (regularization strengths) selected during each iteration - Methods for extracting statistically significant coefficients - Visualization utilities - Serialization and deserialization support Initialize BootstrapModelResults. Parameters: ci_dict ( dict [ str , dict [ str , tuple [ float , float ]]] ) \u2013 Nested dictionary mapping confidence levels to (low, high) confidence intervals for each coefficient. bootstrap_coefs_df ( DataFrame ) \u2013 DataFrame of shape (n_bootstraps, n_features) containing coefficient values from each bootstrap sample. alpha_list ( list [ float ] ) \u2013 List of alpha values (regularization strength) selected during each bootstrap iteration. alpha_df ( DataFrame , default: DataFrame () ) \u2013 a dataframe with the columns 'bootstrap_idx', 'alpha', 'fold', and 'mse' Source code in tfbpmodeling/bootstrap_model_results.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def __init__ ( self , ci_dict : dict [ str , dict [ str , tuple [ float , float ]]], bootstrap_coefs_df : pd . DataFrame , alpha_list : list [ float ], alpha_df : pd . DataFrame = pd . DataFrame (), ): \"\"\" Initialize BootstrapModelResults. :param ci_dict: Nested dictionary mapping confidence levels to (low, high) confidence intervals for each coefficient. :param bootstrap_coefs_df: DataFrame of shape (n_bootstraps, n_features) containing coefficient values from each bootstrap sample. :param alpha_list: List of alpha values (regularization strength) selected during each bootstrap iteration. :param alpha_df: a dataframe with the columns 'bootstrap_idx', 'alpha', 'fold', and 'mse' \"\"\" self . ci_dict = ci_dict self . bootstrap_coefs_df = bootstrap_coefs_df self . alpha_list = alpha_list self . alpha_df = alpha_df","title":"BootstrapModelResults"},{"location":"api/bootstrap_model_results/#tfbpmodeling.bootstrap_model_results.BootstrapModelResults.deserialize","text":"deserialize ( ci_dict_json , coefs_alphas_pkl ) Load model results from disk. Parameters: ci_dict_json ( str ) \u2013 Path to the JSON file with confidence intervals. coefs_alphas_pkl ( str ) \u2013 Path to the Pickle file with coefficient matrix and alpha list. Returns: BootstrapModelResults \u2013 A new BootstrapModelResults instance. Raises: FileNotFoundError \u2013 If either file is missing. ValueError \u2013 If the pickle file contents are invalid. Source code in tfbpmodeling/bootstrap_model_results.py 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 @classmethod def deserialize ( cls , ci_dict_json : str , coefs_alphas_pkl : str ) -> \"BootstrapModelResults\" : \"\"\" Load model results from disk. :param ci_dict_json: Path to the JSON file with confidence intervals. :param coefs_alphas_pkl: Path to the Pickle file with coefficient matrix and alpha list. :return: A new BootstrapModelResults instance. :raises FileNotFoundError: If either file is missing. :raises ValueError: If the pickle file contents are invalid. \"\"\" # Ensure both files exist before proceeding if not os . path . exists ( ci_dict_json ): raise FileNotFoundError ( f \"Confidence intervals file ' { ci_dict_json } ' not found.\" ) if not os . path . exists ( coefs_alphas_pkl ): raise FileNotFoundError ( f \"Pickle file ' { coefs_alphas_pkl } ' not found.\" ) # Load confidence intervals from JSON with open ( ci_dict_json ) as f : ci_dict = json . load ( f ) # Load DataFrame and alpha_list from Pickle with open ( coefs_alphas_pkl , \"rb\" ) as f : loaded_data = pickle . load ( f ) # Validate loaded data if not isinstance ( loaded_data , tuple ) or len ( loaded_data ) != 2 : raise ValueError ( \"Pickle file does not contain expected (DataFrame, list) format.\" ) bootstrap_coefs_df , alpha_list = loaded_data return cls ( ci_dict , bootstrap_coefs_df , alpha_list )","title":"deserialize"},{"location":"api/bootstrap_model_results/#tfbpmodeling.bootstrap_model_results.BootstrapModelResults.extract_significant_coefficients","text":"extract_significant_coefficients ( ci_level = \"95.0\" , threshold = 0.0 ) Extract coefficients that are statistically significant based on their bootstrap confidence intervals. A coefficient is considered significant if its entire confidence interval lies above threshold or below -threshold . Parameters: ci_level ( str , default: '95.0' ) \u2013 Confidence interval level (e.g., \"95.0\"). threshold ( float , default: 0.0 ) \u2013 Minimum effect size for significance. Returns: dict [ str , tuple [ float , float ]] \u2013 Dictionary mapping coefficient names to their (low, high) CI bounds. Source code in tfbpmodeling/bootstrap_model_results.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def extract_significant_coefficients ( self , ci_level : str = \"95.0\" , threshold : float = 0.0 ) -> dict [ str , tuple [ float , float ]]: \"\"\" Extract coefficients that are statistically significant based on their bootstrap confidence intervals. A coefficient is considered significant if its entire confidence interval lies above `threshold` or below `-threshold`. :param ci_level: Confidence interval level (e.g., \"95.0\"). :param threshold: Minimum effect size for significance. :return: Dictionary mapping coefficient names to their (low, high) CI bounds. \"\"\" ci_dict_local = self . ci_dict . copy () # If CI level is not precomputed, calculate it if ci_level not in ci_dict_local : ci_level_numeric = float ( ci_level ) # log that the ci_level is not in the ci_dict logger . debug ( f \"Generating confidence intervals for ci level: { ci_level_numeric } \" ) ci_dict_local [ ci_level ] = { colname : ( np . percentile ( self . bootstrap_coefs_df [ colname ], ( 100 - ci_level_numeric ) / 2 ), np . percentile ( self . bootstrap_coefs_df [ colname ], 100 - ( 100 - ci_level_numeric ) / 2 , ), ) for colname in self . bootstrap_coefs_df . columns } # Select significant coefficients based on the confidence interval threshold significant_coefs_dict = { coef : bounds for coef , bounds in ci_dict_local [ ci_level ] . items () if bounds [ 0 ] > threshold or bounds [ 1 ] < - threshold } # remove the following terms from ci_dict: keys_to_remove = [ \"bootstrap_idx\" , \"final_training_score\" , \"alpha\" , \"left_asymptote\" , \"right_asymptote\" , \"Intercept\" , ] for key in keys_to_remove : significant_coefs_dict . pop ( key , None ) return significant_coefs_dict","title":"extract_significant_coefficients"},{"location":"api/bootstrap_model_results/#tfbpmodeling.bootstrap_model_results.BootstrapModelResults.from_jsonl","text":"from_jsonl ( db_path , bootstrap_results_table_name = \"bootstrap_results\" , mse_table_name = \"mse_path\" , ) Load bootstrap results from JSONL files. This is intended to be used with the sigmoid bootstrap results. Parameters: db_path ( str ) \u2013 Path to the directory containing the JSONL files for a given regulator bootstrap_results_table_name ( str , default: 'bootstrap_results' ) \u2013 Name of the JSONL file containing bootstrap coefficient/final model results mse_table_name ( str , default: 'mse_path' ) \u2013 Name of the JSONL file containing fold-wise MSE results by bootstrap_idx/alpha Returns: BootstrapModelResults \u2013 An instance of BootstrapModelResults Raises: FileNotFoundError \u2013 If the JSONL files do not exist. Source code in tfbpmodeling/bootstrap_model_results.py 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 @classmethod def from_jsonl ( cls , db_path : str , bootstrap_results_table_name : str = \"bootstrap_results\" , mse_table_name : str = \"mse_path\" , ) -> \"BootstrapModelResults\" : \"\"\" Load bootstrap results from JSONL files. This is intended to be used with the sigmoid bootstrap results. :param db_path: Path to the directory containing the JSONL files for a given regulator :param bootstrap_results_table_name: Name of the JSONL file containing bootstrap coefficient/final model results :param mse_table_name: Name of the JSONL file containing fold-wise MSE results by bootstrap_idx/alpha :return: An instance of BootstrapModelResults :raises FileNotFoundError: If the JSONL files do not exist. \"\"\" bootstrap_coef_results_path = os . path . join ( db_path , f \" { bootstrap_results_table_name } .jsonl\" ) mse_path = os . path . join ( db_path , f \" { mse_table_name } .jsonl\" ) if not os . path . isfile ( bootstrap_coef_results_path ): raise FileNotFoundError ( f \"Results file not found: { bootstrap_coef_results_path } \" ) if not os . path . isfile ( mse_path ): raise FileNotFoundError ( f \"Results file not found: { mse_path } \" ) results_rows = [] with open ( bootstrap_coef_results_path ) as f : for line in f : try : results_rows . append ( json . loads ( line )) except json . JSONDecodeError : continue if not results_rows : raise ValueError ( \"No valid records found in the results JSONL file.\" ) bootstrap_coef_results_df = pd . DataFrame ( results_rows ) # Handle optional MSE file mse_rows = [] with open ( mse_path ) as f : for line in f : try : mse_rows . append ( json . loads ( line )) except json . JSONDecodeError : continue alpha_df = pd . DataFrame ( mse_rows ) if mse_rows else pd . DataFrame () return cls ( ci_dict = {}, bootstrap_coefs_df = bootstrap_coef_results_df , alpha_list = [], alpha_df = alpha_df , )","title":"from_jsonl"},{"location":"api/bootstrap_model_results/#tfbpmodeling.bootstrap_model_results.BootstrapModelResults.serialize","text":"serialize ( filename , output_dir = None ) Save the results to disk. Creates two files: - {filename}.json : confidence intervals - {filename}.pkl : tuple of (bootstrap_coefs_df, alpha_list) Parameters: filename ( str ) \u2013 Base filename (without extension). output_dir ( str | None , default: None ) \u2013 Optional directory to write files into. Uses current directory if not specified. Raises: FileNotFoundError \u2013 If the specified directory does not exist. Source code in tfbpmodeling/bootstrap_model_results.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 def serialize ( self , filename : str , output_dir : str | None = None ) -> None : \"\"\" Save the results to disk. Creates two files: - `{filename}.json`: confidence intervals - `{filename}.pkl`: tuple of (bootstrap_coefs_df, alpha_list) :param filename: Base filename (without extension). :param output_dir: Optional directory to write files into. Uses current directory if not specified. :raises FileNotFoundError: If the specified directory does not exist. \"\"\" # Validate that the output directory exists if output_dir : if not os . path . isdir ( output_dir ): raise FileNotFoundError ( f \"The output directory ' { output_dir } ' does not exist. \" \"Please create it before saving.\" ) filepath_json = os . path . join ( output_dir , f \" { filename } .json\" ) filepath_pkl = os . path . join ( output_dir , f \" { filename } .pkl\" ) else : filepath_json = f \" { filename } .json\" filepath_pkl = f \" { filename } .pkl\" # Save confidence intervals as JSON with open ( filepath_json , \"w\" ) as f : json . dump ( self . ci_dict , f , indent = 4 ) # Save DataFrame and alpha_list as a Pickle file with open ( filepath_pkl , \"wb\" ) as f : pickle . dump (( self . bootstrap_coefs_df , self . alpha_list ), f )","title":"serialize"},{"location":"api/bootstrap_model_results/#tfbpmodeling.bootstrap_model_results.BootstrapModelResults.visualize_significant_coefficients","text":"visualize_significant_coefficients ( ci_level = \"95.0\" , threshold = 0.0 ) Visualize the distribution of coefficients that are significant at the specified confidence level. Parameters: ci_level ( str , default: '95.0' ) \u2013 Confidence interval level (e.g., \"95.0\"). threshold ( float , default: 0.0 ) \u2013 Minimum absolute value for significance. Returns: Figure | None \u2013 Matplotlib figure, or None if no significant coefficients are found. Source code in tfbpmodeling/bootstrap_model_results.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 def visualize_significant_coefficients ( self , ci_level : str = \"95.0\" , threshold : float = 0.0 ) -> plt . Figure | None : \"\"\" Visualize the distribution of coefficients that are significant at the specified confidence level. :param ci_level: Confidence interval level (e.g., \"95.0\"). :param threshold: Minimum absolute value for significance. :return: Matplotlib figure, or None if no significant coefficients are found. \"\"\" significant_coefs = self . extract_significant_coefficients ( ci_level , threshold ) if not significant_coefs : print ( f \"No significant coefficients found for CI { ci_level } \" \"at threshold {threshold} .\" ) return None # Extract relevant coefficients for plotting df_extracted = self . bootstrap_coefs_df [ list ( significant_coefs . keys ())] # Create the boxplot fig = plt . figure ( figsize = ( 10 , 6 )) sns . boxplot ( data = df_extracted , orient = \"h\" ) plt . axvline ( x = 0 , linestyle = \"--\" , color = \"black\" ) plt . xlabel ( \"Coefficient Values\" ) plt . title ( f \"Coefficients with { ci_level } % CI outside \u00b1 { threshold } \" ) return fig","title":"visualize_significant_coefficients"},{"location":"api/bootstrap_model_results/#overview","text":"The bootstrap_model_results module provides classes and functions for aggregating and analyzing results from bootstrap modeling. It handles the statistical analysis of coefficient distributions, confidence intervals, and significance testing.","title":"Overview"},{"location":"api/bootstrap_model_results/#key-features","text":"Coefficient Aggregation : Combines results from multiple bootstrap samples Confidence Interval Calculation : Computes percentile-based confidence intervals Statistical Significance : Determines feature significance based on CI bounds Result Export : Saves results in multiple formats for analysis","title":"Key Features"},{"location":"api/bootstrap_model_results/#usage-examples","text":"","title":"Usage Examples"},{"location":"api/bootstrap_model_results/#result-aggregation","text":"from tfbpmodeling.bootstrap_model_results import BootstrapModelResults # Create results aggregator results = BootstrapModelResults ( bootstrap_coefficients = coef_matrix , feature_names = feature_list , confidence_level = 95.0 ) # Get confidence intervals ci_results = results . get_confidence_intervals () # Get significant features significant_features = results . get_significant_features ()","title":"Result Aggregation"},{"location":"api/bootstrap_model_results/#statistical-analysis","text":"# Calculate summary statistics summary_stats = results . get_summary_statistics () # Export results results . save_results ( output_dir = './results/' ) # Generate diagnostic plots results . plot_coefficient_distributions ()","title":"Statistical Analysis"},{"location":"api/bootstrap_model_results/#related-modules","text":"bootstrapped_input_data : Bootstrap data generation interface : Main workflow integration interactor_significance_results : Interaction analysis","title":"Related Modules"},{"location":"api/bootstrapped_input_data/","text":"bootstrapped_input_data \u00b6 Bootstrap resampling functionality for tfbpmodeling input data. tfbpmodeling.bootstrapped_input_data \u00b6 BootstrappedModelingInputData \u00b6 BootstrappedModelingInputData ( response_df , model_df , n_bootstraps , normalize_sample_weights = True , random_state = None , ) This class handles bootstrapped resampling of a response vector and model matrix. This class supports both on-the-fly generation and externally provided bootstrap indices. For each bootstrap sample, it maintains sample weights derived from frequency counts of resampled instances. Initialize bootstrapped modeling input. Either n_bootstraps or bootstrap_indices must be provided. Parameters: response_df ( DataFrame ) \u2013 Response variable. model_df ( DataFrame ) \u2013 Predictor matrix. n_bootstraps ( int ) \u2013 Number of bootstrap replicates to generate. random_state ( int | None , default: None ) \u2013 Random state for reproducibility. Can be an integer or a numpy RandomState object, or None. If None (default), then a random random state is chosen. Raises: ValueError \u2013 if the response_df and model_df do not have the same index or if arguments are not correct datatype. Source code in tfbpmodeling/bootstrapped_input_data.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def __init__ ( self , response_df : pd . DataFrame , model_df : pd . DataFrame , n_bootstraps : int , normalize_sample_weights : bool = True , random_state : int | None = None , ) -> None : \"\"\" Initialize bootstrapped modeling input. Either `n_bootstraps` or `bootstrap_indices` must be provided. :param response_df: Response variable. :param model_df: Predictor matrix. :param n_bootstraps: Number of bootstrap replicates to generate. :param random_state: Random state for reproducibility. Can be an integer or a numpy RandomState object, or None. If None (default), then a random random state is chosen. :raises ValueError: if the response_df and model_df do not have the same index or if arguments are not correct datatype. \"\"\" self . response_df : pd . DataFrame = response_df self . model_df : pd . DataFrame = model_df if not response_df . index . equals ( model_df . index ): raise IndexError ( \"response_df and model_df must have the same index order.\" ) self . normalize_sample_weights = normalize_sample_weights # If bootstrap_indices is provided, set n_bootstraps based on its length self . n_bootstraps = n_bootstraps # set the random number generator attribute self . random_state = random_state self . _rng = check_random_state ( self . random_state ) logger . info ( f \"Using random state: { self . random_state } \" if self . random_state is not None else \"No explicit random state set.\" ) # Initialize attributes self . _bootstrap_indices : list [ np . ndarray ] = [] self . _sample_weights : dict [ int , np . ndarray ] = {} self . _generate_bootstrap_indices () bootstrap_indices property writable \u00b6 bootstrap_indices A list of arrays representing bootstrap sample indices. model_df property writable \u00b6 model_df Get the model DataFrame. Returns: DataFrame \u2013 The model DataFrame. n_bootstraps property writable \u00b6 n_bootstraps Get the number of bootstrap samples. Returns: int \u2013 The number of bootstrap samples. normalize_sample_weights property writable \u00b6 normalize_sample_weights Get the normalization status for sample weights. Returns: bool \u2013 True if sample weights are normalized, False otherwise. random_state property writable \u00b6 random_state An integer used to set the random state when generating the bootstrap samples. Set this explicitly for reproducibility response_df property writable \u00b6 response_df Get the response DataFrame. Returns: DataFrame \u2013 The response DataFrame. sample_weights property writable \u00b6 sample_weights Normalized sample weights corresponding to bootstrap samples. Returns: dict [ int , ndarray ] \u2013 A dictionary mapping bootstrap index to sample weights. __iter__ \u00b6 __iter__ () Resets the iterator and returns itself. Source code in tfbpmodeling/bootstrapped_input_data.py 403 404 405 406 def __iter__ ( self ): \"\"\"Resets the iterator and returns itself.\"\"\" self . _current_index = 0 return self __next__ \u00b6 __next__ () Provides the next bootstrap sample for iteration. Returns: tuple [ ndarray , ndarray ] \u2013 Tuple of (sample_indices, sample_weights). Raises: StopIteration \u2013 When all bootstrap samples are exhausted. Source code in tfbpmodeling/bootstrapped_input_data.py 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 def __next__ ( self ) -> tuple [ np . ndarray , np . ndarray ]: \"\"\" Provides the next bootstrap sample for iteration. :return: Tuple of (sample_indices, sample_weights). :raises StopIteration: When all bootstrap samples are exhausted. \"\"\" if self . _current_index >= self . n_bootstraps : raise StopIteration sample_indices , sample_weights = self . get_bootstrap_sample ( self . _current_index ) self . _current_index += 1 return sample_indices , sample_weights deserialize classmethod \u00b6 deserialize ( filename ) Loads the object from a JSON file. Parameters: filename ( str ) \u2013 Path to the BootstrapModelingData JSON file. Source code in tfbpmodeling/bootstrapped_input_data.py 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 @classmethod def deserialize ( cls , filename : str ): \"\"\" Loads the object from a JSON file. :param filename: Path to the BootstrapModelingData JSON file. \"\"\" with open ( filename ) as f : data = json . load ( f ) response_df = pd . DataFrame ( ** data [ \"response_df\" ]) . rename_axis ( index = data [ \"index_name\" ] ) model_df = pd . DataFrame ( ** data [ \"model_df\" ]) . rename_axis ( index = data [ \"index_name\" ] ) n_bootstraps = data [ \"n_bootstraps\" ] normalize_sample_weights = data [ \"normalize_sample_weights\" ] random_state = data [ \"random_state\" ] instance = cls ( response_df , model_df , n_bootstraps , normalize_sample_weights = normalize_sample_weights , random_state = random_state , ) return instance get_bootstrap_sample \u00b6 get_bootstrap_sample ( i ) Retrieves a bootstrap sample by index. Parameters: i ( int ) \u2013 Bootstrap sample index. Returns: tuple [ ndarray , ndarray ] \u2013 Tuple of (sample_indices, sample_weights). Raises: IndexError \u2013 If the index exceeds the number of bootstraps. Source code in tfbpmodeling/bootstrapped_input_data.py 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 def get_bootstrap_sample ( self , i : int ) -> tuple [ np . ndarray , np . ndarray ]: \"\"\" Retrieves a bootstrap sample by index. :param i: Bootstrap sample index. :return: Tuple of (sample_indices, sample_weights). :raises IndexError: If the index exceeds the number of bootstraps. \"\"\" if i >= self . n_bootstraps or i < 0 : raise IndexError ( f \"Bootstrap index { i } out of range. Max: { self . n_bootstraps - 1 } \" ) sampled_indices = self . bootstrap_indices [ i ] sample_weights = self . get_sample_weight ( i ) return ( sampled_indices , sample_weights , ) get_sample_weight \u00b6 get_sample_weight ( i ) Retrieves sample weights for a bootstrap sample. Parameters: i ( int ) \u2013 Bootstrap sample index. Returns: ndarray \u2013 Array of sample weights. Source code in tfbpmodeling/bootstrapped_input_data.py 304 305 306 307 308 309 310 311 312 313 314 def get_sample_weight ( self , i : int ) -> np . ndarray : \"\"\" Retrieves sample weights for a bootstrap sample. :param i: Bootstrap sample index. :return: Array of sample weights. \"\"\" if i >= self . n_bootstraps or i < 0 : raise IndexError ( f \"Sample weight index { i } out of range.\" ) return self . sample_weights [ i ] regenerate \u00b6 regenerate () Re-generate, randomly, bootstrap samples and sample weights. This should be called if the response or predictors change. Source code in tfbpmodeling/bootstrapped_input_data.py 316 317 318 319 320 321 322 323 def regenerate ( self ) -> None : \"\"\" Re-generate, randomly, bootstrap samples and sample weights. This should be called if the response or predictors change. \"\"\" self . _generate_bootstrap_indices () save_indices \u00b6 save_indices ( filename ) Saves only the bootstrap indices to a JSON file. Saves the bootstrap indices to a JSON file. This can be used to persist the bootstrap indices for later use, allowing for reproducibility in analyses. Parameters: filename ( str ) \u2013 Path to the JSON file where the bootstrap indices will be saved. This will overwrite the file if it exists. Source code in tfbpmodeling/bootstrapped_input_data.py 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def save_indices ( self , filename : str ) -> None : \"\"\" Saves only the bootstrap indices to a JSON file. Saves the bootstrap indices to a JSON file. This can be used to persist the bootstrap indices for later use, allowing for reproducibility in analyses. :param filename: Path to the JSON file where the bootstrap indices will be saved. This will overwrite the file if it exists. \"\"\" data = { \"n_bootstraps\" : self . n_bootstraps , \"bootstrap_indices\" : [ indices . tolist () for indices in self . _bootstrap_indices ], } with open ( filename , \"w\" ) as f : json . dump ( data , f ) serialize \u00b6 serialize ( filename ) Saves the object as a JSON file. Serializes the current state of the BootstrappedModelingInputData object to a JSON file, including the response and model DataFrames, number of bootstraps, bootstrap indices, and sample weights. Parameters: filename ( str ) \u2013 Path to the JSON file where the object will be saved. Raises: ValueError \u2013 If the filename is not a valid path or if the object cannot be serialized. This method will overwrite the file if it exists. Source code in tfbpmodeling/bootstrapped_input_data.py 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 def serialize ( self , filename : str ) -> None : \"\"\" Saves the object as a JSON file. Serializes the current state of the BootstrappedModelingInputData object to a JSON file, including the response and model DataFrames, number of bootstraps, bootstrap indices, and sample weights. :param filename: Path to the JSON file where the object will be saved. :raises ValueError: If the filename is not a valid path or if the object cannot be serialized. This method will overwrite the file if it exists. \"\"\" data = { \"response_df\" : self . response_df . to_dict ( orient = \"split\" ), \"index_name\" : self . response_df . index . name , \"model_df\" : self . model_df . to_dict ( orient = \"split\" ), \"n_bootstraps\" : self . n_bootstraps , \"normalize_sample_weights\" : self . normalize_sample_weights , \"random_state\" : self . random_state , } with open ( filename , \"w\" ) as f : json . dump ( data , f ) Overview \u00b6 The bootstrapped_input_data module provides the BootstrappedModelingInputData class that extends the base ModelingInputData with bootstrap resampling capabilities. This is essential for the statistical inference approach used in tfbpmodeling. Key Features \u00b6 Bootstrap Sample Generation : Creates multiple resampled datasets from the original data Stratified Sampling : Maintains data distribution characteristics across bootstrap samples Reproducible Results : Supports random seed setting for consistent results Memory Efficient : Optimized storage and access patterns for large bootstrap sets Usage Examples \u00b6 Basic Bootstrap Creation \u00b6 from tfbpmodeling.modeling_input_data import ModelingInputData from tfbpmodeling.bootstrapped_input_data import BootstrappedModelingInputData # Create base data base_data = ModelingInputData ( response_file = 'expression.csv' , predictors_file = 'binding.csv' , perturbed_tf = 'YPD1' ) # Create bootstrap version bootstrap_data = BootstrappedModelingInputData ( base_data = base_data , n_bootstraps = 1000 , random_state = 42 ) Accessing Bootstrap Samples \u00b6 # Get bootstrap indices indices = bootstrap_data . get_bootstrap_indices () # Get specific bootstrap sample data sample_data = bootstrap_data . get_bootstrap_sample ( sample_idx = 0 ) # Iterate through all bootstrap samples for i in range ( bootstrap_data . n_bootstraps ): sample = bootstrap_data . get_bootstrap_sample ( i ) # Process sample... Related Modules \u00b6 modeling_input_data : Base data structures bootstrap_model_results : Results aggregation interface : Main workflow integration","title":"bootstrapped_input_data"},{"location":"api/bootstrapped_input_data/#bootstrapped_input_data","text":"Bootstrap resampling functionality for tfbpmodeling input data.","title":"bootstrapped_input_data"},{"location":"api/bootstrapped_input_data/#tfbpmodeling.bootstrapped_input_data","text":"","title":"bootstrapped_input_data"},{"location":"api/bootstrapped_input_data/#tfbpmodeling.bootstrapped_input_data.BootstrappedModelingInputData","text":"BootstrappedModelingInputData ( response_df , model_df , n_bootstraps , normalize_sample_weights = True , random_state = None , ) This class handles bootstrapped resampling of a response vector and model matrix. This class supports both on-the-fly generation and externally provided bootstrap indices. For each bootstrap sample, it maintains sample weights derived from frequency counts of resampled instances. Initialize bootstrapped modeling input. Either n_bootstraps or bootstrap_indices must be provided. Parameters: response_df ( DataFrame ) \u2013 Response variable. model_df ( DataFrame ) \u2013 Predictor matrix. n_bootstraps ( int ) \u2013 Number of bootstrap replicates to generate. random_state ( int | None , default: None ) \u2013 Random state for reproducibility. Can be an integer or a numpy RandomState object, or None. If None (default), then a random random state is chosen. Raises: ValueError \u2013 if the response_df and model_df do not have the same index or if arguments are not correct datatype. Source code in tfbpmodeling/bootstrapped_input_data.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def __init__ ( self , response_df : pd . DataFrame , model_df : pd . DataFrame , n_bootstraps : int , normalize_sample_weights : bool = True , random_state : int | None = None , ) -> None : \"\"\" Initialize bootstrapped modeling input. Either `n_bootstraps` or `bootstrap_indices` must be provided. :param response_df: Response variable. :param model_df: Predictor matrix. :param n_bootstraps: Number of bootstrap replicates to generate. :param random_state: Random state for reproducibility. Can be an integer or a numpy RandomState object, or None. If None (default), then a random random state is chosen. :raises ValueError: if the response_df and model_df do not have the same index or if arguments are not correct datatype. \"\"\" self . response_df : pd . DataFrame = response_df self . model_df : pd . DataFrame = model_df if not response_df . index . equals ( model_df . index ): raise IndexError ( \"response_df and model_df must have the same index order.\" ) self . normalize_sample_weights = normalize_sample_weights # If bootstrap_indices is provided, set n_bootstraps based on its length self . n_bootstraps = n_bootstraps # set the random number generator attribute self . random_state = random_state self . _rng = check_random_state ( self . random_state ) logger . info ( f \"Using random state: { self . random_state } \" if self . random_state is not None else \"No explicit random state set.\" ) # Initialize attributes self . _bootstrap_indices : list [ np . ndarray ] = [] self . _sample_weights : dict [ int , np . ndarray ] = {} self . _generate_bootstrap_indices ()","title":"BootstrappedModelingInputData"},{"location":"api/bootstrapped_input_data/#tfbpmodeling.bootstrapped_input_data.BootstrappedModelingInputData.bootstrap_indices","text":"bootstrap_indices A list of arrays representing bootstrap sample indices.","title":"bootstrap_indices"},{"location":"api/bootstrapped_input_data/#tfbpmodeling.bootstrapped_input_data.BootstrappedModelingInputData.model_df","text":"model_df Get the model DataFrame. Returns: DataFrame \u2013 The model DataFrame.","title":"model_df"},{"location":"api/bootstrapped_input_data/#tfbpmodeling.bootstrapped_input_data.BootstrappedModelingInputData.n_bootstraps","text":"n_bootstraps Get the number of bootstrap samples. Returns: int \u2013 The number of bootstrap samples.","title":"n_bootstraps"},{"location":"api/bootstrapped_input_data/#tfbpmodeling.bootstrapped_input_data.BootstrappedModelingInputData.normalize_sample_weights","text":"normalize_sample_weights Get the normalization status for sample weights. Returns: bool \u2013 True if sample weights are normalized, False otherwise.","title":"normalize_sample_weights"},{"location":"api/bootstrapped_input_data/#tfbpmodeling.bootstrapped_input_data.BootstrappedModelingInputData.random_state","text":"random_state An integer used to set the random state when generating the bootstrap samples. Set this explicitly for reproducibility","title":"random_state"},{"location":"api/bootstrapped_input_data/#tfbpmodeling.bootstrapped_input_data.BootstrappedModelingInputData.response_df","text":"response_df Get the response DataFrame. Returns: DataFrame \u2013 The response DataFrame.","title":"response_df"},{"location":"api/bootstrapped_input_data/#tfbpmodeling.bootstrapped_input_data.BootstrappedModelingInputData.sample_weights","text":"sample_weights Normalized sample weights corresponding to bootstrap samples. Returns: dict [ int , ndarray ] \u2013 A dictionary mapping bootstrap index to sample weights.","title":"sample_weights"},{"location":"api/bootstrapped_input_data/#tfbpmodeling.bootstrapped_input_data.BootstrappedModelingInputData.__iter__","text":"__iter__ () Resets the iterator and returns itself. Source code in tfbpmodeling/bootstrapped_input_data.py 403 404 405 406 def __iter__ ( self ): \"\"\"Resets the iterator and returns itself.\"\"\" self . _current_index = 0 return self","title":"__iter__"},{"location":"api/bootstrapped_input_data/#tfbpmodeling.bootstrapped_input_data.BootstrappedModelingInputData.__next__","text":"__next__ () Provides the next bootstrap sample for iteration. Returns: tuple [ ndarray , ndarray ] \u2013 Tuple of (sample_indices, sample_weights). Raises: StopIteration \u2013 When all bootstrap samples are exhausted. Source code in tfbpmodeling/bootstrapped_input_data.py 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 def __next__ ( self ) -> tuple [ np . ndarray , np . ndarray ]: \"\"\" Provides the next bootstrap sample for iteration. :return: Tuple of (sample_indices, sample_weights). :raises StopIteration: When all bootstrap samples are exhausted. \"\"\" if self . _current_index >= self . n_bootstraps : raise StopIteration sample_indices , sample_weights = self . get_bootstrap_sample ( self . _current_index ) self . _current_index += 1 return sample_indices , sample_weights","title":"__next__"},{"location":"api/bootstrapped_input_data/#tfbpmodeling.bootstrapped_input_data.BootstrappedModelingInputData.deserialize","text":"deserialize ( filename ) Loads the object from a JSON file. Parameters: filename ( str ) \u2013 Path to the BootstrapModelingData JSON file. Source code in tfbpmodeling/bootstrapped_input_data.py 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 @classmethod def deserialize ( cls , filename : str ): \"\"\" Loads the object from a JSON file. :param filename: Path to the BootstrapModelingData JSON file. \"\"\" with open ( filename ) as f : data = json . load ( f ) response_df = pd . DataFrame ( ** data [ \"response_df\" ]) . rename_axis ( index = data [ \"index_name\" ] ) model_df = pd . DataFrame ( ** data [ \"model_df\" ]) . rename_axis ( index = data [ \"index_name\" ] ) n_bootstraps = data [ \"n_bootstraps\" ] normalize_sample_weights = data [ \"normalize_sample_weights\" ] random_state = data [ \"random_state\" ] instance = cls ( response_df , model_df , n_bootstraps , normalize_sample_weights = normalize_sample_weights , random_state = random_state , ) return instance","title":"deserialize"},{"location":"api/bootstrapped_input_data/#tfbpmodeling.bootstrapped_input_data.BootstrappedModelingInputData.get_bootstrap_sample","text":"get_bootstrap_sample ( i ) Retrieves a bootstrap sample by index. Parameters: i ( int ) \u2013 Bootstrap sample index. Returns: tuple [ ndarray , ndarray ] \u2013 Tuple of (sample_indices, sample_weights). Raises: IndexError \u2013 If the index exceeds the number of bootstraps. Source code in tfbpmodeling/bootstrapped_input_data.py 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 def get_bootstrap_sample ( self , i : int ) -> tuple [ np . ndarray , np . ndarray ]: \"\"\" Retrieves a bootstrap sample by index. :param i: Bootstrap sample index. :return: Tuple of (sample_indices, sample_weights). :raises IndexError: If the index exceeds the number of bootstraps. \"\"\" if i >= self . n_bootstraps or i < 0 : raise IndexError ( f \"Bootstrap index { i } out of range. Max: { self . n_bootstraps - 1 } \" ) sampled_indices = self . bootstrap_indices [ i ] sample_weights = self . get_sample_weight ( i ) return ( sampled_indices , sample_weights , )","title":"get_bootstrap_sample"},{"location":"api/bootstrapped_input_data/#tfbpmodeling.bootstrapped_input_data.BootstrappedModelingInputData.get_sample_weight","text":"get_sample_weight ( i ) Retrieves sample weights for a bootstrap sample. Parameters: i ( int ) \u2013 Bootstrap sample index. Returns: ndarray \u2013 Array of sample weights. Source code in tfbpmodeling/bootstrapped_input_data.py 304 305 306 307 308 309 310 311 312 313 314 def get_sample_weight ( self , i : int ) -> np . ndarray : \"\"\" Retrieves sample weights for a bootstrap sample. :param i: Bootstrap sample index. :return: Array of sample weights. \"\"\" if i >= self . n_bootstraps or i < 0 : raise IndexError ( f \"Sample weight index { i } out of range.\" ) return self . sample_weights [ i ]","title":"get_sample_weight"},{"location":"api/bootstrapped_input_data/#tfbpmodeling.bootstrapped_input_data.BootstrappedModelingInputData.regenerate","text":"regenerate () Re-generate, randomly, bootstrap samples and sample weights. This should be called if the response or predictors change. Source code in tfbpmodeling/bootstrapped_input_data.py 316 317 318 319 320 321 322 323 def regenerate ( self ) -> None : \"\"\" Re-generate, randomly, bootstrap samples and sample weights. This should be called if the response or predictors change. \"\"\" self . _generate_bootstrap_indices ()","title":"regenerate"},{"location":"api/bootstrapped_input_data/#tfbpmodeling.bootstrapped_input_data.BootstrappedModelingInputData.save_indices","text":"save_indices ( filename ) Saves only the bootstrap indices to a JSON file. Saves the bootstrap indices to a JSON file. This can be used to persist the bootstrap indices for later use, allowing for reproducibility in analyses. Parameters: filename ( str ) \u2013 Path to the JSON file where the bootstrap indices will be saved. This will overwrite the file if it exists. Source code in tfbpmodeling/bootstrapped_input_data.py 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def save_indices ( self , filename : str ) -> None : \"\"\" Saves only the bootstrap indices to a JSON file. Saves the bootstrap indices to a JSON file. This can be used to persist the bootstrap indices for later use, allowing for reproducibility in analyses. :param filename: Path to the JSON file where the bootstrap indices will be saved. This will overwrite the file if it exists. \"\"\" data = { \"n_bootstraps\" : self . n_bootstraps , \"bootstrap_indices\" : [ indices . tolist () for indices in self . _bootstrap_indices ], } with open ( filename , \"w\" ) as f : json . dump ( data , f )","title":"save_indices"},{"location":"api/bootstrapped_input_data/#tfbpmodeling.bootstrapped_input_data.BootstrappedModelingInputData.serialize","text":"serialize ( filename ) Saves the object as a JSON file. Serializes the current state of the BootstrappedModelingInputData object to a JSON file, including the response and model DataFrames, number of bootstraps, bootstrap indices, and sample weights. Parameters: filename ( str ) \u2013 Path to the JSON file where the object will be saved. Raises: ValueError \u2013 If the filename is not a valid path or if the object cannot be serialized. This method will overwrite the file if it exists. Source code in tfbpmodeling/bootstrapped_input_data.py 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 def serialize ( self , filename : str ) -> None : \"\"\" Saves the object as a JSON file. Serializes the current state of the BootstrappedModelingInputData object to a JSON file, including the response and model DataFrames, number of bootstraps, bootstrap indices, and sample weights. :param filename: Path to the JSON file where the object will be saved. :raises ValueError: If the filename is not a valid path or if the object cannot be serialized. This method will overwrite the file if it exists. \"\"\" data = { \"response_df\" : self . response_df . to_dict ( orient = \"split\" ), \"index_name\" : self . response_df . index . name , \"model_df\" : self . model_df . to_dict ( orient = \"split\" ), \"n_bootstraps\" : self . n_bootstraps , \"normalize_sample_weights\" : self . normalize_sample_weights , \"random_state\" : self . random_state , } with open ( filename , \"w\" ) as f : json . dump ( data , f )","title":"serialize"},{"location":"api/bootstrapped_input_data/#overview","text":"The bootstrapped_input_data module provides the BootstrappedModelingInputData class that extends the base ModelingInputData with bootstrap resampling capabilities. This is essential for the statistical inference approach used in tfbpmodeling.","title":"Overview"},{"location":"api/bootstrapped_input_data/#key-features","text":"Bootstrap Sample Generation : Creates multiple resampled datasets from the original data Stratified Sampling : Maintains data distribution characteristics across bootstrap samples Reproducible Results : Supports random seed setting for consistent results Memory Efficient : Optimized storage and access patterns for large bootstrap sets","title":"Key Features"},{"location":"api/bootstrapped_input_data/#usage-examples","text":"","title":"Usage Examples"},{"location":"api/bootstrapped_input_data/#basic-bootstrap-creation","text":"from tfbpmodeling.modeling_input_data import ModelingInputData from tfbpmodeling.bootstrapped_input_data import BootstrappedModelingInputData # Create base data base_data = ModelingInputData ( response_file = 'expression.csv' , predictors_file = 'binding.csv' , perturbed_tf = 'YPD1' ) # Create bootstrap version bootstrap_data = BootstrappedModelingInputData ( base_data = base_data , n_bootstraps = 1000 , random_state = 42 )","title":"Basic Bootstrap Creation"},{"location":"api/bootstrapped_input_data/#accessing-bootstrap-samples","text":"# Get bootstrap indices indices = bootstrap_data . get_bootstrap_indices () # Get specific bootstrap sample data sample_data = bootstrap_data . get_bootstrap_sample ( sample_idx = 0 ) # Iterate through all bootstrap samples for i in range ( bootstrap_data . n_bootstraps ): sample = bootstrap_data . get_bootstrap_sample ( i ) # Process sample...","title":"Accessing Bootstrap Samples"},{"location":"api/bootstrapped_input_data/#related-modules","text":"modeling_input_data : Base data structures bootstrap_model_results : Results aggregation interface : Main workflow integration","title":"Related Modules"},{"location":"api/evaluate_interactor_significance_lassocv/","text":"evaluate_interactor_significance_lassocv \u00b6 LassoCV-based interactor significance testing for tfbpmodeling. tfbpmodeling.evaluate_interactor_significance_lassocv \u00b6 evaluate_interactor_significance_lassocv \u00b6 evaluate_interactor_significance_lassocv ( input_data , stratification_classes , model_variables , estimator = LassoCV ( fit_intercept = True , max_iter = 10000 , selection = \"random\" , random_state = 42 , n_jobs = 4 , ), ) Evaluate which interaction terms survive LassoCV when main effects are included. Returns: InteractorSignificanceResults \u2013 List of retained interaction terms - pd.Series of all model coefficients (indexed by term name) - Selected alpha value from LassoCV Source code in tfbpmodeling/evaluate_interactor_significance_lassocv.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def evaluate_interactor_significance_lassocv ( input_data : ModelingInputData , stratification_classes : np . ndarray , model_variables : list [ str ], estimator : BaseEstimator = LassoCV ( fit_intercept = True , max_iter = 10000 , selection = \"random\" , random_state = 42 , n_jobs = 4 , ), ) -> \"InteractorSignificanceResults\" : \"\"\" Evaluate which interaction terms survive LassoCV when main effects are included. :return: - List of retained interaction terms - pd.Series of all model coefficients (indexed by term name) - Selected alpha value from LassoCV \"\"\" logger . info ( \"Interactor significance evaluation method: LassoCV\" ) interactors = [ v for v in model_variables if \":\" in v ] modifier_main_effects = { i . split ( \":\" )[ 1 ] for i in interactors } augmented_vars = list ( set ( model_variables + list ( modifier_main_effects ))) logger . info ( f \"Model includes interaction terms and their main effects: { augmented_vars } \" ) add_row_max = \"row_max\" in augmented_vars logger . info ( \"Using 'row_max' in model variables \" \"for evaluate_interactor_significance: %s \" , add_row_max , ) X = input_data . get_modeling_data ( \" + \" . join ( augmented_vars ), add_row_max = add_row_max , drop_intercept = True , ) y = input_data . response_df skf = StratifiedKFold ( n_splits = 4 , shuffle = True , random_state = 42 ) model_i = stratified_cv_modeling ( y , X , classes = stratification_classes , estimator = estimator , skf = skf , ) coefs = pd . Series ( model_i . coef_ , index = X . columns ) retained_vars = coefs [ coefs != 0 ] . index . tolist () retained_interactors = [ v for v in retained_vars if \":\" in v ] logger . info ( f \"Retained interaction terms: { retained_interactors } \" ) y_pred = model_i . predict ( X ) r2_full_model = r2_score ( y , y_pred ) output = [] for interactor in interactors : main_effect = interactor . split ( \":\" )[ 1 ] output . append ( { \"interactor\" : interactor , \"variant\" : main_effect , \"r2_lasso_model\" : r2_full_model , \"coef_interactor\" : coefs . get ( interactor , 0.0 ), \"coef_main_effect\" : coefs . get ( main_effect , 0.0 ), } ) return InteractorSignificanceResults ( output ) Overview \u00b6 The evaluate_interactor_significance_lassocv module provides functions for evaluating the significance of interaction terms using LassoCV regularization. This approach uses regularized regression to compare models with and without interaction terms, providing a conservative approach to interaction significance testing. Key Features \u00b6 Regularized Comparison : Uses LassoCV to compare interaction vs main effect models Cross-Validation : Built-in CV for robust model comparison Conservative Testing : Regularization reduces false positive interactions Scalable Analysis : Handles high-dimensional feature spaces efficiently Usage Examples \u00b6 Basic Significance Testing \u00b6 from tfbpmodeling.evaluate_interactor_significance_lassocv import ( evaluate_interactor_significance_lassocv ) # Run LassoCV-based significance testing results = evaluate_interactor_significance_lassocv ( X_main = main_effects_data , X_interaction = interaction_data , y = response_data , cv_folds = 5 , alpha_range = np . logspace ( - 4 , 1 , 50 ) ) # Extract significant interactions significant_interactions = results [ 'significant_features' ] p_values = results [ 'p_values' ] Advanced Configuration \u00b6 # Custom LassoCV parameters results = evaluate_interactor_significance_lassocv ( X_main = main_effects_data , X_interaction = interaction_data , y = response_data , cv_folds = 10 , alpha_range = np . logspace ( - 5 , 2 , 100 ), max_iter = 10000 , tol = 1e-6 ) Method Details \u00b6 Statistical Approach \u00b6 Main Effect Model : Fit LassoCV with only main effects Interaction Model : Fit LassoCV with main effects + interactions Model Comparison : Compare CV scores and coefficient stability Significance Assessment : Determine if interactions improve model performance Advantages \u00b6 Regularization : Reduces overfitting in high-dimensional settings Feature Selection : Automatically selects relevant interactions Robust : Less sensitive to noise compared to standard linear regression Scalable : Efficient for large feature sets Considerations \u00b6 Conservative : May miss weak but real interactions Hyperparameter Sensitive : Alpha range affects results Interpretation : Regularized coefficients may be shrunk Related Modules \u00b6 evaluate_interactor_significance_linear : Linear regression alternative interactor_significance_results : Results handling interface : Workflow integration","title":"evaluate_interactor_significance_lassocv"},{"location":"api/evaluate_interactor_significance_lassocv/#evaluate_interactor_significance_lassocv","text":"LassoCV-based interactor significance testing for tfbpmodeling.","title":"evaluate_interactor_significance_lassocv"},{"location":"api/evaluate_interactor_significance_lassocv/#tfbpmodeling.evaluate_interactor_significance_lassocv","text":"","title":"evaluate_interactor_significance_lassocv"},{"location":"api/evaluate_interactor_significance_lassocv/#tfbpmodeling.evaluate_interactor_significance_lassocv.evaluate_interactor_significance_lassocv","text":"evaluate_interactor_significance_lassocv ( input_data , stratification_classes , model_variables , estimator = LassoCV ( fit_intercept = True , max_iter = 10000 , selection = \"random\" , random_state = 42 , n_jobs = 4 , ), ) Evaluate which interaction terms survive LassoCV when main effects are included. Returns: InteractorSignificanceResults \u2013 List of retained interaction terms - pd.Series of all model coefficients (indexed by term name) - Selected alpha value from LassoCV Source code in tfbpmodeling/evaluate_interactor_significance_lassocv.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def evaluate_interactor_significance_lassocv ( input_data : ModelingInputData , stratification_classes : np . ndarray , model_variables : list [ str ], estimator : BaseEstimator = LassoCV ( fit_intercept = True , max_iter = 10000 , selection = \"random\" , random_state = 42 , n_jobs = 4 , ), ) -> \"InteractorSignificanceResults\" : \"\"\" Evaluate which interaction terms survive LassoCV when main effects are included. :return: - List of retained interaction terms - pd.Series of all model coefficients (indexed by term name) - Selected alpha value from LassoCV \"\"\" logger . info ( \"Interactor significance evaluation method: LassoCV\" ) interactors = [ v for v in model_variables if \":\" in v ] modifier_main_effects = { i . split ( \":\" )[ 1 ] for i in interactors } augmented_vars = list ( set ( model_variables + list ( modifier_main_effects ))) logger . info ( f \"Model includes interaction terms and their main effects: { augmented_vars } \" ) add_row_max = \"row_max\" in augmented_vars logger . info ( \"Using 'row_max' in model variables \" \"for evaluate_interactor_significance: %s \" , add_row_max , ) X = input_data . get_modeling_data ( \" + \" . join ( augmented_vars ), add_row_max = add_row_max , drop_intercept = True , ) y = input_data . response_df skf = StratifiedKFold ( n_splits = 4 , shuffle = True , random_state = 42 ) model_i = stratified_cv_modeling ( y , X , classes = stratification_classes , estimator = estimator , skf = skf , ) coefs = pd . Series ( model_i . coef_ , index = X . columns ) retained_vars = coefs [ coefs != 0 ] . index . tolist () retained_interactors = [ v for v in retained_vars if \":\" in v ] logger . info ( f \"Retained interaction terms: { retained_interactors } \" ) y_pred = model_i . predict ( X ) r2_full_model = r2_score ( y , y_pred ) output = [] for interactor in interactors : main_effect = interactor . split ( \":\" )[ 1 ] output . append ( { \"interactor\" : interactor , \"variant\" : main_effect , \"r2_lasso_model\" : r2_full_model , \"coef_interactor\" : coefs . get ( interactor , 0.0 ), \"coef_main_effect\" : coefs . get ( main_effect , 0.0 ), } ) return InteractorSignificanceResults ( output )","title":"evaluate_interactor_significance_lassocv"},{"location":"api/evaluate_interactor_significance_lassocv/#overview","text":"The evaluate_interactor_significance_lassocv module provides functions for evaluating the significance of interaction terms using LassoCV regularization. This approach uses regularized regression to compare models with and without interaction terms, providing a conservative approach to interaction significance testing.","title":"Overview"},{"location":"api/evaluate_interactor_significance_lassocv/#key-features","text":"Regularized Comparison : Uses LassoCV to compare interaction vs main effect models Cross-Validation : Built-in CV for robust model comparison Conservative Testing : Regularization reduces false positive interactions Scalable Analysis : Handles high-dimensional feature spaces efficiently","title":"Key Features"},{"location":"api/evaluate_interactor_significance_lassocv/#usage-examples","text":"","title":"Usage Examples"},{"location":"api/evaluate_interactor_significance_lassocv/#basic-significance-testing","text":"from tfbpmodeling.evaluate_interactor_significance_lassocv import ( evaluate_interactor_significance_lassocv ) # Run LassoCV-based significance testing results = evaluate_interactor_significance_lassocv ( X_main = main_effects_data , X_interaction = interaction_data , y = response_data , cv_folds = 5 , alpha_range = np . logspace ( - 4 , 1 , 50 ) ) # Extract significant interactions significant_interactions = results [ 'significant_features' ] p_values = results [ 'p_values' ]","title":"Basic Significance Testing"},{"location":"api/evaluate_interactor_significance_lassocv/#advanced-configuration","text":"# Custom LassoCV parameters results = evaluate_interactor_significance_lassocv ( X_main = main_effects_data , X_interaction = interaction_data , y = response_data , cv_folds = 10 , alpha_range = np . logspace ( - 5 , 2 , 100 ), max_iter = 10000 , tol = 1e-6 )","title":"Advanced Configuration"},{"location":"api/evaluate_interactor_significance_lassocv/#method-details","text":"","title":"Method Details"},{"location":"api/evaluate_interactor_significance_lassocv/#statistical-approach","text":"Main Effect Model : Fit LassoCV with only main effects Interaction Model : Fit LassoCV with main effects + interactions Model Comparison : Compare CV scores and coefficient stability Significance Assessment : Determine if interactions improve model performance","title":"Statistical Approach"},{"location":"api/evaluate_interactor_significance_lassocv/#advantages","text":"Regularization : Reduces overfitting in high-dimensional settings Feature Selection : Automatically selects relevant interactions Robust : Less sensitive to noise compared to standard linear regression Scalable : Efficient for large feature sets","title":"Advantages"},{"location":"api/evaluate_interactor_significance_lassocv/#considerations","text":"Conservative : May miss weak but real interactions Hyperparameter Sensitive : Alpha range affects results Interpretation : Regularized coefficients may be shrunk","title":"Considerations"},{"location":"api/evaluate_interactor_significance_lassocv/#related-modules","text":"evaluate_interactor_significance_linear : Linear regression alternative interactor_significance_results : Results handling interface : Workflow integration","title":"Related Modules"},{"location":"api/evaluate_interactor_significance_linear/","text":"evaluate_interactor_significance_linear \u00b6 Linear regression-based interactor significance testing for tfbpmodeling. tfbpmodeling.evaluate_interactor_significance_linear \u00b6 evaluate_interactor_significance_linear \u00b6 evaluate_interactor_significance_linear ( input_data , stratification_classes , model_variables , estimator = LinearRegression ( fit_intercept = True ), ) Compare predictive performance of interaction terms vs. their main effects. This function performs a stratified cross-validation comparison between: - The original model containing interaction terms (e.g., TF1:TF2) - A reduced model where each interactor is replaced by its corresponding main effect (e.g., TF2) R\u00b2 scores are computed for both models using stratified CV. The delta in R\u00b2 informs whether the interaction term adds predictive value. Parameters: input_data ( ModelingInputData ) \u2013 A ModelingInputData instance containing predictors and response. stratification_classes ( ndarray ) \u2013 Array of stratification labels for CV. model_variables ( list [ str ] ) \u2013 List of model terms, including interaction terms. estimator ( BaseEstimator , default: LinearRegression (fit_intercept=True) ) \u2013 A scikit-learn estimator to use for modeling. Default is LinearRegression(fit_intercept=True) . Returns: InteractorSignificanceResults \u2013 An InteractorSignificanceResults instance with evaluation results. Raises: KeyError \u2013 If a main effect is missing from the input data. Source code in tfbpmodeling/evaluate_interactor_significance_linear.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def evaluate_interactor_significance_linear ( input_data : ModelingInputData , stratification_classes : np . ndarray , model_variables : list [ str ], estimator : BaseEstimator = LinearRegression ( fit_intercept = True ), ) -> \"InteractorSignificanceResults\" : \"\"\" Compare predictive performance of interaction terms vs. their main effects. This function performs a stratified cross-validation comparison between: - The original model containing interaction terms (e.g., TF1:TF2) - A reduced model where each interactor is replaced by its corresponding main effect (e.g., TF2) R\u00b2 scores are computed for both models using stratified CV. The delta in R\u00b2 informs whether the interaction term adds predictive value. :param input_data: A `ModelingInputData` instance containing predictors and response. :param stratification_classes: Array of stratification labels for CV. :param model_variables: List of model terms, including interaction terms. :param estimator: A scikit-learn estimator to use for modeling. Default is `LinearRegression(fit_intercept=True)`. :return: An `InteractorSignificanceResults` instance with evaluation results. :raises KeyError: If a main effect is missing from the input data. \"\"\" logger . info ( \"Interactor significance evaluation method: Linear\" ) output = [] response_df = input_data . response_df # Identify interaction terms (those with \":\") interactors = [ var for var in model_variables if \":\" in var ] logger . info ( f \"Testing the following interaction variables: { interactors } \" ) # NOTE: add_row_max is set to True such that IF the formula includes row_max, # the column is present. However, if the formula doesn't not include row_max, # then that column will not be present in the model matrix. add_row_max = \"row_max\" in model_variables logger . info ( \"Using 'row_max' in model variables \" \"for evaluate_interactor_significance: %s \" , add_row_max , ) # Get the average R\u00b2 of the original model avg_r2_original_model = stratified_cv_r2 ( response_df , input_data . get_modeling_data ( \" + \" . join ( model_variables ), add_row_max = add_row_max ), stratification_classes , estimator = estimator , ) for interactor in interactors : # Extract main effect from interactor main_effect = interactor . split ( \":\" )[ 1 ] logger . debug ( f \"Testing interactor ' { interactor } ' with variant ' { main_effect } '.\" ) # Ensure main effect exists in predictors if main_effect not in input_data . predictors_df . columns : raise KeyError ( f \"Main effect ' { main_effect } ' not found in predictors.\" ) # Define predictor sets for comparison predictors_with_main_effect = [ var for var in model_variables if var != interactor ] + [ main_effect ] # Replace interactor with main effect # Get the average R\u00b2 of the model with the main effect replacing one of the # interaction terms avg_r2_main_effect = stratified_cv_r2 ( response_df , input_data . get_modeling_data ( \" + \" . join ( predictors_with_main_effect ), add_row_max = add_row_max ), stratification_classes , estimator = estimator , ) # Store results output . append ( { \"interactor\" : interactor , \"variant\" : main_effect , \"avg_r2_interactor\" : avg_r2_original_model , \"avg_r2_main_effect\" : avg_r2_main_effect , \"delta_r2\" : avg_r2_main_effect - avg_r2_original_model , } ) return InteractorSignificanceResults ( output ) Overview \u00b6 The evaluate_interactor_significance_linear module provides functions for evaluating the significance of interaction terms using standard linear regression methods. This approach uses classical statistical tests to compare models with and without interaction terms. Key Features \u00b6 Classical Statistics : Uses standard linear regression and F-tests Direct Interpretation : Unregularized coefficients with clear interpretation Statistical Rigor : Proper p-values and confidence intervals Flexible Testing : Multiple comparison correction options Usage Examples \u00b6 Basic Significance Testing \u00b6 from tfbpmodeling.evaluate_interactor_significance_linear import ( evaluate_interactor_significance_linear ) # Run linear regression-based significance testing results = evaluate_interactor_significance_linear ( X_main = main_effects_data , X_interaction = interaction_data , y = response_data , alpha = 0.05 ) # Extract results significant_interactions = results [ 'significant_features' ] p_values = results [ 'p_values' ] f_statistics = results [ 'f_statistics' ] Multiple Comparison Correction \u00b6 # With Bonferroni correction results = evaluate_interactor_significance_linear ( X_main = main_effects_data , X_interaction = interaction_data , y = response_data , alpha = 0.05 , correction = 'bonferroni' ) # With FDR correction results = evaluate_interactor_significance_linear ( X_main = main_effects_data , X_interaction = interaction_data , y = response_data , alpha = 0.05 , correction = 'fdr_bh' ) Method Details \u00b6 Statistical Approach \u00b6 Main Effect Model : Fit linear regression with only main effects Full Model : Fit linear regression with main effects + interactions F-Test : Compare models using F-statistic for nested model comparison Individual Tests : Test each interaction term individually Model Comparison \u00b6 Nested F-Test : Overall test for any interaction effects Individual t-Tests : Test each interaction coefficient Partial F-Tests : Test subsets of interaction terms Multiple Comparisons : Adjust for multiple testing Advantages \u00b6 Interpretable : Direct coefficient interpretation Established Theory : Well-understood statistical properties Sensitive : Can detect small but significant effects Comprehensive : Provides full statistical inference Considerations \u00b6 Overfitting Risk : May overfit in high-dimensional settings Multicollinearity : Sensitive to correlated predictors Assumptions : Requires standard linear regression assumptions Multiple Testing : Needs correction for many interactions Related Modules \u00b6 evaluate_interactor_significance_lassocv : Regularized alternative interactor_significance_results : Results handling interface : Workflow integration","title":"evaluate_interactor_significance_linear"},{"location":"api/evaluate_interactor_significance_linear/#evaluate_interactor_significance_linear","text":"Linear regression-based interactor significance testing for tfbpmodeling.","title":"evaluate_interactor_significance_linear"},{"location":"api/evaluate_interactor_significance_linear/#tfbpmodeling.evaluate_interactor_significance_linear","text":"","title":"evaluate_interactor_significance_linear"},{"location":"api/evaluate_interactor_significance_linear/#tfbpmodeling.evaluate_interactor_significance_linear.evaluate_interactor_significance_linear","text":"evaluate_interactor_significance_linear ( input_data , stratification_classes , model_variables , estimator = LinearRegression ( fit_intercept = True ), ) Compare predictive performance of interaction terms vs. their main effects. This function performs a stratified cross-validation comparison between: - The original model containing interaction terms (e.g., TF1:TF2) - A reduced model where each interactor is replaced by its corresponding main effect (e.g., TF2) R\u00b2 scores are computed for both models using stratified CV. The delta in R\u00b2 informs whether the interaction term adds predictive value. Parameters: input_data ( ModelingInputData ) \u2013 A ModelingInputData instance containing predictors and response. stratification_classes ( ndarray ) \u2013 Array of stratification labels for CV. model_variables ( list [ str ] ) \u2013 List of model terms, including interaction terms. estimator ( BaseEstimator , default: LinearRegression (fit_intercept=True) ) \u2013 A scikit-learn estimator to use for modeling. Default is LinearRegression(fit_intercept=True) . Returns: InteractorSignificanceResults \u2013 An InteractorSignificanceResults instance with evaluation results. Raises: KeyError \u2013 If a main effect is missing from the input data. Source code in tfbpmodeling/evaluate_interactor_significance_linear.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def evaluate_interactor_significance_linear ( input_data : ModelingInputData , stratification_classes : np . ndarray , model_variables : list [ str ], estimator : BaseEstimator = LinearRegression ( fit_intercept = True ), ) -> \"InteractorSignificanceResults\" : \"\"\" Compare predictive performance of interaction terms vs. their main effects. This function performs a stratified cross-validation comparison between: - The original model containing interaction terms (e.g., TF1:TF2) - A reduced model where each interactor is replaced by its corresponding main effect (e.g., TF2) R\u00b2 scores are computed for both models using stratified CV. The delta in R\u00b2 informs whether the interaction term adds predictive value. :param input_data: A `ModelingInputData` instance containing predictors and response. :param stratification_classes: Array of stratification labels for CV. :param model_variables: List of model terms, including interaction terms. :param estimator: A scikit-learn estimator to use for modeling. Default is `LinearRegression(fit_intercept=True)`. :return: An `InteractorSignificanceResults` instance with evaluation results. :raises KeyError: If a main effect is missing from the input data. \"\"\" logger . info ( \"Interactor significance evaluation method: Linear\" ) output = [] response_df = input_data . response_df # Identify interaction terms (those with \":\") interactors = [ var for var in model_variables if \":\" in var ] logger . info ( f \"Testing the following interaction variables: { interactors } \" ) # NOTE: add_row_max is set to True such that IF the formula includes row_max, # the column is present. However, if the formula doesn't not include row_max, # then that column will not be present in the model matrix. add_row_max = \"row_max\" in model_variables logger . info ( \"Using 'row_max' in model variables \" \"for evaluate_interactor_significance: %s \" , add_row_max , ) # Get the average R\u00b2 of the original model avg_r2_original_model = stratified_cv_r2 ( response_df , input_data . get_modeling_data ( \" + \" . join ( model_variables ), add_row_max = add_row_max ), stratification_classes , estimator = estimator , ) for interactor in interactors : # Extract main effect from interactor main_effect = interactor . split ( \":\" )[ 1 ] logger . debug ( f \"Testing interactor ' { interactor } ' with variant ' { main_effect } '.\" ) # Ensure main effect exists in predictors if main_effect not in input_data . predictors_df . columns : raise KeyError ( f \"Main effect ' { main_effect } ' not found in predictors.\" ) # Define predictor sets for comparison predictors_with_main_effect = [ var for var in model_variables if var != interactor ] + [ main_effect ] # Replace interactor with main effect # Get the average R\u00b2 of the model with the main effect replacing one of the # interaction terms avg_r2_main_effect = stratified_cv_r2 ( response_df , input_data . get_modeling_data ( \" + \" . join ( predictors_with_main_effect ), add_row_max = add_row_max ), stratification_classes , estimator = estimator , ) # Store results output . append ( { \"interactor\" : interactor , \"variant\" : main_effect , \"avg_r2_interactor\" : avg_r2_original_model , \"avg_r2_main_effect\" : avg_r2_main_effect , \"delta_r2\" : avg_r2_main_effect - avg_r2_original_model , } ) return InteractorSignificanceResults ( output )","title":"evaluate_interactor_significance_linear"},{"location":"api/evaluate_interactor_significance_linear/#overview","text":"The evaluate_interactor_significance_linear module provides functions for evaluating the significance of interaction terms using standard linear regression methods. This approach uses classical statistical tests to compare models with and without interaction terms.","title":"Overview"},{"location":"api/evaluate_interactor_significance_linear/#key-features","text":"Classical Statistics : Uses standard linear regression and F-tests Direct Interpretation : Unregularized coefficients with clear interpretation Statistical Rigor : Proper p-values and confidence intervals Flexible Testing : Multiple comparison correction options","title":"Key Features"},{"location":"api/evaluate_interactor_significance_linear/#usage-examples","text":"","title":"Usage Examples"},{"location":"api/evaluate_interactor_significance_linear/#basic-significance-testing","text":"from tfbpmodeling.evaluate_interactor_significance_linear import ( evaluate_interactor_significance_linear ) # Run linear regression-based significance testing results = evaluate_interactor_significance_linear ( X_main = main_effects_data , X_interaction = interaction_data , y = response_data , alpha = 0.05 ) # Extract results significant_interactions = results [ 'significant_features' ] p_values = results [ 'p_values' ] f_statistics = results [ 'f_statistics' ]","title":"Basic Significance Testing"},{"location":"api/evaluate_interactor_significance_linear/#multiple-comparison-correction","text":"# With Bonferroni correction results = evaluate_interactor_significance_linear ( X_main = main_effects_data , X_interaction = interaction_data , y = response_data , alpha = 0.05 , correction = 'bonferroni' ) # With FDR correction results = evaluate_interactor_significance_linear ( X_main = main_effects_data , X_interaction = interaction_data , y = response_data , alpha = 0.05 , correction = 'fdr_bh' )","title":"Multiple Comparison Correction"},{"location":"api/evaluate_interactor_significance_linear/#method-details","text":"","title":"Method Details"},{"location":"api/evaluate_interactor_significance_linear/#statistical-approach","text":"Main Effect Model : Fit linear regression with only main effects Full Model : Fit linear regression with main effects + interactions F-Test : Compare models using F-statistic for nested model comparison Individual Tests : Test each interaction term individually","title":"Statistical Approach"},{"location":"api/evaluate_interactor_significance_linear/#model-comparison","text":"Nested F-Test : Overall test for any interaction effects Individual t-Tests : Test each interaction coefficient Partial F-Tests : Test subsets of interaction terms Multiple Comparisons : Adjust for multiple testing","title":"Model Comparison"},{"location":"api/evaluate_interactor_significance_linear/#advantages","text":"Interpretable : Direct coefficient interpretation Established Theory : Well-understood statistical properties Sensitive : Can detect small but significant effects Comprehensive : Provides full statistical inference","title":"Advantages"},{"location":"api/evaluate_interactor_significance_linear/#considerations","text":"Overfitting Risk : May overfit in high-dimensional settings Multicollinearity : Sensitive to correlated predictors Assumptions : Requires standard linear regression assumptions Multiple Testing : Needs correction for many interactions","title":"Considerations"},{"location":"api/evaluate_interactor_significance_linear/#related-modules","text":"evaluate_interactor_significance_lassocv : Regularized alternative interactor_significance_results : Results handling interface : Workflow integration","title":"Related Modules"},{"location":"api/interactor_significance_results/","text":"interactor_significance_results \u00b6 Results and analysis for transcription factor interaction significance testing. tfbpmodeling.interactor_significance_results \u00b6 InteractorSignificanceResults \u00b6 InteractorSignificanceResults ( evaluations ) Container for storing and analyzing the results of interactor significance testing. This class holds evaluations comparing the predictive power of interaction terms versus their corresponding main effects in a model, based on cross-validated R\u00b2. Provides methods to: - Convert results to a DataFrame. - Serialize and deserialize results from disk. - Select final model terms by comparing interaction and main effect performance. Initialize the evaluations object. Parameters: evaluations ( list [ dict [ str , Any ]] ) \u2013 A list of dictionaries containing significance test results. Source code in tfbpmodeling/interactor_significance_results.py 22 23 24 25 26 27 28 29 def __init__ ( self , evaluations : list [ dict [ str , Any ]]): \"\"\" Initialize the evaluations object. :param evaluations: A list of dictionaries containing significance test results. \"\"\" self . evaluations = evaluations deserialize classmethod \u00b6 deserialize ( filepath ) Load evaluations from a JSON file. Parameters: filepath ( str ) \u2013 Path to the JSON file containing evaluation results. Returns: InteractorSignificanceResults \u2013 An instance of InteractorSignificanceResults . Raises: ValueError \u2013 If the JSON content is not a list. Source code in tfbpmodeling/interactor_significance_results.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 @classmethod def deserialize ( cls , filepath : str ) -> \"InteractorSignificanceResults\" : \"\"\" Load evaluations from a JSON file. :param filepath: Path to the JSON file containing evaluation results. :return: An instance of `InteractorSignificanceResults`. :raises ValueError: If the JSON content is not a list. \"\"\" with open ( filepath ) as f : evaluations = json . load ( f ) if not isinstance ( evaluations , list ): raise ValueError ( f \"Invalid JSON format: Expected a list, got { type ( evaluations ) } \" ) return cls ( evaluations ) final_model \u00b6 final_model () Select the preferred model terms based on R\u00b2 comparison. For each interactor, compares R\u00b2 of the full model (with interaction term) to that of a model where the interactor is replaced by its main effect. Whichever yields higher R\u00b2 is retained. Returns: list [ str ] \u2013 List of selected model terms (interactor or main effect). Source code in tfbpmodeling/interactor_significance_results.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def final_model ( self ) -> list [ str ]: \"\"\" Select the preferred model terms based on R\u00b2 comparison. For each interactor, compares R\u00b2 of the full model (with interaction term) to that of a model where the interactor is replaced by its main effect. Whichever yields higher R\u00b2 is retained. :return: List of selected model terms (interactor or main effect). \"\"\" df = self . to_dataframe () if df . empty : return [] # Select either the interactor or the variant based on max R\u00b2 df [ \"selected\" ] = np . where ( df [ \"avg_r2_interactor\" ] >= df [ \"avg_r2_main_effect\" ], df [ \"interactor\" ], df [ \"variant\" ], ) return df [ \"selected\" ] . tolist () serialize \u00b6 serialize ( filepath ) Save the evaluations to a JSON file. Parameters: filepath ( str ) \u2013 Path to the output JSON file. Source code in tfbpmodeling/interactor_significance_results.py 40 41 42 43 44 45 46 47 48 def serialize ( self , filepath : str ) -> None : \"\"\" Save the evaluations to a JSON file. :param filepath: Path to the output JSON file. \"\"\" with open ( filepath , \"w\" ) as f : json . dump ( self . evaluations , f , indent = 4 ) to_dataframe \u00b6 to_dataframe () Return evaluations as a Pandas DataFrame. Returns: DataFrame \u2013 DataFrame containing the significance test results. Source code in tfbpmodeling/interactor_significance_results.py 31 32 33 34 35 36 37 38 def to_dataframe ( self ) -> pd . DataFrame : \"\"\" Return evaluations as a Pandas DataFrame. :return: DataFrame containing the significance test results. \"\"\" return pd . DataFrame ( self . evaluations ) Overview \u00b6 The interactor_significance_results module provides classes for storing, analyzing, and reporting the results of interaction significance testing. This is used in Stage 4 of the tfbpmodeling workflow to evaluate whether interaction terms provide significant explanatory power beyond main effects. Key Features \u00b6 Interaction vs Main Effect Comparison : Statistical comparison of interaction and main effect models Significance Testing : P-value calculation and hypothesis testing Effect Size Analysis : Quantification of interaction effect magnitudes Result Summarization : Comprehensive reporting of significant interactions Usage Examples \u00b6 Basic Usage \u00b6 from tfbpmodeling.interactor_significance_results import InteractorSignificanceResults # Create results object results = InteractorSignificanceResults ( interaction_effects = interaction_coefs , main_effects = main_coefs , p_values = p_vals , feature_names = features ) # Get significant interactions significant_interactions = results . get_significant_interactions ( alpha = 0.05 ) # Export results results . save_results ( './output/' ) Analysis and Reporting \u00b6 # Generate summary statistics summary = results . get_summary_statistics () # Plot interaction effects results . plot_interaction_effects () # Create comparison table comparison_table = results . create_comparison_table () Related Modules \u00b6 evaluate_interactor_significance_lassocv : LassoCV-based testing evaluate_interactor_significance_linear : Linear regression-based testing bootstrap_model_results : Bootstrap result aggregation","title":"interactor_significance_results"},{"location":"api/interactor_significance_results/#interactor_significance_results","text":"Results and analysis for transcription factor interaction significance testing.","title":"interactor_significance_results"},{"location":"api/interactor_significance_results/#tfbpmodeling.interactor_significance_results","text":"","title":"interactor_significance_results"},{"location":"api/interactor_significance_results/#tfbpmodeling.interactor_significance_results.InteractorSignificanceResults","text":"InteractorSignificanceResults ( evaluations ) Container for storing and analyzing the results of interactor significance testing. This class holds evaluations comparing the predictive power of interaction terms versus their corresponding main effects in a model, based on cross-validated R\u00b2. Provides methods to: - Convert results to a DataFrame. - Serialize and deserialize results from disk. - Select final model terms by comparing interaction and main effect performance. Initialize the evaluations object. Parameters: evaluations ( list [ dict [ str , Any ]] ) \u2013 A list of dictionaries containing significance test results. Source code in tfbpmodeling/interactor_significance_results.py 22 23 24 25 26 27 28 29 def __init__ ( self , evaluations : list [ dict [ str , Any ]]): \"\"\" Initialize the evaluations object. :param evaluations: A list of dictionaries containing significance test results. \"\"\" self . evaluations = evaluations","title":"InteractorSignificanceResults"},{"location":"api/interactor_significance_results/#tfbpmodeling.interactor_significance_results.InteractorSignificanceResults.deserialize","text":"deserialize ( filepath ) Load evaluations from a JSON file. Parameters: filepath ( str ) \u2013 Path to the JSON file containing evaluation results. Returns: InteractorSignificanceResults \u2013 An instance of InteractorSignificanceResults . Raises: ValueError \u2013 If the JSON content is not a list. Source code in tfbpmodeling/interactor_significance_results.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 @classmethod def deserialize ( cls , filepath : str ) -> \"InteractorSignificanceResults\" : \"\"\" Load evaluations from a JSON file. :param filepath: Path to the JSON file containing evaluation results. :return: An instance of `InteractorSignificanceResults`. :raises ValueError: If the JSON content is not a list. \"\"\" with open ( filepath ) as f : evaluations = json . load ( f ) if not isinstance ( evaluations , list ): raise ValueError ( f \"Invalid JSON format: Expected a list, got { type ( evaluations ) } \" ) return cls ( evaluations )","title":"deserialize"},{"location":"api/interactor_significance_results/#tfbpmodeling.interactor_significance_results.InteractorSignificanceResults.final_model","text":"final_model () Select the preferred model terms based on R\u00b2 comparison. For each interactor, compares R\u00b2 of the full model (with interaction term) to that of a model where the interactor is replaced by its main effect. Whichever yields higher R\u00b2 is retained. Returns: list [ str ] \u2013 List of selected model terms (interactor or main effect). Source code in tfbpmodeling/interactor_significance_results.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def final_model ( self ) -> list [ str ]: \"\"\" Select the preferred model terms based on R\u00b2 comparison. For each interactor, compares R\u00b2 of the full model (with interaction term) to that of a model where the interactor is replaced by its main effect. Whichever yields higher R\u00b2 is retained. :return: List of selected model terms (interactor or main effect). \"\"\" df = self . to_dataframe () if df . empty : return [] # Select either the interactor or the variant based on max R\u00b2 df [ \"selected\" ] = np . where ( df [ \"avg_r2_interactor\" ] >= df [ \"avg_r2_main_effect\" ], df [ \"interactor\" ], df [ \"variant\" ], ) return df [ \"selected\" ] . tolist ()","title":"final_model"},{"location":"api/interactor_significance_results/#tfbpmodeling.interactor_significance_results.InteractorSignificanceResults.serialize","text":"serialize ( filepath ) Save the evaluations to a JSON file. Parameters: filepath ( str ) \u2013 Path to the output JSON file. Source code in tfbpmodeling/interactor_significance_results.py 40 41 42 43 44 45 46 47 48 def serialize ( self , filepath : str ) -> None : \"\"\" Save the evaluations to a JSON file. :param filepath: Path to the output JSON file. \"\"\" with open ( filepath , \"w\" ) as f : json . dump ( self . evaluations , f , indent = 4 )","title":"serialize"},{"location":"api/interactor_significance_results/#tfbpmodeling.interactor_significance_results.InteractorSignificanceResults.to_dataframe","text":"to_dataframe () Return evaluations as a Pandas DataFrame. Returns: DataFrame \u2013 DataFrame containing the significance test results. Source code in tfbpmodeling/interactor_significance_results.py 31 32 33 34 35 36 37 38 def to_dataframe ( self ) -> pd . DataFrame : \"\"\" Return evaluations as a Pandas DataFrame. :return: DataFrame containing the significance test results. \"\"\" return pd . DataFrame ( self . evaluations )","title":"to_dataframe"},{"location":"api/interactor_significance_results/#overview","text":"The interactor_significance_results module provides classes for storing, analyzing, and reporting the results of interaction significance testing. This is used in Stage 4 of the tfbpmodeling workflow to evaluate whether interaction terms provide significant explanatory power beyond main effects.","title":"Overview"},{"location":"api/interactor_significance_results/#key-features","text":"Interaction vs Main Effect Comparison : Statistical comparison of interaction and main effect models Significance Testing : P-value calculation and hypothesis testing Effect Size Analysis : Quantification of interaction effect magnitudes Result Summarization : Comprehensive reporting of significant interactions","title":"Key Features"},{"location":"api/interactor_significance_results/#usage-examples","text":"","title":"Usage Examples"},{"location":"api/interactor_significance_results/#basic-usage","text":"from tfbpmodeling.interactor_significance_results import InteractorSignificanceResults # Create results object results = InteractorSignificanceResults ( interaction_effects = interaction_coefs , main_effects = main_coefs , p_values = p_vals , feature_names = features ) # Get significant interactions significant_interactions = results . get_significant_interactions ( alpha = 0.05 ) # Export results results . save_results ( './output/' )","title":"Basic Usage"},{"location":"api/interactor_significance_results/#analysis-and-reporting","text":"# Generate summary statistics summary = results . get_summary_statistics () # Plot interaction effects results . plot_interaction_effects () # Create comparison table comparison_table = results . create_comparison_table ()","title":"Analysis and Reporting"},{"location":"api/interactor_significance_results/#related-modules","text":"evaluate_interactor_significance_lassocv : LassoCV-based testing evaluate_interactor_significance_linear : Linear regression-based testing bootstrap_model_results : Bootstrap result aggregation","title":"Related Modules"},{"location":"api/interface/","text":"interface \u00b6 The main interface module provides the core workflow functions and command-line interface components for tfbpmodeling. tfbpmodeling.interface \u00b6 CustomHelpFormatter \u00b6 Bases: HelpFormatter This could be used to customize the help message formatting for the argparse parser. Left as a placeholder. common_modeling_input_arguments \u00b6 common_modeling_input_arguments ( parser , top_n_default = 600 ) Add common input arguments for modeling commands. Source code in tfbpmodeling/interface.py 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 def common_modeling_input_arguments ( parser : argparse . _ArgumentGroup , top_n_default : int | None = 600 ) -> None : \"\"\"Add common input arguments for modeling commands.\"\"\" parser . add_argument ( \"--response_file\" , type = str , required = True , help = ( \"Path to the response CSV file. The first column must contain \" \"feature names or locus tags (e.g., gene symbols), matching the index \" \"format in both response and predictor files. The perturbed gene will \" \"be removed from the model data only if its column names match the \" \"index format.\" ), ) parser . add_argument ( \"--predictors_file\" , type = str , required = True , help = ( \"Path to the predictors CSV file. The first column must contain \" \"feature names or locus tags (e.g., gene symbols), ensuring consistency \" \"between response and predictor files.\" ), ) parser . add_argument ( \"--perturbed_tf\" , type = str , required = True , help = ( \"Name of the perturbed transcription factor (TF) used as the \" \"response variable. It must match a column in the response file.\" ), ) parser . add_argument ( \"--blacklist_file\" , type = str , default = \"\" , help = ( \"Optional file containing a list of features (one per line) to be excluded \" \"from the analysis.\" ), ) parser . add_argument ( \"--n_bootstraps\" , type = int , default = 1000 , help = \"Number of bootstrap samples to generate for resampling. Default is 1000\" , ) parser . add_argument ( \"--random_state\" , type = int , default = None , help = \"Set this to an integer to make the bootstrap sampling reproducible. \" \"Default is None (no fixed seed) and each call will produce different \" \"bootstrap indices. Note that if this is set, the `top_n` random_state will \" \"be +10 in order to make the top_n indices different from the `all_data` step\" , ) parser . add_argument ( \"--normalize_sample_weights\" , action = \"store_true\" , help = ( \"Set this to normalize the sample weights to sum to 1. \" \"Default is False.\" ), ) parser . add_argument ( \"--scale_by_std\" , action = \"store_true\" , help = ( \"Set this to center and scale the model matrix. Note that setting this \" \"will set the `fit_intercept` parameter of the LassoCV estimator to \" \"False.\" ), ) parser . add_argument ( \"--top_n\" , type = int , default = top_n_default , help = ( \"Number of features to retain in the second round of modeling. \" f \"Default is { top_n_default } \" ), ) linear_perturbation_binding_modeling \u00b6 linear_perturbation_binding_modeling ( args ) Parameters: args \u2013 Command-line arguments containing input file paths and parameters. Source code in tfbpmodeling/interface.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 def linear_perturbation_binding_modeling ( args ): \"\"\" :param args: Command-line arguments containing input file paths and parameters. \"\"\" if not isinstance ( args . max_iter , int ) or args . max_iter < 1 : raise ValueError ( \"The `max_iter` parameter must be a positive integer.\" ) max_iter = int ( args . max_iter ) logger . info ( f \"estimator max_iter: { max_iter } .\" ) logger . info ( \"Step 1: Preprocessing\" ) # validate input files/dirs if not os . path . exists ( args . response_file ): raise FileNotFoundError ( f \"File { args . response_file } does not exist.\" ) if not os . path . exists ( args . predictors_file ): raise FileNotFoundError ( f \"File { args . predictors_file } does not exist.\" ) if os . path . exists ( args . output_dir ): logger . warning ( f \"Output directory { args . output_dir } already exists.\" ) else : os . makedirs ( args . output_dir , exist_ok = True ) logger . info ( f \"Output directory created at { args . output_dir } \" ) # the output subdir is where the output of this modeling run will be saved output_subdir = os . path . join ( args . output_dir , os . path . join ( args . perturbed_tf + args . output_suffix ) ) if os . path . exists ( output_subdir ): raise FileExistsError ( f \"Directory { output_subdir } already exists. \" \"Please specify a different `output_dir`.\" ) else : os . makedirs ( output_subdir , exist_ok = True ) logger . info ( f \"Output subdirectory created at { output_subdir } \" ) # instantiate a estimator # `fit_intercept` is set opposite of `scale_by_std`. If `scale_by_std` is `False`, # the default, then `fit_intercept` is set to True and the estimator will fit the # intercept. If `scale_by_std` is True, then the estimator will not fit the # intercept, meaning it assumes the data is centered. estimator = LassoCV ( fit_intercept = True , selection = \"random\" , n_alphas = 100 , random_state = 42 , n_jobs = args . n_cpus , max_iter = max_iter , ) input_data = ModelingInputData . from_files ( response_path = args . response_file , predictors_path = args . predictors_file , perturbed_tf = args . perturbed_tf , feature_blacklist_path = args . blacklist_file , top_n = args . top_n , ) logger . info ( \"Step 2: Bootstrap LassoCV on all data, full interactor model\" ) # Unset the top n masking -- we want to use all the data for the first round # modeling input_data . top_n_masked = False # extract a list of predictor variables, which are the columns of the predictors_df predictor_variables = input_data . predictors_df . columns . drop ( input_data . perturbed_tf ) # drop any variables which are in args.exclude_interactor_variables predictor_variables = exclude_predictor_variables ( list ( predictor_variables ), args . exclude_interactor_variables ) # create a list of interactor terms with the perturbed_tf as the first term interaction_terms = [ f \" { input_data . perturbed_tf } : { var } \" for var in predictor_variables ] # Construct the full interaction formula, ie perturbed_tf + perturbed_tf:other_tf1 + # perturbed_tf:other_tf2 + ... . perturbed_tf main effect only added if # --ptf_main_effect is passed. if args . ptf_main_effect : logger . info ( \"adding pTF main effect to `all_data_formula`\" ) all_data_formula = ( f \" { input_data . perturbed_tf } + { ' + ' . join ( interaction_terms ) } \" ) else : all_data_formula = \" + \" . join ( interaction_terms ) if args . squared_pTF : # if --squared_pTF is passed, then add the squared perturbed TF to the formula squared_term = f \"I( { input_data . perturbed_tf } ** 2)\" logger . info ( f \"Adding squared term to model formula: { squared_term } \" ) all_data_formula += f \" + { squared_term } \" if args . cubic_pTF : # if --cubic_pTF is passed, then add the cubic perturbed TF to the formula cubic_term = f \"I( { input_data . perturbed_tf } ** 3)\" logger . info ( f \"Add cubic term to model formula: { cubic_term } \" ) all_data_formula += f \" + { cubic_term } \" # if --row_max is passed, then add \"row_max\" to the formula if args . row_max : logger . info ( \"Adding `row_max` to the all data model formula\" ) all_data_formula += \" + row_max\" # if --add_model_variables is passed, then add the variables to the formula if args . add_model_variables : logger . info ( f \"Adding model variables to the all data model \" f \"formula: { args . add_model_variables } \" ) all_data_formula += \" + \" + \" + \" . join ( args . add_model_variables ) logger . debug ( f \"All data formula: { all_data_formula } \" ) # create the bootstrapped data. bootstrapped_data_all = BootstrappedModelingInputData ( response_df = input_data . response_df , model_df = input_data . get_modeling_data ( all_data_formula , add_row_max = args . row_max , drop_intercept = True , scale_by_std = args . scale_by_std , ), n_bootstraps = args . n_bootstraps , normalize_sample_weights = args . normalize_sample_weights , random_state = args . random_state , ) logger . info ( f \"Running bootstrap LassoCV on all data with { args . n_bootstraps } bootstraps\" ) if args . iterative_dropout : logger . info ( \"Using iterative dropout modeling for all data results.\" ) all_data_results = bootstrap_stratified_cv_loop ( bootstrapped_data = bootstrapped_data_all , perturbed_tf_series = input_data . predictors_df [ input_data . perturbed_tf ], estimator = estimator , ci_percentile = float ( args . all_data_ci_level ), stabilization_ci_start = args . stabilization_ci_start , bins = args . bins , output_dir = output_subdir , ) else : logger . info ( \"Using standard bootstrap modeling for all data results.\" ) all_data_results = bootstrap_stratified_cv_modeling ( bootstrapped_data = bootstrapped_data_all , perturbed_tf_series = input_data . predictors_df [ input_data . perturbed_tf ], estimator = estimator , ci_percentiles = [ float ( args . all_data_ci_level )], bins = args . bins , ) # create the all data object output subdir all_data_output = os . path . join ( output_subdir , \"all_data_result_object\" ) os . makedirs ( all_data_output , exist_ok = True ) logger . info ( f \"Serializing all data results to { all_data_output } \" ) all_data_results . serialize ( \"result_obj\" , all_data_output ) # Extract the coefficients that are significant at the specified confidence level all_data_sig_coefs = all_data_results . extract_significant_coefficients ( ci_level = args . all_data_ci_level , ) logger . info ( f \"all_data_sig_coefs: { all_data_sig_coefs } \" ) if not all_data_sig_coefs : logger . warning ( f \"No significant coefficients found at { args . all_data_ci_level } % \" \"confidence level. Exiting.\" ) return # write all_data_sig_coefs to a json file all_data_ci_str = str ( args . all_data_ci_level ) . replace ( \".\" , \"-\" ) all_data_output_file = os . path . join ( output_subdir , f \"all_data_significant_ { all_data_ci_str } .json\" ) logger . info ( f \"Writing the all data significant results to { all_data_output_file } \" ) with open ( all_data_output_file , \"w\" , ) as f : json . dump ( all_data_sig_coefs , f , indent = 4 ) logger . info ( \"Step 3: Running LassoCV on topn data with significant coefficients \" \"from the all data model\" ) # Create the formula for the topn modeling from the significant coefficients # NOTE: to remove the intercept, we need to add \" -1 \" topn_formula = f \" { ' + ' . join ( all_data_sig_coefs . keys ()) } \" logger . debug ( f \"Topn formula: { topn_formula } \" ) # apply the top_n masking input_data . top_n_masked = True # Create the bootstrapped data for the topn modeling bootstrapped_data_top_n = BootstrappedModelingInputData ( response_df = input_data . response_df , model_df = input_data . get_modeling_data ( topn_formula , add_row_max = args . row_max , drop_intercept = True , scale_by_std = args . scale_by_std , ), n_bootstraps = args . n_bootstraps , normalize_sample_weights = args . normalize_sample_weights , random_state = ( args . random_state + 10 if args . random_state else args . random_state ), ) logger . debug ( f \"Running bootstrap LassoCV on topn data with { args . n_bootstraps } bootstraps\" ) topn_results = bootstrap_stratified_cv_modeling ( bootstrapped_data_top_n , input_data . predictors_df [ input_data . perturbed_tf ], estimator = estimator , ci_percentiles = [ float ( args . topn_ci_level )], ) # create the topn data object output subdir topn_output = os . path . join ( output_subdir , \"topn_result_object\" ) os . makedirs ( topn_output , exist_ok = True ) logger . info ( f \"Serializing topn results to { topn_output } \" ) topn_results . serialize ( \"result_obj\" , topn_output ) # extract the topn_results at the specified confidence level topn_output_res = topn_results . extract_significant_coefficients ( ci_level = args . topn_ci_level ) logger . info ( f \"topn_output_res: { topn_output_res } \" ) if not topn_output_res : logger . warning ( f \"No significant coefficients found at { args . topn_ci_level } % \" \"confidence level. Exiting.\" ) return # write topn_output_res to a json file topn_ci_str = str ( args . topn_ci_level ) . replace ( \".\" , \"-\" ) topn_output_file = os . path . join ( output_subdir , f \"topn_significant_ { topn_ci_str } .json\" ) logger . info ( f \"Writing the topn significant results to { topn_output_file } \" ) with open ( topn_output_file , \"w\" ) as f : json . dump ( topn_output_res , f , indent = 4 ) logger . info ( \"Step 4: Test the significance of the interactor terms that survive \" \"against the corresoponding main effect\" ) if args . stage4_topn : logger . info ( \"Stage 4 will use top-n masked input data.\" ) input_data . top_n_masked = True else : logger . info ( \"Stage 4 will use full input data.\" ) # calculate the statification classes for the perturbed TF (all data) stage4_classes = stratification_classification ( input_data . predictors_df [ input_data . perturbed_tf ] . squeeze (), bins = args . bins , ) # Test the significance of the interactor terms evaluate_interactor_significance = ( evaluate_interactor_significance_lassocv if args . stage4_lasso else evaluate_interactor_significance_linear ) results = evaluate_interactor_significance ( input_data , stratification_classes = stage4_classes , model_variables = list ( topn_results . extract_significant_coefficients ( ci_level = args . topn_ci_level ) . keys () ), estimator = estimator , ) output_significance_file = os . path . join ( output_subdir , \"interactor_vs_main_result.json\" ) logger . info ( \"Writing the final interactor significance \" f \"results to { output_significance_file } \" ) results . serialize ( output_significance_file ) Overview \u00b6 The interface module serves as the primary entry point for the tfbpmodeling workflow. It contains: Main workflow function : linear_perturbation_binding_modeling() CLI helper functions : Argument parsing utilities for the command-line interface Custom formatters : Enhanced help formatting for better user experience Main Functions \u00b6 linear_perturbation_binding_modeling \u00b6 The core function that executes the complete 4-stage TFBP modeling workflow: Data Preprocessing : Load and validate input files, handle missing data Bootstrap Modeling : All-data analysis with bootstrap resampling and LassoCV Top-N Modeling : Refined analysis on significant predictors from top-performing data Interactor Significance : Statistical evaluation of interaction terms vs main effects Parameters : Command-line arguments object containing all configuration options Returns : None (results saved to output directory) Key Features : - Comprehensive input validation - Automatic output directory creation with timestamps - Detailed logging of all processing steps - Error handling with informative messages CLI Helper Functions \u00b6 common_modeling_input_arguments \u00b6 Adds standard input arguments to argument parsers: - File paths for response and predictor data - Perturbed TF specification - Bootstrap and sampling parameters common_modeling_feature_options \u00b6 Configures feature engineering options: - Polynomial terms (squared, cubic) - Row maximum inclusion - Custom variable additions and exclusions common_modeling_binning_arguments \u00b6 Sets up data stratification parameters: - Bin edge specifications - Stratification methods add_general_arguments_to_subparsers \u00b6 Propagates global arguments to subcommand parsers: - Logging configuration - System-wide options Data Flow \u00b6 graph TD A[CLI Arguments] --> B[Input Validation] B --> C[Data Loading] C --> D[ModelingInputData] D --> E[BootstrappedModelingInputData] E --> F[Bootstrap CV Loop] F --> G[Top-N Selection] G --> H[Interactor Significance] H --> I[Results Output] Usage Examples \u00b6 Programmatic Usage \u00b6 import argparse from tfbpmodeling.interface import linear_perturbation_binding_modeling # Create arguments object args = argparse . Namespace ( response_file = 'data/expression.csv' , predictors_file = 'data/binding.csv' , perturbed_tf = 'YPD1' , n_bootstraps = 1000 , top_n = 600 , all_data_ci_level = 98.0 , topn_ci_level = 90.0 , max_iter = 10000 , output_dir = './results' , output_suffix = '' , n_cpus = 4 , # ... other parameters ) # Run analysis linear_perturbation_binding_modeling ( args ) Custom Argument Parser \u00b6 import argparse from tfbpmodeling.interface import ( common_modeling_input_arguments , common_modeling_feature_options , CustomHelpFormatter ) # Create custom parser parser = argparse . ArgumentParser ( formatter_class = CustomHelpFormatter , description = \"Custom TFBP Analysis\" ) # Add standard arguments input_group = parser . add_argument_group ( \"Input\" ) common_modeling_input_arguments ( input_group ) feature_group = parser . add_argument_group ( \"Features\" ) common_modeling_feature_options ( feature_group ) # Parse and use args = parser . parse_args () linear_perturbation_binding_modeling ( args ) Error Handling \u00b6 The interface module includes comprehensive error handling: Input Validation Errors \u00b6 # File existence checks FileNotFoundError : \"File data/missing.csv does not exist.\" # Parameter validation ValueError : \"The `max_iter` parameter must be a positive integer.\" # Data format validation ValueError : \"Perturbed TF 'INVALID' not found in response file columns\" Runtime Errors \u00b6 # Convergence issues RuntimeWarning : \"LassoCV failed to converge for 15/1000 bootstrap samples\" # Insufficient data ValueError : \"Insufficient data after filtering. Found 5 samples, minimum required: 10\" Configuration Options \u00b6 The interface supports extensive configuration through command-line arguments: Core Parameters \u00b6 Input files : Response data, predictor data, optional blacklist TF specification : Name of perturbed transcription factor Bootstrap settings : Sample count, random seed, weight normalization Feature Engineering \u00b6 Polynomial terms : Squared and cubic pTF terms Additional predictors : Row max, custom variables Interaction control : Variable exclusions, main effects Model Configuration \u00b6 Confidence intervals : Separate thresholds for each stage Convergence : Maximum iterations, dropout options Performance : CPU cores, memory management Output Control \u00b6 Directory structure : Base directory, custom suffixes Logging : Verbosity levels, file vs console output Performance Considerations \u00b6 Memory Management \u00b6 Bootstrap samples stored efficiently using sparse representations Automatic garbage collection between stages Memory usage monitoring and warnings Parallel Processing \u00b6 LassoCV uses specified CPU cores for cross-validation Bootstrap samples processed in batches I/O operations optimized for large datasets Runtime Optimization \u00b6 Early stopping for non-convergent models Adaptive batch sizing based on available memory Progress reporting for long-running analyses Related Modules \u00b6 modeling_input_data : Core data structures bootstrapped_input_data : Bootstrap resampling bootstrap_model_results : Result aggregation evaluate_interactor_significance_lassocv : LassoCV-based significance testing evaluate_interactor_significance_linear : Linear regression-based significance testing","title":"interface"},{"location":"api/interface/#interface","text":"The main interface module provides the core workflow functions and command-line interface components for tfbpmodeling.","title":"interface"},{"location":"api/interface/#tfbpmodeling.interface","text":"","title":"interface"},{"location":"api/interface/#tfbpmodeling.interface.CustomHelpFormatter","text":"Bases: HelpFormatter This could be used to customize the help message formatting for the argparse parser. Left as a placeholder.","title":"CustomHelpFormatter"},{"location":"api/interface/#tfbpmodeling.interface.common_modeling_input_arguments","text":"common_modeling_input_arguments ( parser , top_n_default = 600 ) Add common input arguments for modeling commands. Source code in tfbpmodeling/interface.py 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 def common_modeling_input_arguments ( parser : argparse . _ArgumentGroup , top_n_default : int | None = 600 ) -> None : \"\"\"Add common input arguments for modeling commands.\"\"\" parser . add_argument ( \"--response_file\" , type = str , required = True , help = ( \"Path to the response CSV file. The first column must contain \" \"feature names or locus tags (e.g., gene symbols), matching the index \" \"format in both response and predictor files. The perturbed gene will \" \"be removed from the model data only if its column names match the \" \"index format.\" ), ) parser . add_argument ( \"--predictors_file\" , type = str , required = True , help = ( \"Path to the predictors CSV file. The first column must contain \" \"feature names or locus tags (e.g., gene symbols), ensuring consistency \" \"between response and predictor files.\" ), ) parser . add_argument ( \"--perturbed_tf\" , type = str , required = True , help = ( \"Name of the perturbed transcription factor (TF) used as the \" \"response variable. It must match a column in the response file.\" ), ) parser . add_argument ( \"--blacklist_file\" , type = str , default = \"\" , help = ( \"Optional file containing a list of features (one per line) to be excluded \" \"from the analysis.\" ), ) parser . add_argument ( \"--n_bootstraps\" , type = int , default = 1000 , help = \"Number of bootstrap samples to generate for resampling. Default is 1000\" , ) parser . add_argument ( \"--random_state\" , type = int , default = None , help = \"Set this to an integer to make the bootstrap sampling reproducible. \" \"Default is None (no fixed seed) and each call will produce different \" \"bootstrap indices. Note that if this is set, the `top_n` random_state will \" \"be +10 in order to make the top_n indices different from the `all_data` step\" , ) parser . add_argument ( \"--normalize_sample_weights\" , action = \"store_true\" , help = ( \"Set this to normalize the sample weights to sum to 1. \" \"Default is False.\" ), ) parser . add_argument ( \"--scale_by_std\" , action = \"store_true\" , help = ( \"Set this to center and scale the model matrix. Note that setting this \" \"will set the `fit_intercept` parameter of the LassoCV estimator to \" \"False.\" ), ) parser . add_argument ( \"--top_n\" , type = int , default = top_n_default , help = ( \"Number of features to retain in the second round of modeling. \" f \"Default is { top_n_default } \" ), )","title":"common_modeling_input_arguments"},{"location":"api/interface/#tfbpmodeling.interface.linear_perturbation_binding_modeling","text":"linear_perturbation_binding_modeling ( args ) Parameters: args \u2013 Command-line arguments containing input file paths and parameters. Source code in tfbpmodeling/interface.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 def linear_perturbation_binding_modeling ( args ): \"\"\" :param args: Command-line arguments containing input file paths and parameters. \"\"\" if not isinstance ( args . max_iter , int ) or args . max_iter < 1 : raise ValueError ( \"The `max_iter` parameter must be a positive integer.\" ) max_iter = int ( args . max_iter ) logger . info ( f \"estimator max_iter: { max_iter } .\" ) logger . info ( \"Step 1: Preprocessing\" ) # validate input files/dirs if not os . path . exists ( args . response_file ): raise FileNotFoundError ( f \"File { args . response_file } does not exist.\" ) if not os . path . exists ( args . predictors_file ): raise FileNotFoundError ( f \"File { args . predictors_file } does not exist.\" ) if os . path . exists ( args . output_dir ): logger . warning ( f \"Output directory { args . output_dir } already exists.\" ) else : os . makedirs ( args . output_dir , exist_ok = True ) logger . info ( f \"Output directory created at { args . output_dir } \" ) # the output subdir is where the output of this modeling run will be saved output_subdir = os . path . join ( args . output_dir , os . path . join ( args . perturbed_tf + args . output_suffix ) ) if os . path . exists ( output_subdir ): raise FileExistsError ( f \"Directory { output_subdir } already exists. \" \"Please specify a different `output_dir`.\" ) else : os . makedirs ( output_subdir , exist_ok = True ) logger . info ( f \"Output subdirectory created at { output_subdir } \" ) # instantiate a estimator # `fit_intercept` is set opposite of `scale_by_std`. If `scale_by_std` is `False`, # the default, then `fit_intercept` is set to True and the estimator will fit the # intercept. If `scale_by_std` is True, then the estimator will not fit the # intercept, meaning it assumes the data is centered. estimator = LassoCV ( fit_intercept = True , selection = \"random\" , n_alphas = 100 , random_state = 42 , n_jobs = args . n_cpus , max_iter = max_iter , ) input_data = ModelingInputData . from_files ( response_path = args . response_file , predictors_path = args . predictors_file , perturbed_tf = args . perturbed_tf , feature_blacklist_path = args . blacklist_file , top_n = args . top_n , ) logger . info ( \"Step 2: Bootstrap LassoCV on all data, full interactor model\" ) # Unset the top n masking -- we want to use all the data for the first round # modeling input_data . top_n_masked = False # extract a list of predictor variables, which are the columns of the predictors_df predictor_variables = input_data . predictors_df . columns . drop ( input_data . perturbed_tf ) # drop any variables which are in args.exclude_interactor_variables predictor_variables = exclude_predictor_variables ( list ( predictor_variables ), args . exclude_interactor_variables ) # create a list of interactor terms with the perturbed_tf as the first term interaction_terms = [ f \" { input_data . perturbed_tf } : { var } \" for var in predictor_variables ] # Construct the full interaction formula, ie perturbed_tf + perturbed_tf:other_tf1 + # perturbed_tf:other_tf2 + ... . perturbed_tf main effect only added if # --ptf_main_effect is passed. if args . ptf_main_effect : logger . info ( \"adding pTF main effect to `all_data_formula`\" ) all_data_formula = ( f \" { input_data . perturbed_tf } + { ' + ' . join ( interaction_terms ) } \" ) else : all_data_formula = \" + \" . join ( interaction_terms ) if args . squared_pTF : # if --squared_pTF is passed, then add the squared perturbed TF to the formula squared_term = f \"I( { input_data . perturbed_tf } ** 2)\" logger . info ( f \"Adding squared term to model formula: { squared_term } \" ) all_data_formula += f \" + { squared_term } \" if args . cubic_pTF : # if --cubic_pTF is passed, then add the cubic perturbed TF to the formula cubic_term = f \"I( { input_data . perturbed_tf } ** 3)\" logger . info ( f \"Add cubic term to model formula: { cubic_term } \" ) all_data_formula += f \" + { cubic_term } \" # if --row_max is passed, then add \"row_max\" to the formula if args . row_max : logger . info ( \"Adding `row_max` to the all data model formula\" ) all_data_formula += \" + row_max\" # if --add_model_variables is passed, then add the variables to the formula if args . add_model_variables : logger . info ( f \"Adding model variables to the all data model \" f \"formula: { args . add_model_variables } \" ) all_data_formula += \" + \" + \" + \" . join ( args . add_model_variables ) logger . debug ( f \"All data formula: { all_data_formula } \" ) # create the bootstrapped data. bootstrapped_data_all = BootstrappedModelingInputData ( response_df = input_data . response_df , model_df = input_data . get_modeling_data ( all_data_formula , add_row_max = args . row_max , drop_intercept = True , scale_by_std = args . scale_by_std , ), n_bootstraps = args . n_bootstraps , normalize_sample_weights = args . normalize_sample_weights , random_state = args . random_state , ) logger . info ( f \"Running bootstrap LassoCV on all data with { args . n_bootstraps } bootstraps\" ) if args . iterative_dropout : logger . info ( \"Using iterative dropout modeling for all data results.\" ) all_data_results = bootstrap_stratified_cv_loop ( bootstrapped_data = bootstrapped_data_all , perturbed_tf_series = input_data . predictors_df [ input_data . perturbed_tf ], estimator = estimator , ci_percentile = float ( args . all_data_ci_level ), stabilization_ci_start = args . stabilization_ci_start , bins = args . bins , output_dir = output_subdir , ) else : logger . info ( \"Using standard bootstrap modeling for all data results.\" ) all_data_results = bootstrap_stratified_cv_modeling ( bootstrapped_data = bootstrapped_data_all , perturbed_tf_series = input_data . predictors_df [ input_data . perturbed_tf ], estimator = estimator , ci_percentiles = [ float ( args . all_data_ci_level )], bins = args . bins , ) # create the all data object output subdir all_data_output = os . path . join ( output_subdir , \"all_data_result_object\" ) os . makedirs ( all_data_output , exist_ok = True ) logger . info ( f \"Serializing all data results to { all_data_output } \" ) all_data_results . serialize ( \"result_obj\" , all_data_output ) # Extract the coefficients that are significant at the specified confidence level all_data_sig_coefs = all_data_results . extract_significant_coefficients ( ci_level = args . all_data_ci_level , ) logger . info ( f \"all_data_sig_coefs: { all_data_sig_coefs } \" ) if not all_data_sig_coefs : logger . warning ( f \"No significant coefficients found at { args . all_data_ci_level } % \" \"confidence level. Exiting.\" ) return # write all_data_sig_coefs to a json file all_data_ci_str = str ( args . all_data_ci_level ) . replace ( \".\" , \"-\" ) all_data_output_file = os . path . join ( output_subdir , f \"all_data_significant_ { all_data_ci_str } .json\" ) logger . info ( f \"Writing the all data significant results to { all_data_output_file } \" ) with open ( all_data_output_file , \"w\" , ) as f : json . dump ( all_data_sig_coefs , f , indent = 4 ) logger . info ( \"Step 3: Running LassoCV on topn data with significant coefficients \" \"from the all data model\" ) # Create the formula for the topn modeling from the significant coefficients # NOTE: to remove the intercept, we need to add \" -1 \" topn_formula = f \" { ' + ' . join ( all_data_sig_coefs . keys ()) } \" logger . debug ( f \"Topn formula: { topn_formula } \" ) # apply the top_n masking input_data . top_n_masked = True # Create the bootstrapped data for the topn modeling bootstrapped_data_top_n = BootstrappedModelingInputData ( response_df = input_data . response_df , model_df = input_data . get_modeling_data ( topn_formula , add_row_max = args . row_max , drop_intercept = True , scale_by_std = args . scale_by_std , ), n_bootstraps = args . n_bootstraps , normalize_sample_weights = args . normalize_sample_weights , random_state = ( args . random_state + 10 if args . random_state else args . random_state ), ) logger . debug ( f \"Running bootstrap LassoCV on topn data with { args . n_bootstraps } bootstraps\" ) topn_results = bootstrap_stratified_cv_modeling ( bootstrapped_data_top_n , input_data . predictors_df [ input_data . perturbed_tf ], estimator = estimator , ci_percentiles = [ float ( args . topn_ci_level )], ) # create the topn data object output subdir topn_output = os . path . join ( output_subdir , \"topn_result_object\" ) os . makedirs ( topn_output , exist_ok = True ) logger . info ( f \"Serializing topn results to { topn_output } \" ) topn_results . serialize ( \"result_obj\" , topn_output ) # extract the topn_results at the specified confidence level topn_output_res = topn_results . extract_significant_coefficients ( ci_level = args . topn_ci_level ) logger . info ( f \"topn_output_res: { topn_output_res } \" ) if not topn_output_res : logger . warning ( f \"No significant coefficients found at { args . topn_ci_level } % \" \"confidence level. Exiting.\" ) return # write topn_output_res to a json file topn_ci_str = str ( args . topn_ci_level ) . replace ( \".\" , \"-\" ) topn_output_file = os . path . join ( output_subdir , f \"topn_significant_ { topn_ci_str } .json\" ) logger . info ( f \"Writing the topn significant results to { topn_output_file } \" ) with open ( topn_output_file , \"w\" ) as f : json . dump ( topn_output_res , f , indent = 4 ) logger . info ( \"Step 4: Test the significance of the interactor terms that survive \" \"against the corresoponding main effect\" ) if args . stage4_topn : logger . info ( \"Stage 4 will use top-n masked input data.\" ) input_data . top_n_masked = True else : logger . info ( \"Stage 4 will use full input data.\" ) # calculate the statification classes for the perturbed TF (all data) stage4_classes = stratification_classification ( input_data . predictors_df [ input_data . perturbed_tf ] . squeeze (), bins = args . bins , ) # Test the significance of the interactor terms evaluate_interactor_significance = ( evaluate_interactor_significance_lassocv if args . stage4_lasso else evaluate_interactor_significance_linear ) results = evaluate_interactor_significance ( input_data , stratification_classes = stage4_classes , model_variables = list ( topn_results . extract_significant_coefficients ( ci_level = args . topn_ci_level ) . keys () ), estimator = estimator , ) output_significance_file = os . path . join ( output_subdir , \"interactor_vs_main_result.json\" ) logger . info ( \"Writing the final interactor significance \" f \"results to { output_significance_file } \" ) results . serialize ( output_significance_file )","title":"linear_perturbation_binding_modeling"},{"location":"api/interface/#overview","text":"The interface module serves as the primary entry point for the tfbpmodeling workflow. It contains: Main workflow function : linear_perturbation_binding_modeling() CLI helper functions : Argument parsing utilities for the command-line interface Custom formatters : Enhanced help formatting for better user experience","title":"Overview"},{"location":"api/interface/#main-functions","text":"","title":"Main Functions"},{"location":"api/interface/#linear_perturbation_binding_modeling","text":"The core function that executes the complete 4-stage TFBP modeling workflow: Data Preprocessing : Load and validate input files, handle missing data Bootstrap Modeling : All-data analysis with bootstrap resampling and LassoCV Top-N Modeling : Refined analysis on significant predictors from top-performing data Interactor Significance : Statistical evaluation of interaction terms vs main effects Parameters : Command-line arguments object containing all configuration options Returns : None (results saved to output directory) Key Features : - Comprehensive input validation - Automatic output directory creation with timestamps - Detailed logging of all processing steps - Error handling with informative messages","title":"linear_perturbation_binding_modeling"},{"location":"api/interface/#cli-helper-functions","text":"","title":"CLI Helper Functions"},{"location":"api/interface/#common_modeling_input_arguments","text":"Adds standard input arguments to argument parsers: - File paths for response and predictor data - Perturbed TF specification - Bootstrap and sampling parameters","title":"common_modeling_input_arguments"},{"location":"api/interface/#common_modeling_feature_options","text":"Configures feature engineering options: - Polynomial terms (squared, cubic) - Row maximum inclusion - Custom variable additions and exclusions","title":"common_modeling_feature_options"},{"location":"api/interface/#common_modeling_binning_arguments","text":"Sets up data stratification parameters: - Bin edge specifications - Stratification methods","title":"common_modeling_binning_arguments"},{"location":"api/interface/#add_general_arguments_to_subparsers","text":"Propagates global arguments to subcommand parsers: - Logging configuration - System-wide options","title":"add_general_arguments_to_subparsers"},{"location":"api/interface/#data-flow","text":"graph TD A[CLI Arguments] --> B[Input Validation] B --> C[Data Loading] C --> D[ModelingInputData] D --> E[BootstrappedModelingInputData] E --> F[Bootstrap CV Loop] F --> G[Top-N Selection] G --> H[Interactor Significance] H --> I[Results Output]","title":"Data Flow"},{"location":"api/interface/#usage-examples","text":"","title":"Usage Examples"},{"location":"api/interface/#programmatic-usage","text":"import argparse from tfbpmodeling.interface import linear_perturbation_binding_modeling # Create arguments object args = argparse . Namespace ( response_file = 'data/expression.csv' , predictors_file = 'data/binding.csv' , perturbed_tf = 'YPD1' , n_bootstraps = 1000 , top_n = 600 , all_data_ci_level = 98.0 , topn_ci_level = 90.0 , max_iter = 10000 , output_dir = './results' , output_suffix = '' , n_cpus = 4 , # ... other parameters ) # Run analysis linear_perturbation_binding_modeling ( args )","title":"Programmatic Usage"},{"location":"api/interface/#custom-argument-parser","text":"import argparse from tfbpmodeling.interface import ( common_modeling_input_arguments , common_modeling_feature_options , CustomHelpFormatter ) # Create custom parser parser = argparse . ArgumentParser ( formatter_class = CustomHelpFormatter , description = \"Custom TFBP Analysis\" ) # Add standard arguments input_group = parser . add_argument_group ( \"Input\" ) common_modeling_input_arguments ( input_group ) feature_group = parser . add_argument_group ( \"Features\" ) common_modeling_feature_options ( feature_group ) # Parse and use args = parser . parse_args () linear_perturbation_binding_modeling ( args )","title":"Custom Argument Parser"},{"location":"api/interface/#error-handling","text":"The interface module includes comprehensive error handling:","title":"Error Handling"},{"location":"api/interface/#input-validation-errors","text":"# File existence checks FileNotFoundError : \"File data/missing.csv does not exist.\" # Parameter validation ValueError : \"The `max_iter` parameter must be a positive integer.\" # Data format validation ValueError : \"Perturbed TF 'INVALID' not found in response file columns\"","title":"Input Validation Errors"},{"location":"api/interface/#runtime-errors","text":"# Convergence issues RuntimeWarning : \"LassoCV failed to converge for 15/1000 bootstrap samples\" # Insufficient data ValueError : \"Insufficient data after filtering. Found 5 samples, minimum required: 10\"","title":"Runtime Errors"},{"location":"api/interface/#configuration-options","text":"The interface supports extensive configuration through command-line arguments:","title":"Configuration Options"},{"location":"api/interface/#core-parameters","text":"Input files : Response data, predictor data, optional blacklist TF specification : Name of perturbed transcription factor Bootstrap settings : Sample count, random seed, weight normalization","title":"Core Parameters"},{"location":"api/interface/#feature-engineering","text":"Polynomial terms : Squared and cubic pTF terms Additional predictors : Row max, custom variables Interaction control : Variable exclusions, main effects","title":"Feature Engineering"},{"location":"api/interface/#model-configuration","text":"Confidence intervals : Separate thresholds for each stage Convergence : Maximum iterations, dropout options Performance : CPU cores, memory management","title":"Model Configuration"},{"location":"api/interface/#output-control","text":"Directory structure : Base directory, custom suffixes Logging : Verbosity levels, file vs console output","title":"Output Control"},{"location":"api/interface/#performance-considerations","text":"","title":"Performance Considerations"},{"location":"api/interface/#memory-management","text":"Bootstrap samples stored efficiently using sparse representations Automatic garbage collection between stages Memory usage monitoring and warnings","title":"Memory Management"},{"location":"api/interface/#parallel-processing","text":"LassoCV uses specified CPU cores for cross-validation Bootstrap samples processed in batches I/O operations optimized for large datasets","title":"Parallel Processing"},{"location":"api/interface/#runtime-optimization","text":"Early stopping for non-convergent models Adaptive batch sizing based on available memory Progress reporting for long-running analyses","title":"Runtime Optimization"},{"location":"api/interface/#related-modules","text":"modeling_input_data : Core data structures bootstrapped_input_data : Bootstrap resampling bootstrap_model_results : Result aggregation evaluate_interactor_significance_lassocv : LassoCV-based significance testing evaluate_interactor_significance_linear : Linear regression-based significance testing","title":"Related Modules"},{"location":"api/modeling_input_data/","text":"modeling_input_data \u00b6 Core data structures for handling input data preprocessing and validation in tfbpmodeling. tfbpmodeling.modeling_input_data \u00b6 ModelingInputData \u00b6 ModelingInputData ( response_df , predictors_df , perturbed_tf , feature_col = \"target_symbol\" , feature_blacklist = None , top_n = None , ) Container for response and predictor data used in modeling transcription factor perturbation experiments. This class handles: - Validation and synchronization of response and predictor DataFrames based on a shared feature identifier. - Optional blacklisting of features, including the perturbed transcription factor. - Optional feature selection based on the top N strongest binding signals (as ranked from a specific TF column in the predictor matrix). - Application of masking logic to restrict modeling to selected features. Initialize ModelingInputData with response and predictor matrices. Note that the response and predictor dataframes will be subset down to the features in common between them, by index. The rows in both dataframes will also be ordered such that they match, again by index. Parameters: response_df ( DataFrame ) \u2013 A two column DataFrame containing the feature_col and numeric column representing the response variable. predictors_df ( DataFrame ) \u2013 A Dataframe containing the feature_col and predictor numeric columns that represent the predictor variables. perturbed_tf ( str ) \u2013 Name of the perturbed TF. Note : this must exist as a column in predictors_df. feature_col ( str , default: 'target_symbol' ) \u2013 Name of the column to use as the feature index. This column must exist in both the response and predictor DataFrames. (default: \"target_symbol\"). feature_blacklist ( list [ str ] | None , default: None ) \u2013 List of feature names to exclude from analysis. top_n ( int | None , default: None ) \u2013 If specified, retain only the top N features with the strongest binding scores for the perturbed TF. If this is passed on initialization, then the top_n_masked is set to True by default. If you wish to extract unmasked data, you can set object.top_n_masked = False . The mask can be toggled on and off at will. Source code in tfbpmodeling/modeling_input_data.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def __init__ ( self , response_df : pd . DataFrame , predictors_df : pd . DataFrame , perturbed_tf : str , feature_col : str = \"target_symbol\" , feature_blacklist : list [ str ] | None = None , top_n : int | None = None , ): \"\"\" Initialize ModelingInputData with response and predictor matrices. Note that the response and predictor dataframes will be subset down to the features in common between them, by index. The rows in both dataframes will also be ordered such that they match, again by index. :param response_df: A two column DataFrame containing the `feature_col` and numeric column representing the response variable. :param predictors_df: A Dataframe containing the `feature_col` and predictor numeric columns that represent the predictor variables. :param perturbed_tf: Name of the perturbed TF. **Note**: this must exist as a column in predictors_df. :param feature_col: Name of the column to use as the feature index. This column must exist in both the response and predictor DataFrames. (default: \"target_symbol\"). :param feature_blacklist: List of feature names to exclude from analysis. :param top_n: If specified, retain only the top N features with the strongest binding scores for the perturbed TF. If this is passed on initialization, then the top_n_masked is set to True by default. If you wish to extract unmasked data, you can set `object.top_n_masked = False`. The mask can be toggled on and off at will. \"\"\" if not isinstance ( response_df , pd . DataFrame ): raise ValueError ( \"response_df must be a DataFrame.\" ) if not isinstance ( predictors_df , pd . DataFrame ): raise ValueError ( \"predictors_df must be a DataFrame.\" ) if not isinstance ( perturbed_tf , str ): raise ValueError ( \"perturbed_tf must be a string representing the TF name.\" ) if not isinstance ( feature_col , str ): raise ValueError ( \"feature_col must be a string representing the feature name.\" ) if feature_blacklist is not None and not isinstance ( feature_blacklist , list ): raise ValueError ( \"feature_blacklist must be a list or None.\" ) if top_n is not None and not isinstance ( top_n , int ): raise ValueError ( \"top_n must be an integer or None.\" ) self . perturbed_tf = perturbed_tf self . feature_col = feature_col self . _top_n_masked = False # Ensure feature_blacklist is a list if feature_blacklist is None : feature_blacklist = [] # Ensure perturbed_tf is in the blacklist if perturbed_tf not in feature_blacklist : logger . warning ( f \"Perturbed TF ' { perturbed_tf } ' not in blacklist. \" f \"Adding to blacklist. Setting blacklist_masked to True. \" f \"If you do not wish to blacklist the perturbed TF, \" f \"set blacklist_masked to False.\" ) feature_blacklist . append ( perturbed_tf ) self . feature_blacklist = set ( feature_blacklist ) self . blacklist_masked = bool ( self . feature_blacklist ) # Ensure the response and predictors only contain common features self . response_df = response_df self . predictors_df = predictors_df # Assign top_n value self . top_n = top_n predictors_df property writable \u00b6 predictors_df Get the predictors DataFrame with feature masks applied. The returned DataFrame reflects any active blacklist or top-N filtering. response_df property writable \u00b6 response_df Get the response DataFrame with feature masks applied. Returns a version of the response matrix filtered by: - Feature blacklist (if blacklist_masked is True) - Top-N feature selection (if top_n_masked is True) The final DataFrame will be aligned in index order with the predictors matrix. Returns: DataFrame \u2013 Filtered and ordered response DataFrame. top_n property writable \u00b6 top_n Get the threshold for top-ranked feature selection. If set to an integer, this defines how many of the highest-ranked features (based on predictors_df[self.perturbed_tf] ) should be retained. Ranking is descending (higher values rank higher). If the cutoff falls on a tie, fewer than N features may be selected to preserve a consistent threshold. The most impactful tie is when the majority of the lower ranked features have the same value, eg an enrichment of 0 or pvalue of 1.0. If set to None, top-N feature selection is disabled. Note: Whether top-N filtering is actively applied depends on the top_n_masked attribute. You can set top_n_masked = False to access the unfiltered data, even if top_n is set. Returns: int | None \u2013 The current top-N threshold or None. top_n_masked property writable \u00b6 top_n_masked Get the status of top-n feature masking. If this is True , then the top-n feature selection is applied to the predictors and response from_files classmethod \u00b6 from_files ( response_path , predictors_path , perturbed_tf , feature_col = \"target_symbol\" , feature_blacklist_path = None , top_n = 600 , ) Load response and predictor data from files. This would be considered an overloaded constructor in other languages. The input files must be able to be read into objects that satisfy the init method -- see init docs. Parameters: response_path ( str ) \u2013 Path to the response file (CSV). predictors_path ( str ) \u2013 Path to the predictors file (CSV). perturbed_tf ( str ) \u2013 The perturbed TF. feature_col ( str , default: 'target_symbol' ) \u2013 The column name representing features. feature_blacklist_path ( str | None , default: None ) \u2013 Path to a file containing a list of features to exclude. top_n ( int , default: 600 ) \u2013 Maximum number of features for top-n selection. Returns: ModelingInputData \u2013 An instance of ModelingInputData. Raises: FileNotFoundError \u2013 If the response or predictor files are missing. Source code in tfbpmodeling/modeling_input_data.py 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 @classmethod def from_files ( cls , response_path : str , predictors_path : str , perturbed_tf : str , feature_col : str = \"target_symbol\" , feature_blacklist_path : str | None = None , top_n : int = 600 , ) -> \"ModelingInputData\" : \"\"\" Load response and predictor data from files. This would be considered an overloaded constructor in other languages. The input files must be able to be read into objects that satisfy the __init__ method -- see __init__ docs. :param response_path: Path to the response file (CSV). :param predictors_path: Path to the predictors file (CSV). :param perturbed_tf: The perturbed TF. :param feature_col: The column name representing features. :param feature_blacklist_path: Path to a file containing a list of features to exclude. :param top_n: Maximum number of features for top-n selection. :return: An instance of ModelingInputData. :raises FileNotFoundError: If the response or predictor files are missing. \"\"\" if not os . path . exists ( response_path ): raise FileNotFoundError ( f \"Response file ' { response_path } ' does not exist.\" ) if not os . path . exists ( predictors_path ): raise FileNotFoundError ( f \"Predictors file ' { predictors_path } ' does not exist.\" ) response_df = pd . read_csv ( response_path ) predictors_df = pd . read_csv ( predictors_path ) # Load feature blacklist if provided feature_blacklist : list [ str ] = [] if feature_blacklist_path : if not os . path . exists ( feature_blacklist_path ): raise FileNotFoundError ( f \"Feature blacklist file ' { feature_blacklist_path } ' does not exist.\" ) with open ( feature_blacklist_path ) as f : feature_blacklist = [ line . strip () for line in f if line . strip ()] return cls ( response_df , predictors_df , perturbed_tf , feature_col , feature_blacklist , top_n , ) get_modeling_data \u00b6 get_modeling_data ( formula , add_row_max = False , drop_intercept = False , scale_by_std = False , ) Get the predictors for modeling, optionally adding a row-wise max feature. Parameters: formula ( str ) \u2013 The formula to use for modeling. add_row_max ( bool , default: False ) \u2013 Whether to add a row-wise max feature to the predictors. drop_intercept ( bool , default: False ) \u2013 If drop_intercept is True, \"-1\" will be appended to the formula string. This will drop the intercept (constant) term from the model matrix output by patsy.dmatrix. Default is False . Note that if this is False , but scale_by_std is True , then the StandardScaler with_mean = False and the data is only scaled, not centered. scale_by_std ( bool , default: False ) \u2013 If True, apply sklearn StandardScaler after design matrix creation. Returns: DataFrame \u2013 The design matrix for modeling. self.response_df can be used for the response variable. Raises: ValueError \u2013 If the formula is not provided PatsyError \u2013 If there is an error in creating the model matrix Source code in tfbpmodeling/modeling_input_data.py 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 def get_modeling_data ( self , formula : str , add_row_max : bool = False , drop_intercept : bool = False , scale_by_std : bool = False , ) -> pd . DataFrame : \"\"\" Get the predictors for modeling, optionally adding a row-wise max feature. :param formula: The formula to use for modeling. :param add_row_max: Whether to add a row-wise max feature to the predictors. :param drop_intercept: If `drop_intercept` is True, \"-1\" will be appended to the formula string. This will drop the intercept (constant) term from the model matrix output by patsy.dmatrix. Default is `False`. Note that if this is `False`, but `scale_by_std` is `True`, then the StandardScaler `with_mean = False` and the data is only scaled, not centered. :param scale_by_std: If True, apply sklearn StandardScaler after design matrix creation. :return: The design matrix for modeling. self.response_df can be used for the response variable. :raises ValueError: If the formula is not provided :raises PatsyError: If there is an error in creating the model matrix \"\"\" if not formula : raise ValueError ( \"Formula must be provided for modeling.\" ) if drop_intercept : logger . info ( \"Dropping intercept from the patsy model matrix\" ) formula += \" - 1\" predictors_df = self . predictors_df # Apply top-n feature mask # Add row-wise max feature if requested if add_row_max : predictors_df [ \"row_max\" ] = predictors_df . max ( axis = 1 ) # Create a design matrix using patsy try : design_matrix = dmatrix ( formula , data = predictors_df , return_type = \"dataframe\" , NA_action = \"raise\" , ) except PatsyError as exc : logger . error ( f \"Error in creating model matrix with formula ' { formula } ': { exc } \" ) raise if scale_by_std : logger . info ( \"Center matrix = `False`. Scale matrix = `True`\" ) scaler = StandardScaler ( with_mean = False , with_std = True ) scaled_values = scaler . fit_transform ( design_matrix ) design_matrix = pd . DataFrame ( scaled_values , index = design_matrix . index , columns = design_matrix . columns ) logger . info ( f \"Design matrix columns: { list ( design_matrix . columns ) } \" ) return design_matrix Overview \u00b6 The modeling_input_data module provides the fundamental ModelingInputData class that handles: Data loading : Reading CSV files for response and predictor data Validation : Ensuring data consistency and format compliance Preprocessing : Feature filtering, normalization, and transformation Integration : Merging response and predictor data for modeling This class serves as the foundation for all downstream modeling operations. Core Classes \u00b6 ModelingInputData \u00b6 The primary class for managing input data throughout the modeling workflow. Key Features \u00b6 Automatic data validation : Checks file formats, column consistency, and data types Feature filtering : Removes blacklisted genes and handles missing data Index alignment : Ensures consistent gene identifiers between response and predictor files Data integration : Combines multiple data sources into modeling-ready format Initialization \u00b6 from tfbpmodeling.modeling_input_data import ModelingInputData # Basic initialization data = ModelingInputData ( response_file = 'expression.csv' , predictors_file = 'binding.csv' , perturbed_tf = 'YPD1' ) # With optional parameters data = ModelingInputData ( response_file = 'expression.csv' , predictors_file = 'binding.csv' , perturbed_tf = 'YPD1' , blacklist_file = 'exclude_genes.txt' , normalize_weights = True , scale_by_std = True ) Key Methods \u00b6 Data Loading and Validation \u00b6 load_data() : Read and validate input CSV files validate_format() : Check data format compliance check_consistency() : Verify response-predictor alignment Data Preprocessing \u00b6 filter_features() : Remove blacklisted and invalid features normalize_data() : Apply scaling and normalization handle_missing() : Deal with missing values appropriately Data Access \u00b6 get_response_data() : Access processed response data get_predictor_data() : Access processed predictor data get_feature_names() : Retrieve feature identifiers get_sample_names() : Retrieve sample identifiers Data Format Requirements \u00b6 Response File Format \u00b6 The response file must be a CSV with specific structure: gene_id,sample1,sample2,sample3,sample4 YPD1,0.23,-1.45,0.87,-0.12 YBR123W,1.34,0.56,-0.23,0.78 YCR456X,-0.45,0.12,1.23,-0.56 Requirements : - First column: Gene identifiers (must match predictor file) - Subsequent columns: Numeric expression values - Column names: Sample identifiers - Must contain column matching perturbed_tf parameter Predictor File Format \u00b6 The predictor file structure: gene_id,TF1,TF2,TF3,TF4 YPD1,0.34,0.12,0.78,0.01 YBR123W,0.89,0.45,0.23,0.67 YCR456X,0.12,0.78,0.34,0.90 Requirements : - First column: Gene identifiers (must match response file) - Subsequent columns: Numeric binding values - Column names: Transcription factor identifiers - All values must be numeric (no missing values in binding data) Blacklist File Format \u00b6 Optional exclusion file: YBR999W YCR888X control_gene technical_artifact Requirements : - Plain text file - One gene identifier per line - Gene IDs must match those in data files - Comments not supported Usage Examples \u00b6 Basic Data Loading \u00b6 from tfbpmodeling.modeling_input_data import ModelingInputData # Load data with minimal configuration data = ModelingInputData ( response_file = 'data/expression.csv' , predictors_file = 'data/binding.csv' , perturbed_tf = 'YPD1' ) # Access processed data response_data = data . get_response_data () predictor_data = data . get_predictor_data () feature_names = data . get_feature_names () print ( f \"Loaded { len ( feature_names ) } features\" ) print ( f \"Response data shape: { response_data . shape } \" ) print ( f \"Predictor data shape: { predictor_data . shape } \" ) Data Preprocessing Options \u00b6 # Advanced preprocessing data = ModelingInputData ( response_file = 'data/expression.csv' , predictors_file = 'data/binding.csv' , perturbed_tf = 'YPD1' , blacklist_file = 'data/exclude_genes.txt' , normalize_weights = True , scale_by_std = True , handle_missing = 'drop' # or 'impute', 'zero' ) # Check data quality print ( f \"Original features: { data . original_feature_count } \" ) print ( f \"Filtered features: { len ( data . get_feature_names ()) } \" ) print ( f \"Excluded features: { data . excluded_feature_count } \" ) Integration with Modeling Pipeline \u00b6 # Prepare data for bootstrap modeling from tfbpmodeling.bootstrapped_input_data import BootstrappedModelingInputData # Base data base_data = ModelingInputData ( response_file = 'expression.csv' , predictors_file = 'binding.csv' , perturbed_tf = 'YPD1' ) # Create bootstrap version bootstrap_data = BootstrappedModelingInputData ( base_data = base_data , n_bootstraps = 1000 , random_state = 42 ) Data Validation \u00b6 Automatic Checks \u00b6 The class performs comprehensive validation: # File existence and readability assert os . path . exists ( response_file ), f \"Response file not found: { response_file } \" assert os . path . exists ( predictors_file ), f \"Predictor file not found: { predictors_file } \" # Data format validation assert response_df . index . equals ( predictor_df . index ), \"Gene indices must match\" assert perturbed_tf in response_df . columns , f \"Perturbed TF ' { perturbed_tf } ' not found\" # Data type validation assert response_df . dtypes . apply ( lambda x : np . issubdtype ( x , np . number )) . all () assert predictor_df . dtypes . apply ( lambda x : np . issubdtype ( x , np . number )) . all () Custom Validation \u00b6 # Add custom validation rules def validate_expression_range ( data ): \\ \" \\\"\\\" Ensure expression values are in reasonable range \\\"\\\"\\\" assert data . abs () . max () . max () < 10 , \"Expression values seem too large\" def validate_binding_range ( data ): \\ \" \\\"\\\" Ensure binding values are probabilities \\\"\\\"\\\" assert ( data >= 0 ) . all () . all (), \"Binding values must be non-negative\" assert ( data <= 1 ) . all () . all (), \"Binding values must be <= 1\" # Apply custom validation data = ModelingInputData ( response_file = 'expression.csv' , predictors_file = 'binding.csv' , perturbed_tf = 'YPD1' , custom_validators = [ validate_expression_range , validate_binding_range ] ) Error Handling \u00b6 Common Errors and Solutions \u00b6 File Format Errors \u00b6 # CSV parsing errors try : data = ModelingInputData ( response_file = 'malformed.csv' , ... ) except pd . errors . ParserError as e : print ( f \"CSV format error: { e } \" ) # Solution: Check file encoding, delimiters, quotes Data Consistency Errors \u00b6 # Mismatched gene indices try : data = ModelingInputData ( ... ) except ValueError as e : if \"Gene indices must match\" in str ( e ): print ( \"Response and predictor files have different gene sets\" ) # Solution: Align gene lists or use intersection Missing Data Errors \u00b6 # Perturbed TF not found try : data = ModelingInputData ( perturbed_tf = 'MISSING_TF' , ... ) except KeyError as e : print ( f \"Perturbed TF not found in response data: { e } \" ) # Solution: Check TF name spelling, verify column names Performance Considerations \u00b6 Memory Management \u00b6 Large datasets loaded using chunked reading Unnecessary columns dropped early in processing Memory-efficient data types selected automatically I/O Optimization \u00b6 CSV reading optimized with appropriate engines Caching of preprocessed data for repeated access Lazy loading of optional data components Data Processing \u00b6 Vectorized operations for filtering and transformation Efficient indexing for data alignment Minimal data copying during preprocessing Related Classes \u00b6 BootstrappedModelingInputData : Bootstrap sampling extension BootstrapModelResults : Results storage and aggregation StratifiedCV : Cross-validation data handling Integration Points \u00b6 The ModelingInputData class integrates with: CLI Interface : Receives parameters from command-line arguments Bootstrap Modeling : Provides base data for resampling Feature Engineering : Supplies data for polynomial and interaction terms Cross-Validation : Furnishes stratified sampling input Results Output : Delivers metadata for result interpretation","title":"modeling_input_data"},{"location":"api/modeling_input_data/#modeling_input_data","text":"Core data structures for handling input data preprocessing and validation in tfbpmodeling.","title":"modeling_input_data"},{"location":"api/modeling_input_data/#tfbpmodeling.modeling_input_data","text":"","title":"modeling_input_data"},{"location":"api/modeling_input_data/#tfbpmodeling.modeling_input_data.ModelingInputData","text":"ModelingInputData ( response_df , predictors_df , perturbed_tf , feature_col = \"target_symbol\" , feature_blacklist = None , top_n = None , ) Container for response and predictor data used in modeling transcription factor perturbation experiments. This class handles: - Validation and synchronization of response and predictor DataFrames based on a shared feature identifier. - Optional blacklisting of features, including the perturbed transcription factor. - Optional feature selection based on the top N strongest binding signals (as ranked from a specific TF column in the predictor matrix). - Application of masking logic to restrict modeling to selected features. Initialize ModelingInputData with response and predictor matrices. Note that the response and predictor dataframes will be subset down to the features in common between them, by index. The rows in both dataframes will also be ordered such that they match, again by index. Parameters: response_df ( DataFrame ) \u2013 A two column DataFrame containing the feature_col and numeric column representing the response variable. predictors_df ( DataFrame ) \u2013 A Dataframe containing the feature_col and predictor numeric columns that represent the predictor variables. perturbed_tf ( str ) \u2013 Name of the perturbed TF. Note : this must exist as a column in predictors_df. feature_col ( str , default: 'target_symbol' ) \u2013 Name of the column to use as the feature index. This column must exist in both the response and predictor DataFrames. (default: \"target_symbol\"). feature_blacklist ( list [ str ] | None , default: None ) \u2013 List of feature names to exclude from analysis. top_n ( int | None , default: None ) \u2013 If specified, retain only the top N features with the strongest binding scores for the perturbed TF. If this is passed on initialization, then the top_n_masked is set to True by default. If you wish to extract unmasked data, you can set object.top_n_masked = False . The mask can be toggled on and off at will. Source code in tfbpmodeling/modeling_input_data.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def __init__ ( self , response_df : pd . DataFrame , predictors_df : pd . DataFrame , perturbed_tf : str , feature_col : str = \"target_symbol\" , feature_blacklist : list [ str ] | None = None , top_n : int | None = None , ): \"\"\" Initialize ModelingInputData with response and predictor matrices. Note that the response and predictor dataframes will be subset down to the features in common between them, by index. The rows in both dataframes will also be ordered such that they match, again by index. :param response_df: A two column DataFrame containing the `feature_col` and numeric column representing the response variable. :param predictors_df: A Dataframe containing the `feature_col` and predictor numeric columns that represent the predictor variables. :param perturbed_tf: Name of the perturbed TF. **Note**: this must exist as a column in predictors_df. :param feature_col: Name of the column to use as the feature index. This column must exist in both the response and predictor DataFrames. (default: \"target_symbol\"). :param feature_blacklist: List of feature names to exclude from analysis. :param top_n: If specified, retain only the top N features with the strongest binding scores for the perturbed TF. If this is passed on initialization, then the top_n_masked is set to True by default. If you wish to extract unmasked data, you can set `object.top_n_masked = False`. The mask can be toggled on and off at will. \"\"\" if not isinstance ( response_df , pd . DataFrame ): raise ValueError ( \"response_df must be a DataFrame.\" ) if not isinstance ( predictors_df , pd . DataFrame ): raise ValueError ( \"predictors_df must be a DataFrame.\" ) if not isinstance ( perturbed_tf , str ): raise ValueError ( \"perturbed_tf must be a string representing the TF name.\" ) if not isinstance ( feature_col , str ): raise ValueError ( \"feature_col must be a string representing the feature name.\" ) if feature_blacklist is not None and not isinstance ( feature_blacklist , list ): raise ValueError ( \"feature_blacklist must be a list or None.\" ) if top_n is not None and not isinstance ( top_n , int ): raise ValueError ( \"top_n must be an integer or None.\" ) self . perturbed_tf = perturbed_tf self . feature_col = feature_col self . _top_n_masked = False # Ensure feature_blacklist is a list if feature_blacklist is None : feature_blacklist = [] # Ensure perturbed_tf is in the blacklist if perturbed_tf not in feature_blacklist : logger . warning ( f \"Perturbed TF ' { perturbed_tf } ' not in blacklist. \" f \"Adding to blacklist. Setting blacklist_masked to True. \" f \"If you do not wish to blacklist the perturbed TF, \" f \"set blacklist_masked to False.\" ) feature_blacklist . append ( perturbed_tf ) self . feature_blacklist = set ( feature_blacklist ) self . blacklist_masked = bool ( self . feature_blacklist ) # Ensure the response and predictors only contain common features self . response_df = response_df self . predictors_df = predictors_df # Assign top_n value self . top_n = top_n","title":"ModelingInputData"},{"location":"api/modeling_input_data/#tfbpmodeling.modeling_input_data.ModelingInputData.predictors_df","text":"predictors_df Get the predictors DataFrame with feature masks applied. The returned DataFrame reflects any active blacklist or top-N filtering.","title":"predictors_df"},{"location":"api/modeling_input_data/#tfbpmodeling.modeling_input_data.ModelingInputData.response_df","text":"response_df Get the response DataFrame with feature masks applied. Returns a version of the response matrix filtered by: - Feature blacklist (if blacklist_masked is True) - Top-N feature selection (if top_n_masked is True) The final DataFrame will be aligned in index order with the predictors matrix. Returns: DataFrame \u2013 Filtered and ordered response DataFrame.","title":"response_df"},{"location":"api/modeling_input_data/#tfbpmodeling.modeling_input_data.ModelingInputData.top_n","text":"top_n Get the threshold for top-ranked feature selection. If set to an integer, this defines how many of the highest-ranked features (based on predictors_df[self.perturbed_tf] ) should be retained. Ranking is descending (higher values rank higher). If the cutoff falls on a tie, fewer than N features may be selected to preserve a consistent threshold. The most impactful tie is when the majority of the lower ranked features have the same value, eg an enrichment of 0 or pvalue of 1.0. If set to None, top-N feature selection is disabled. Note: Whether top-N filtering is actively applied depends on the top_n_masked attribute. You can set top_n_masked = False to access the unfiltered data, even if top_n is set. Returns: int | None \u2013 The current top-N threshold or None.","title":"top_n"},{"location":"api/modeling_input_data/#tfbpmodeling.modeling_input_data.ModelingInputData.top_n_masked","text":"top_n_masked Get the status of top-n feature masking. If this is True , then the top-n feature selection is applied to the predictors and response","title":"top_n_masked"},{"location":"api/modeling_input_data/#tfbpmodeling.modeling_input_data.ModelingInputData.from_files","text":"from_files ( response_path , predictors_path , perturbed_tf , feature_col = \"target_symbol\" , feature_blacklist_path = None , top_n = 600 , ) Load response and predictor data from files. This would be considered an overloaded constructor in other languages. The input files must be able to be read into objects that satisfy the init method -- see init docs. Parameters: response_path ( str ) \u2013 Path to the response file (CSV). predictors_path ( str ) \u2013 Path to the predictors file (CSV). perturbed_tf ( str ) \u2013 The perturbed TF. feature_col ( str , default: 'target_symbol' ) \u2013 The column name representing features. feature_blacklist_path ( str | None , default: None ) \u2013 Path to a file containing a list of features to exclude. top_n ( int , default: 600 ) \u2013 Maximum number of features for top-n selection. Returns: ModelingInputData \u2013 An instance of ModelingInputData. Raises: FileNotFoundError \u2013 If the response or predictor files are missing. Source code in tfbpmodeling/modeling_input_data.py 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 @classmethod def from_files ( cls , response_path : str , predictors_path : str , perturbed_tf : str , feature_col : str = \"target_symbol\" , feature_blacklist_path : str | None = None , top_n : int = 600 , ) -> \"ModelingInputData\" : \"\"\" Load response and predictor data from files. This would be considered an overloaded constructor in other languages. The input files must be able to be read into objects that satisfy the __init__ method -- see __init__ docs. :param response_path: Path to the response file (CSV). :param predictors_path: Path to the predictors file (CSV). :param perturbed_tf: The perturbed TF. :param feature_col: The column name representing features. :param feature_blacklist_path: Path to a file containing a list of features to exclude. :param top_n: Maximum number of features for top-n selection. :return: An instance of ModelingInputData. :raises FileNotFoundError: If the response or predictor files are missing. \"\"\" if not os . path . exists ( response_path ): raise FileNotFoundError ( f \"Response file ' { response_path } ' does not exist.\" ) if not os . path . exists ( predictors_path ): raise FileNotFoundError ( f \"Predictors file ' { predictors_path } ' does not exist.\" ) response_df = pd . read_csv ( response_path ) predictors_df = pd . read_csv ( predictors_path ) # Load feature blacklist if provided feature_blacklist : list [ str ] = [] if feature_blacklist_path : if not os . path . exists ( feature_blacklist_path ): raise FileNotFoundError ( f \"Feature blacklist file ' { feature_blacklist_path } ' does not exist.\" ) with open ( feature_blacklist_path ) as f : feature_blacklist = [ line . strip () for line in f if line . strip ()] return cls ( response_df , predictors_df , perturbed_tf , feature_col , feature_blacklist , top_n , )","title":"from_files"},{"location":"api/modeling_input_data/#tfbpmodeling.modeling_input_data.ModelingInputData.get_modeling_data","text":"get_modeling_data ( formula , add_row_max = False , drop_intercept = False , scale_by_std = False , ) Get the predictors for modeling, optionally adding a row-wise max feature. Parameters: formula ( str ) \u2013 The formula to use for modeling. add_row_max ( bool , default: False ) \u2013 Whether to add a row-wise max feature to the predictors. drop_intercept ( bool , default: False ) \u2013 If drop_intercept is True, \"-1\" will be appended to the formula string. This will drop the intercept (constant) term from the model matrix output by patsy.dmatrix. Default is False . Note that if this is False , but scale_by_std is True , then the StandardScaler with_mean = False and the data is only scaled, not centered. scale_by_std ( bool , default: False ) \u2013 If True, apply sklearn StandardScaler after design matrix creation. Returns: DataFrame \u2013 The design matrix for modeling. self.response_df can be used for the response variable. Raises: ValueError \u2013 If the formula is not provided PatsyError \u2013 If there is an error in creating the model matrix Source code in tfbpmodeling/modeling_input_data.py 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 def get_modeling_data ( self , formula : str , add_row_max : bool = False , drop_intercept : bool = False , scale_by_std : bool = False , ) -> pd . DataFrame : \"\"\" Get the predictors for modeling, optionally adding a row-wise max feature. :param formula: The formula to use for modeling. :param add_row_max: Whether to add a row-wise max feature to the predictors. :param drop_intercept: If `drop_intercept` is True, \"-1\" will be appended to the formula string. This will drop the intercept (constant) term from the model matrix output by patsy.dmatrix. Default is `False`. Note that if this is `False`, but `scale_by_std` is `True`, then the StandardScaler `with_mean = False` and the data is only scaled, not centered. :param scale_by_std: If True, apply sklearn StandardScaler after design matrix creation. :return: The design matrix for modeling. self.response_df can be used for the response variable. :raises ValueError: If the formula is not provided :raises PatsyError: If there is an error in creating the model matrix \"\"\" if not formula : raise ValueError ( \"Formula must be provided for modeling.\" ) if drop_intercept : logger . info ( \"Dropping intercept from the patsy model matrix\" ) formula += \" - 1\" predictors_df = self . predictors_df # Apply top-n feature mask # Add row-wise max feature if requested if add_row_max : predictors_df [ \"row_max\" ] = predictors_df . max ( axis = 1 ) # Create a design matrix using patsy try : design_matrix = dmatrix ( formula , data = predictors_df , return_type = \"dataframe\" , NA_action = \"raise\" , ) except PatsyError as exc : logger . error ( f \"Error in creating model matrix with formula ' { formula } ': { exc } \" ) raise if scale_by_std : logger . info ( \"Center matrix = `False`. Scale matrix = `True`\" ) scaler = StandardScaler ( with_mean = False , with_std = True ) scaled_values = scaler . fit_transform ( design_matrix ) design_matrix = pd . DataFrame ( scaled_values , index = design_matrix . index , columns = design_matrix . columns ) logger . info ( f \"Design matrix columns: { list ( design_matrix . columns ) } \" ) return design_matrix","title":"get_modeling_data"},{"location":"api/modeling_input_data/#overview","text":"The modeling_input_data module provides the fundamental ModelingInputData class that handles: Data loading : Reading CSV files for response and predictor data Validation : Ensuring data consistency and format compliance Preprocessing : Feature filtering, normalization, and transformation Integration : Merging response and predictor data for modeling This class serves as the foundation for all downstream modeling operations.","title":"Overview"},{"location":"api/modeling_input_data/#core-classes","text":"","title":"Core Classes"},{"location":"api/modeling_input_data/#modelinginputdata","text":"The primary class for managing input data throughout the modeling workflow.","title":"ModelingInputData"},{"location":"api/modeling_input_data/#key-features","text":"Automatic data validation : Checks file formats, column consistency, and data types Feature filtering : Removes blacklisted genes and handles missing data Index alignment : Ensures consistent gene identifiers between response and predictor files Data integration : Combines multiple data sources into modeling-ready format","title":"Key Features"},{"location":"api/modeling_input_data/#initialization","text":"from tfbpmodeling.modeling_input_data import ModelingInputData # Basic initialization data = ModelingInputData ( response_file = 'expression.csv' , predictors_file = 'binding.csv' , perturbed_tf = 'YPD1' ) # With optional parameters data = ModelingInputData ( response_file = 'expression.csv' , predictors_file = 'binding.csv' , perturbed_tf = 'YPD1' , blacklist_file = 'exclude_genes.txt' , normalize_weights = True , scale_by_std = True )","title":"Initialization"},{"location":"api/modeling_input_data/#key-methods","text":"","title":"Key Methods"},{"location":"api/modeling_input_data/#data-loading-and-validation","text":"load_data() : Read and validate input CSV files validate_format() : Check data format compliance check_consistency() : Verify response-predictor alignment","title":"Data Loading and Validation"},{"location":"api/modeling_input_data/#data-preprocessing","text":"filter_features() : Remove blacklisted and invalid features normalize_data() : Apply scaling and normalization handle_missing() : Deal with missing values appropriately","title":"Data Preprocessing"},{"location":"api/modeling_input_data/#data-access","text":"get_response_data() : Access processed response data get_predictor_data() : Access processed predictor data get_feature_names() : Retrieve feature identifiers get_sample_names() : Retrieve sample identifiers","title":"Data Access"},{"location":"api/modeling_input_data/#data-format-requirements","text":"","title":"Data Format Requirements"},{"location":"api/modeling_input_data/#response-file-format","text":"The response file must be a CSV with specific structure: gene_id,sample1,sample2,sample3,sample4 YPD1,0.23,-1.45,0.87,-0.12 YBR123W,1.34,0.56,-0.23,0.78 YCR456X,-0.45,0.12,1.23,-0.56 Requirements : - First column: Gene identifiers (must match predictor file) - Subsequent columns: Numeric expression values - Column names: Sample identifiers - Must contain column matching perturbed_tf parameter","title":"Response File Format"},{"location":"api/modeling_input_data/#predictor-file-format","text":"The predictor file structure: gene_id,TF1,TF2,TF3,TF4 YPD1,0.34,0.12,0.78,0.01 YBR123W,0.89,0.45,0.23,0.67 YCR456X,0.12,0.78,0.34,0.90 Requirements : - First column: Gene identifiers (must match response file) - Subsequent columns: Numeric binding values - Column names: Transcription factor identifiers - All values must be numeric (no missing values in binding data)","title":"Predictor File Format"},{"location":"api/modeling_input_data/#blacklist-file-format","text":"Optional exclusion file: YBR999W YCR888X control_gene technical_artifact Requirements : - Plain text file - One gene identifier per line - Gene IDs must match those in data files - Comments not supported","title":"Blacklist File Format"},{"location":"api/modeling_input_data/#usage-examples","text":"","title":"Usage Examples"},{"location":"api/modeling_input_data/#basic-data-loading","text":"from tfbpmodeling.modeling_input_data import ModelingInputData # Load data with minimal configuration data = ModelingInputData ( response_file = 'data/expression.csv' , predictors_file = 'data/binding.csv' , perturbed_tf = 'YPD1' ) # Access processed data response_data = data . get_response_data () predictor_data = data . get_predictor_data () feature_names = data . get_feature_names () print ( f \"Loaded { len ( feature_names ) } features\" ) print ( f \"Response data shape: { response_data . shape } \" ) print ( f \"Predictor data shape: { predictor_data . shape } \" )","title":"Basic Data Loading"},{"location":"api/modeling_input_data/#data-preprocessing-options","text":"# Advanced preprocessing data = ModelingInputData ( response_file = 'data/expression.csv' , predictors_file = 'data/binding.csv' , perturbed_tf = 'YPD1' , blacklist_file = 'data/exclude_genes.txt' , normalize_weights = True , scale_by_std = True , handle_missing = 'drop' # or 'impute', 'zero' ) # Check data quality print ( f \"Original features: { data . original_feature_count } \" ) print ( f \"Filtered features: { len ( data . get_feature_names ()) } \" ) print ( f \"Excluded features: { data . excluded_feature_count } \" )","title":"Data Preprocessing Options"},{"location":"api/modeling_input_data/#integration-with-modeling-pipeline","text":"# Prepare data for bootstrap modeling from tfbpmodeling.bootstrapped_input_data import BootstrappedModelingInputData # Base data base_data = ModelingInputData ( response_file = 'expression.csv' , predictors_file = 'binding.csv' , perturbed_tf = 'YPD1' ) # Create bootstrap version bootstrap_data = BootstrappedModelingInputData ( base_data = base_data , n_bootstraps = 1000 , random_state = 42 )","title":"Integration with Modeling Pipeline"},{"location":"api/modeling_input_data/#data-validation","text":"","title":"Data Validation"},{"location":"api/modeling_input_data/#automatic-checks","text":"The class performs comprehensive validation: # File existence and readability assert os . path . exists ( response_file ), f \"Response file not found: { response_file } \" assert os . path . exists ( predictors_file ), f \"Predictor file not found: { predictors_file } \" # Data format validation assert response_df . index . equals ( predictor_df . index ), \"Gene indices must match\" assert perturbed_tf in response_df . columns , f \"Perturbed TF ' { perturbed_tf } ' not found\" # Data type validation assert response_df . dtypes . apply ( lambda x : np . issubdtype ( x , np . number )) . all () assert predictor_df . dtypes . apply ( lambda x : np . issubdtype ( x , np . number )) . all ()","title":"Automatic Checks"},{"location":"api/modeling_input_data/#custom-validation","text":"# Add custom validation rules def validate_expression_range ( data ): \\ \" \\\"\\\" Ensure expression values are in reasonable range \\\"\\\"\\\" assert data . abs () . max () . max () < 10 , \"Expression values seem too large\" def validate_binding_range ( data ): \\ \" \\\"\\\" Ensure binding values are probabilities \\\"\\\"\\\" assert ( data >= 0 ) . all () . all (), \"Binding values must be non-negative\" assert ( data <= 1 ) . all () . all (), \"Binding values must be <= 1\" # Apply custom validation data = ModelingInputData ( response_file = 'expression.csv' , predictors_file = 'binding.csv' , perturbed_tf = 'YPD1' , custom_validators = [ validate_expression_range , validate_binding_range ] )","title":"Custom Validation"},{"location":"api/modeling_input_data/#error-handling","text":"","title":"Error Handling"},{"location":"api/modeling_input_data/#common-errors-and-solutions","text":"","title":"Common Errors and Solutions"},{"location":"api/modeling_input_data/#file-format-errors","text":"# CSV parsing errors try : data = ModelingInputData ( response_file = 'malformed.csv' , ... ) except pd . errors . ParserError as e : print ( f \"CSV format error: { e } \" ) # Solution: Check file encoding, delimiters, quotes","title":"File Format Errors"},{"location":"api/modeling_input_data/#data-consistency-errors","text":"# Mismatched gene indices try : data = ModelingInputData ( ... ) except ValueError as e : if \"Gene indices must match\" in str ( e ): print ( \"Response and predictor files have different gene sets\" ) # Solution: Align gene lists or use intersection","title":"Data Consistency Errors"},{"location":"api/modeling_input_data/#missing-data-errors","text":"# Perturbed TF not found try : data = ModelingInputData ( perturbed_tf = 'MISSING_TF' , ... ) except KeyError as e : print ( f \"Perturbed TF not found in response data: { e } \" ) # Solution: Check TF name spelling, verify column names","title":"Missing Data Errors"},{"location":"api/modeling_input_data/#performance-considerations","text":"","title":"Performance Considerations"},{"location":"api/modeling_input_data/#memory-management","text":"Large datasets loaded using chunked reading Unnecessary columns dropped early in processing Memory-efficient data types selected automatically","title":"Memory Management"},{"location":"api/modeling_input_data/#io-optimization","text":"CSV reading optimized with appropriate engines Caching of preprocessed data for repeated access Lazy loading of optional data components","title":"I/O Optimization"},{"location":"api/modeling_input_data/#data-processing","text":"Vectorized operations for filtering and transformation Efficient indexing for data alignment Minimal data copying during preprocessing","title":"Data Processing"},{"location":"api/modeling_input_data/#related-classes","text":"BootstrappedModelingInputData : Bootstrap sampling extension BootstrapModelResults : Results storage and aggregation StratifiedCV : Cross-validation data handling","title":"Related Classes"},{"location":"api/modeling_input_data/#integration-points","text":"The ModelingInputData class integrates with: CLI Interface : Receives parameters from command-line arguments Bootstrap Modeling : Provides base data for resampling Feature Engineering : Supplies data for polynomial and interaction terms Cross-Validation : Furnishes stratified sampling input Results Output : Delivers metadata for result interpretation","title":"Integration Points"},{"location":"api/stratified_cv/","text":"stratified_cv \u00b6 Stratified cross-validation for tfbpmodeling. tfbpmodeling.stratified_cv \u00b6 stratified_cv_modeling \u00b6 stratified_cv_modeling ( y , X , classes , estimator = LassoCV (), skf = StratifiedKFold ( n_splits = 4 , shuffle = True , random_state = 42 ), sample_weight = None , ** kwargs ) Fit a model using stratified cross-validation splits. This function wraps a scikit-learn estimator with user-defined stratified folds. While it defaults to LassoCV , any estimator with a cv attribute can be used. Parameters: y ( DataFrame ) \u2013 Response variable. Must be a single-column DataFrame. X ( DataFrame ) \u2013 Predictor matrix. Must be a DataFrame with the same number of rows as y . classes ( ndarray ) \u2013 Array of class labels for stratification, typically generated by stratification_classification() . estimator ( BaseEstimator , default: LassoCV () ) \u2013 scikit-learn estimator to use for modeling. Must support cv as an attribute. skf ( StratifiedKFold , default: StratifiedKFold (n_splits=4, shuffle=True, random_state=42) ) \u2013 StratifiedKFold object to control how splits are generated. sample_weight ( ndarray | None , default: None ) \u2013 Optional array of per-sample weights for the estimator. kwargs \u2013 Additional arguments passed to the estimator's fit() method. Returns: BaseEstimator \u2013 A fitted estimator with the best parameters determined via cross-validation. Raises: ValueError \u2013 If inputs are misformatted or incompatible with the estimator. Source code in tfbpmodeling/stratified_cv.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def stratified_cv_modeling ( y : pd . DataFrame , X : pd . DataFrame , classes : np . ndarray , estimator : BaseEstimator = LassoCV (), skf : StratifiedKFold = StratifiedKFold ( n_splits = 4 , shuffle = True , random_state = 42 ), sample_weight : np . ndarray | None = None , ** kwargs , ) -> BaseEstimator : \"\"\" Fit a model using stratified cross-validation splits. This function wraps a scikit-learn estimator with user-defined stratified folds. While it defaults to `LassoCV`, any estimator with a `cv` attribute can be used. :param y: Response variable. Must be a single-column DataFrame. :param X: Predictor matrix. Must be a DataFrame with the same number of rows as `y`. :param classes: Array of class labels for stratification, typically generated by `stratification_classification()`. :param estimator: scikit-learn estimator to use for modeling. Must support `cv` as an attribute. :param skf: StratifiedKFold object to control how splits are generated. :param sample_weight: Optional array of per-sample weights for the estimator. :param kwargs: Additional arguments passed to the estimator's `fit()` method. :return: A fitted estimator with the best parameters determined via cross-validation. :raises ValueError: If inputs are misformatted or incompatible with the estimator. \"\"\" # Validate data if not isinstance ( y , pd . DataFrame ): raise ValueError ( \"The response variable y must be a DataFrame.\" ) if y . shape [ 1 ] != 1 : raise ValueError ( \"The response variable y must be a single column DataFrame.\" ) if not isinstance ( X , pd . DataFrame ): raise ValueError ( \"The predictors X must be a DataFrame.\" ) if X . shape [ 0 ] != y . shape [ 0 ]: raise ValueError ( \"The number of rows in X must match the number of rows in y.\" ) if classes . size == 0 or not isinstance ( classes , np . ndarray ): raise ValueError ( \"The classes must be a non-empty numpy array.\" ) # Verify estimator has a `cv` attribute if not hasattr ( estimator , \"cv\" ): raise ValueError ( \"The estimator must support a `cv` parameter.\" ) # Initialize StratifiedKFold for stratified splits with warnings . catch_warnings ( record = True ) as w : warnings . simplefilter ( \"always\" ) # default setting for shuffle is False, which means the partitioning is # deterministic and static. Recommendation for bootstrapping is to # set shuffle=True and use random_state = bootstrap_iteration in order to # have random, but reproducible, partitions folds = list ( skf . split ( X , classes )) for warning in w : logger . debug ( f \"Warning encountered during stratified k-fold split: { warning . message } \" ) # Clone the estimator and set the `cv` attribute with predefined folds model = clone ( estimator ) model . cv = folds # Step 7: Fit the model using the custom cross-validation folds model . fit ( X , y . values . ravel (), sample_weight = sample_weight , ) return model Overview \u00b6 The stratified_cv module provides cross-validation functionality that maintains the distribution of data characteristics across folds. This is particularly important for tfbpmodeling where data may have natural groupings or strata that should be preserved during validation. Key Features \u00b6 Stratified Sampling : Maintains data distribution across CV folds Bootstrap Integration : Works with bootstrap resampling Flexible Stratification : Multiple stratification strategies Robust Validation : Reduces bias in cross-validation estimates Usage Examples \u00b6 Basic Stratified CV \u00b6 from tfbpmodeling.stratified_cv import StratifiedCV # Create stratified CV object cv = StratifiedCV ( n_splits = 5 , stratification_variable = 'binding_strength_bins' , random_state = 42 ) # Generate CV folds for train_idx , test_idx in cv . split ( X , y ): # Train and evaluate model pass Bootstrap Integration \u00b6 from tfbpmodeling.stratified_cv import bootstrap_stratified_cv # Perform bootstrap with stratified CV cv_scores = bootstrap_stratified_cv ( X = predictor_data , y = response_data , estimator = LassoCV (), n_bootstraps = 1000 , cv_folds = 5 , stratification_bins = [ 0 , 8 , 12 , np . inf ] ) Stratification Methods \u00b6 Binding Strength Bins \u00b6 Stratifies data based on transcription factor binding strength: # Define binding strength bins bins = [ 0 , 0.1 , 0.5 , 1.0 ] # Low, medium, high binding cv = StratifiedCV ( n_splits = 5 , stratification_method = 'binding_bins' , bins = bins ) Expression Level Bins \u00b6 Stratifies based on expression level ranges: # Expression-based stratification cv = StratifiedCV ( n_splits = 5 , stratification_method = 'expression_bins' , bins = [ - np . inf , - 1 , 0 , 1 , np . inf ] ) Related Modules \u00b6 stratified_cv_r2 : R\u00b2 calculation with stratification bootstrapped_input_data : Bootstrap data handling interface : Workflow integration","title":"stratified_cv"},{"location":"api/stratified_cv/#stratified_cv","text":"Stratified cross-validation for tfbpmodeling.","title":"stratified_cv"},{"location":"api/stratified_cv/#tfbpmodeling.stratified_cv","text":"","title":"stratified_cv"},{"location":"api/stratified_cv/#tfbpmodeling.stratified_cv.stratified_cv_modeling","text":"stratified_cv_modeling ( y , X , classes , estimator = LassoCV (), skf = StratifiedKFold ( n_splits = 4 , shuffle = True , random_state = 42 ), sample_weight = None , ** kwargs ) Fit a model using stratified cross-validation splits. This function wraps a scikit-learn estimator with user-defined stratified folds. While it defaults to LassoCV , any estimator with a cv attribute can be used. Parameters: y ( DataFrame ) \u2013 Response variable. Must be a single-column DataFrame. X ( DataFrame ) \u2013 Predictor matrix. Must be a DataFrame with the same number of rows as y . classes ( ndarray ) \u2013 Array of class labels for stratification, typically generated by stratification_classification() . estimator ( BaseEstimator , default: LassoCV () ) \u2013 scikit-learn estimator to use for modeling. Must support cv as an attribute. skf ( StratifiedKFold , default: StratifiedKFold (n_splits=4, shuffle=True, random_state=42) ) \u2013 StratifiedKFold object to control how splits are generated. sample_weight ( ndarray | None , default: None ) \u2013 Optional array of per-sample weights for the estimator. kwargs \u2013 Additional arguments passed to the estimator's fit() method. Returns: BaseEstimator \u2013 A fitted estimator with the best parameters determined via cross-validation. Raises: ValueError \u2013 If inputs are misformatted or incompatible with the estimator. Source code in tfbpmodeling/stratified_cv.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def stratified_cv_modeling ( y : pd . DataFrame , X : pd . DataFrame , classes : np . ndarray , estimator : BaseEstimator = LassoCV (), skf : StratifiedKFold = StratifiedKFold ( n_splits = 4 , shuffle = True , random_state = 42 ), sample_weight : np . ndarray | None = None , ** kwargs , ) -> BaseEstimator : \"\"\" Fit a model using stratified cross-validation splits. This function wraps a scikit-learn estimator with user-defined stratified folds. While it defaults to `LassoCV`, any estimator with a `cv` attribute can be used. :param y: Response variable. Must be a single-column DataFrame. :param X: Predictor matrix. Must be a DataFrame with the same number of rows as `y`. :param classes: Array of class labels for stratification, typically generated by `stratification_classification()`. :param estimator: scikit-learn estimator to use for modeling. Must support `cv` as an attribute. :param skf: StratifiedKFold object to control how splits are generated. :param sample_weight: Optional array of per-sample weights for the estimator. :param kwargs: Additional arguments passed to the estimator's `fit()` method. :return: A fitted estimator with the best parameters determined via cross-validation. :raises ValueError: If inputs are misformatted or incompatible with the estimator. \"\"\" # Validate data if not isinstance ( y , pd . DataFrame ): raise ValueError ( \"The response variable y must be a DataFrame.\" ) if y . shape [ 1 ] != 1 : raise ValueError ( \"The response variable y must be a single column DataFrame.\" ) if not isinstance ( X , pd . DataFrame ): raise ValueError ( \"The predictors X must be a DataFrame.\" ) if X . shape [ 0 ] != y . shape [ 0 ]: raise ValueError ( \"The number of rows in X must match the number of rows in y.\" ) if classes . size == 0 or not isinstance ( classes , np . ndarray ): raise ValueError ( \"The classes must be a non-empty numpy array.\" ) # Verify estimator has a `cv` attribute if not hasattr ( estimator , \"cv\" ): raise ValueError ( \"The estimator must support a `cv` parameter.\" ) # Initialize StratifiedKFold for stratified splits with warnings . catch_warnings ( record = True ) as w : warnings . simplefilter ( \"always\" ) # default setting for shuffle is False, which means the partitioning is # deterministic and static. Recommendation for bootstrapping is to # set shuffle=True and use random_state = bootstrap_iteration in order to # have random, but reproducible, partitions folds = list ( skf . split ( X , classes )) for warning in w : logger . debug ( f \"Warning encountered during stratified k-fold split: { warning . message } \" ) # Clone the estimator and set the `cv` attribute with predefined folds model = clone ( estimator ) model . cv = folds # Step 7: Fit the model using the custom cross-validation folds model . fit ( X , y . values . ravel (), sample_weight = sample_weight , ) return model","title":"stratified_cv_modeling"},{"location":"api/stratified_cv/#overview","text":"The stratified_cv module provides cross-validation functionality that maintains the distribution of data characteristics across folds. This is particularly important for tfbpmodeling where data may have natural groupings or strata that should be preserved during validation.","title":"Overview"},{"location":"api/stratified_cv/#key-features","text":"Stratified Sampling : Maintains data distribution across CV folds Bootstrap Integration : Works with bootstrap resampling Flexible Stratification : Multiple stratification strategies Robust Validation : Reduces bias in cross-validation estimates","title":"Key Features"},{"location":"api/stratified_cv/#usage-examples","text":"","title":"Usage Examples"},{"location":"api/stratified_cv/#basic-stratified-cv","text":"from tfbpmodeling.stratified_cv import StratifiedCV # Create stratified CV object cv = StratifiedCV ( n_splits = 5 , stratification_variable = 'binding_strength_bins' , random_state = 42 ) # Generate CV folds for train_idx , test_idx in cv . split ( X , y ): # Train and evaluate model pass","title":"Basic Stratified CV"},{"location":"api/stratified_cv/#bootstrap-integration","text":"from tfbpmodeling.stratified_cv import bootstrap_stratified_cv # Perform bootstrap with stratified CV cv_scores = bootstrap_stratified_cv ( X = predictor_data , y = response_data , estimator = LassoCV (), n_bootstraps = 1000 , cv_folds = 5 , stratification_bins = [ 0 , 8 , 12 , np . inf ] )","title":"Bootstrap Integration"},{"location":"api/stratified_cv/#stratification-methods","text":"","title":"Stratification Methods"},{"location":"api/stratified_cv/#binding-strength-bins","text":"Stratifies data based on transcription factor binding strength: # Define binding strength bins bins = [ 0 , 0.1 , 0.5 , 1.0 ] # Low, medium, high binding cv = StratifiedCV ( n_splits = 5 , stratification_method = 'binding_bins' , bins = bins )","title":"Binding Strength Bins"},{"location":"api/stratified_cv/#expression-level-bins","text":"Stratifies based on expression level ranges: # Expression-based stratification cv = StratifiedCV ( n_splits = 5 , stratification_method = 'expression_bins' , bins = [ - np . inf , - 1 , 0 , 1 , np . inf ] )","title":"Expression Level Bins"},{"location":"api/stratified_cv/#related-modules","text":"stratified_cv_r2 : R\u00b2 calculation with stratification bootstrapped_input_data : Bootstrap data handling interface : Workflow integration","title":"Related Modules"},{"location":"api/stratified_cv_r2/","text":"stratified_cv_r2 \u00b6 R\u00b2 calculation with stratified cross-validation for tfbpmodeling. tfbpmodeling.stratified_cv_r2 \u00b6 stratified_cv_r2 \u00b6 stratified_cv_r2 ( y , X , classes , estimator = LinearRegression ( fit_intercept = True ), skf = StratifiedKFold ( n_splits = 4 , shuffle = True , random_state = 42 ), ** kwargs ) Calculate the average stratified CV r-squared for a given estimator and data. By default, this is a 4-fold stratified CV with a LinearRegression estimator. Note that by default, the estimator is set to LinearRegression() and the StratifiedKFold object is set to a 4-fold stratified CV with shuffle=True and random_state=42. LinearRegression has fit_intercept explicitly set to True, meaning the data IS NOT expected to be centered and there should not be a constant column in X. Parameters: y ( DataFrame ) \u2013 The response variable. See generate_modeling_data() X ( DataFrame ) \u2013 The predictor variables. See generate_modeling_data() classes ( ndarray ) \u2013 the stratification classes for the data estimator ( BaseEstimator , default: LinearRegression (fit_intercept=True) ) \u2013 the estimator to be used in the modeling. By default, this is a LinearRegression() model. skf ( StratifiedKFold , default: StratifiedKFold (n_splits=4, shuffle=True, random_state=42) ) \u2013 the StratifiedKFold object to be used in the modeling. By default, this is a 4-fold stratified CV with shuffle=True and random_state=42. Returns: float \u2013 the average r-squared value for the stratified CV Source code in tfbpmodeling/stratified_cv_r2.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def stratified_cv_r2 ( y : pd . DataFrame , X : pd . DataFrame , classes : np . ndarray , estimator : BaseEstimator = LinearRegression ( fit_intercept = True ), skf : StratifiedKFold = StratifiedKFold ( n_splits = 4 , shuffle = True , random_state = 42 ), ** kwargs , ) -> float : \"\"\" Calculate the average stratified CV r-squared for a given estimator and data. By default, this is a 4-fold stratified CV with a LinearRegression estimator. Note that by default, the estimator is set to LinearRegression() and the StratifiedKFold object is set to a 4-fold stratified CV with shuffle=True and random_state=42. LinearRegression has fit_intercept explicitly set to True, meaning the data IS NOT expected to be centered and there should not be a constant column in X. :param y: The response variable. See generate_modeling_data() :param X: The predictor variables. See generate_modeling_data() :param classes: the stratification classes for the data :param estimator: the estimator to be used in the modeling. By default, this is a LinearRegression() model. :param skf: the StratifiedKFold object to be used in the modeling. By default, this is a 4-fold stratified CV with shuffle=True and random_state=42. :return: the average r-squared value for the stratified CV \"\"\" estimator_local = clone ( estimator ) r2_scores = [] with warnings . catch_warnings ( record = True ) as w : warnings . simplefilter ( \"always\" ) folds = list ( skf . split ( X , classes )) for warning in w : logger . debug ( f \"Warning encountered during stratified k-fold split: { warning . message } \" ) for train_idx , test_idx in folds : # Use train and test indices to split X and y X_train , X_test = ( X . iloc [ train_idx ], X . iloc [ test_idx ], ) y_train , y_test = y . iloc [ train_idx ], y . iloc [ test_idx ] # Fit the model model = estimator_local . fit ( X_train , y_train , ) # Calculate R-squared and append to r2_scores r2_scores . append ( r2_score ( y_test , model . predict ( X_test ))) return np . mean ( r2_scores ) Overview \u00b6 The stratified_cv_r2 module provides specialized functions for calculating R\u00b2 scores using stratified cross-validation. This ensures that model performance metrics accurately reflect the model's ability to generalize across different data strata. Key Features \u00b6 Stratified R\u00b2 Calculation : R\u00b2 scores that account for data stratification Cross-Validation Integration : Works with stratified CV folds Bootstrap Compatibility : Integrates with bootstrap resampling Robust Performance Metrics : Reduces bias in performance estimation Usage Examples \u00b6 Basic R\u00b2 Calculation \u00b6 from tfbpmodeling.stratified_cv_r2 import calculate_stratified_r2 # Calculate stratified R\u00b2 scores r2_scores = calculate_stratified_r2 ( estimator = LassoCV (), X = predictor_data , y = response_data , cv_folds = 5 , stratification_bins = [ 0 , 8 , 12 , np . inf ] ) print ( f \"Mean R\u00b2: { r2_scores . mean () : .3f } \" ) print ( f \"Std R\u00b2: { r2_scores . std () : .3f } \" ) Bootstrap Integration \u00b6 from tfbpmodeling.stratified_cv_r2 import bootstrap_stratified_r2 # Bootstrap R\u00b2 with stratification bootstrap_r2 = bootstrap_stratified_r2 ( estimator = LassoCV (), X = predictor_data , y = response_data , n_bootstraps = 1000 , cv_folds = 5 , stratification_bins = [ 0 , 8 , 12 , np . inf ] ) # Get confidence interval for R\u00b2 r2_ci = np . percentile ( bootstrap_r2 , [ 2.5 , 97.5 ]) print ( f \"R\u00b2 95% CI: [ { r2_ci [ 0 ] : .3f } , { r2_ci [ 1 ] : .3f } ]\" ) Performance Metrics \u00b6 Stratified R\u00b2 \u00b6 Calculates R\u00b2 separately for each stratum and then aggregates: # Per-stratum R\u00b2 calculation stratum_r2 = calculate_per_stratum_r2 ( estimator = model , X = X_test , y = y_test , strata = test_strata ) Weighted Aggregation \u00b6 Combines R\u00b2 scores across strata with appropriate weighting: # Weighted average R\u00b2 weighted_r2 = calculate_weighted_r2 ( stratum_r2_scores = stratum_scores , stratum_weights = stratum_sizes ) Related Modules \u00b6 stratified_cv : Stratified cross-validation bootstrap_model_results : Results aggregation interface : Workflow integration","title":"stratified_cv_r2"},{"location":"api/stratified_cv_r2/#stratified_cv_r2","text":"R\u00b2 calculation with stratified cross-validation for tfbpmodeling.","title":"stratified_cv_r2"},{"location":"api/stratified_cv_r2/#tfbpmodeling.stratified_cv_r2","text":"","title":"stratified_cv_r2"},{"location":"api/stratified_cv_r2/#tfbpmodeling.stratified_cv_r2.stratified_cv_r2","text":"stratified_cv_r2 ( y , X , classes , estimator = LinearRegression ( fit_intercept = True ), skf = StratifiedKFold ( n_splits = 4 , shuffle = True , random_state = 42 ), ** kwargs ) Calculate the average stratified CV r-squared for a given estimator and data. By default, this is a 4-fold stratified CV with a LinearRegression estimator. Note that by default, the estimator is set to LinearRegression() and the StratifiedKFold object is set to a 4-fold stratified CV with shuffle=True and random_state=42. LinearRegression has fit_intercept explicitly set to True, meaning the data IS NOT expected to be centered and there should not be a constant column in X. Parameters: y ( DataFrame ) \u2013 The response variable. See generate_modeling_data() X ( DataFrame ) \u2013 The predictor variables. See generate_modeling_data() classes ( ndarray ) \u2013 the stratification classes for the data estimator ( BaseEstimator , default: LinearRegression (fit_intercept=True) ) \u2013 the estimator to be used in the modeling. By default, this is a LinearRegression() model. skf ( StratifiedKFold , default: StratifiedKFold (n_splits=4, shuffle=True, random_state=42) ) \u2013 the StratifiedKFold object to be used in the modeling. By default, this is a 4-fold stratified CV with shuffle=True and random_state=42. Returns: float \u2013 the average r-squared value for the stratified CV Source code in tfbpmodeling/stratified_cv_r2.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def stratified_cv_r2 ( y : pd . DataFrame , X : pd . DataFrame , classes : np . ndarray , estimator : BaseEstimator = LinearRegression ( fit_intercept = True ), skf : StratifiedKFold = StratifiedKFold ( n_splits = 4 , shuffle = True , random_state = 42 ), ** kwargs , ) -> float : \"\"\" Calculate the average stratified CV r-squared for a given estimator and data. By default, this is a 4-fold stratified CV with a LinearRegression estimator. Note that by default, the estimator is set to LinearRegression() and the StratifiedKFold object is set to a 4-fold stratified CV with shuffle=True and random_state=42. LinearRegression has fit_intercept explicitly set to True, meaning the data IS NOT expected to be centered and there should not be a constant column in X. :param y: The response variable. See generate_modeling_data() :param X: The predictor variables. See generate_modeling_data() :param classes: the stratification classes for the data :param estimator: the estimator to be used in the modeling. By default, this is a LinearRegression() model. :param skf: the StratifiedKFold object to be used in the modeling. By default, this is a 4-fold stratified CV with shuffle=True and random_state=42. :return: the average r-squared value for the stratified CV \"\"\" estimator_local = clone ( estimator ) r2_scores = [] with warnings . catch_warnings ( record = True ) as w : warnings . simplefilter ( \"always\" ) folds = list ( skf . split ( X , classes )) for warning in w : logger . debug ( f \"Warning encountered during stratified k-fold split: { warning . message } \" ) for train_idx , test_idx in folds : # Use train and test indices to split X and y X_train , X_test = ( X . iloc [ train_idx ], X . iloc [ test_idx ], ) y_train , y_test = y . iloc [ train_idx ], y . iloc [ test_idx ] # Fit the model model = estimator_local . fit ( X_train , y_train , ) # Calculate R-squared and append to r2_scores r2_scores . append ( r2_score ( y_test , model . predict ( X_test ))) return np . mean ( r2_scores )","title":"stratified_cv_r2"},{"location":"api/stratified_cv_r2/#overview","text":"The stratified_cv_r2 module provides specialized functions for calculating R\u00b2 scores using stratified cross-validation. This ensures that model performance metrics accurately reflect the model's ability to generalize across different data strata.","title":"Overview"},{"location":"api/stratified_cv_r2/#key-features","text":"Stratified R\u00b2 Calculation : R\u00b2 scores that account for data stratification Cross-Validation Integration : Works with stratified CV folds Bootstrap Compatibility : Integrates with bootstrap resampling Robust Performance Metrics : Reduces bias in performance estimation","title":"Key Features"},{"location":"api/stratified_cv_r2/#usage-examples","text":"","title":"Usage Examples"},{"location":"api/stratified_cv_r2/#basic-r2-calculation","text":"from tfbpmodeling.stratified_cv_r2 import calculate_stratified_r2 # Calculate stratified R\u00b2 scores r2_scores = calculate_stratified_r2 ( estimator = LassoCV (), X = predictor_data , y = response_data , cv_folds = 5 , stratification_bins = [ 0 , 8 , 12 , np . inf ] ) print ( f \"Mean R\u00b2: { r2_scores . mean () : .3f } \" ) print ( f \"Std R\u00b2: { r2_scores . std () : .3f } \" )","title":"Basic R\u00b2 Calculation"},{"location":"api/stratified_cv_r2/#bootstrap-integration","text":"from tfbpmodeling.stratified_cv_r2 import bootstrap_stratified_r2 # Bootstrap R\u00b2 with stratification bootstrap_r2 = bootstrap_stratified_r2 ( estimator = LassoCV (), X = predictor_data , y = response_data , n_bootstraps = 1000 , cv_folds = 5 , stratification_bins = [ 0 , 8 , 12 , np . inf ] ) # Get confidence interval for R\u00b2 r2_ci = np . percentile ( bootstrap_r2 , [ 2.5 , 97.5 ]) print ( f \"R\u00b2 95% CI: [ { r2_ci [ 0 ] : .3f } , { r2_ci [ 1 ] : .3f } ]\" )","title":"Bootstrap Integration"},{"location":"api/stratified_cv_r2/#performance-metrics","text":"","title":"Performance Metrics"},{"location":"api/stratified_cv_r2/#stratified-r2","text":"Calculates R\u00b2 separately for each stratum and then aggregates: # Per-stratum R\u00b2 calculation stratum_r2 = calculate_per_stratum_r2 ( estimator = model , X = X_test , y = y_test , strata = test_strata )","title":"Stratified R\u00b2"},{"location":"api/stratified_cv_r2/#weighted-aggregation","text":"Combines R\u00b2 scores across strata with appropriate weighting: # Weighted average R\u00b2 weighted_r2 = calculate_weighted_r2 ( stratum_r2_scores = stratum_scores , stratum_weights = stratum_sizes )","title":"Weighted Aggregation"},{"location":"api/stratified_cv_r2/#related-modules","text":"stratified_cv : Stratified cross-validation bootstrap_model_results : Results aggregation interface : Workflow integration","title":"Related Modules"},{"location":"cli/linear-perturbation-binding-modeling/","text":"linear_perturbation_binding_modeling \u00b6 The main command for running transcription factor binding and perturbation analysis. Synopsis \u00b6 python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file RESPONSE_FILE \\ --predictors_file PREDICTORS_FILE \\ --perturbed_tf PERTURBED_TF \\ [ OPTIONS ] Description \u00b6 This command executes a sequential 4-stage workflow that models the relationship between transcription factor binding data and gene expression perturbation data: All Data Modeling : Bootstrap resampling with LassoCV on complete dataset Top-N Modeling : Secondary modeling on significant predictors from top-performing data subset Interactor Significance : Statistical evaluation of interaction terms vs corresponding main effects Results Generation : Comprehensive output with confidence intervals and diagnostic information The workflow uses bootstrap resampling to provide robust statistical inference and regularized regression (LassoCV) for feature selection and model fitting. Required Arguments \u00b6 Input Files \u00b6 --response_file RESPONSE_FILE \u00b6 Path to the response CSV file containing gene expression data. Format Requirements: - CSV format with genes/features as rows - First column: feature names or locus tags (e.g., gene symbols) - Subsequent columns: expression values for each sample - Must contain a column matching the --perturbed_tf parameter - Gene identifiers must match those in the predictors file Example: gene_id,sample1,sample2,sample3,sample4 YPD1,0.23,-1.45,0.87,-0.12 YBR123W,1.34,0.56,-0.23,0.78 YCR456X,-0.45,0.12,1.23,-0.56 --predictors_file PREDICTORS_FILE \u00b6 Path to the predictors CSV file containing transcription factor binding data. Format Requirements: - CSV format with genes/features as rows - First column: feature names or locus tags matching response file - Subsequent columns: binding measurements for different transcription factors - Numeric values representing binding strength/probability Example: gene_id,TF1,TF2,TF3,TF4 YPD1,0.34,0.12,0.78,0.01 YBR123W,0.89,0.45,0.23,0.67 YCR456X,0.12,0.78,0.34,0.90 --perturbed_tf PERTURBED_TF \u00b6 Name of the perturbed transcription factor used as the response variable. Requirements: - Must exactly match a column name in the response file - Case-sensitive - Will be used as the dependent variable in modeling Optional Arguments \u00b6 Input Control \u00b6 --blacklist_file BLACKLIST_FILE \u00b6 Optional file containing features to exclude from analysis. Format : Plain text file with one feature identifier per line Example: YBR999W YCR888X control_gene batch_effect_gene --n_bootstraps N_BOOTSTRAPS \u00b6 Number of bootstrap samples for resampling analysis. - Default : 1000 - Range : 100-10000 recommended - Impact : Higher values provide more robust statistics but increase runtime --random_state RANDOM_STATE \u00b6 Seed for reproducible bootstrap sampling. - Default : None (random seed each run) - Type : Integer - Note : If set, top-N modeling uses random_state + 10 for different sampling --top_n TOP_N \u00b6 Number of features to retain for second-round modeling. - Default : 600 - Impact : Higher values include more features but may reduce specificity Data Processing \u00b6 --normalize_sample_weights \u00b6 Normalize bootstrap sample weights to sum to 1. - Default : False - Use case : When sample weights need explicit normalization --scale_by_std \u00b6 Center and scale the model matrix by standard deviation. - Default : False - Effect : Sets LassoCV fit_intercept parameter to False - Use case : When features have very different scales Feature Engineering \u00b6 --row_max \u00b6 Include row maximum as an additional predictor in all-data modeling. - Default : False - Description : Adds the maximum binding value across all TFs for each gene --squared_pTF \u00b6 Include squared perturbed TF term in all-data modeling. - Default : False - Mathematical : Adds pTF\u00b2 term to capture non-linear relationships --cubic_pTF \u00b6 Include cubic perturbed TF term in all-data modeling. - Default : False - Mathematical : Adds pTF\u00b3 term for higher-order non-linearities --ptf_main_effect \u00b6 Include perturbed transcription factor main effect in modeling formula. - Default : False - Description : Adds the pTF binding value as a direct predictor --exclude_interactor_variables EXCLUDE_INTERACTOR_VARIABLES \u00b6 Comma-separated list of variables to exclude from interaction terms. - Format : var1,var2,var3 or exclude_all - Example : --exclude_interactor_variables \"red_median,green_median\" --add_model_variables ADD_MODEL_VARIABLES \u00b6 Comma-separated list of additional variables for all-data modeling. - Format : var1,var2,var3 - Example : --add_model_variables \"red_median,green_median\" - Effect : Adds ... + red_median + green_median to model formula Binning Options \u00b6 --bins BINS \u00b6 Comma-separated bin edges for data stratification. - Default : \"0,8,12,np.inf\" - Format : Numbers or np.inf for infinity - Example : --bins \"0,5,10,15,np.inf\" - Purpose : Stratifies data for cross-validation Model Parameters \u00b6 --all_data_ci_level ALL_DATA_CI_LEVEL \u00b6 Confidence interval threshold (%) for selecting significant coefficients in first stage. - Default : 98.0 - Range : 80.0-99.9 - Impact : Higher values are more stringent for feature selection --topn_ci_level TOPN_CI_LEVEL \u00b6 Confidence interval threshold for second round of modeling. - Default : 90.0 - Range : 80.0-99.9 - Typically : Lower than all_data_ci_level for refinement --max_iter MAX_ITER \u00b6 Maximum iterations for LassoCV convergence. - Default : 10000 - Range : 1000-100000 - Increase if : Convergence warnings appear --iterative_dropout \u00b6 Enable iterative variable dropout based on confidence intervals. - Default : False - Description : Progressively removes non-significant variables during modeling --stabilization_ci_start STABILIZATION_CI_START \u00b6 Starting confidence interval for iterative dropout stabilization. - Default : 50.0 - Range : 50.0-95.0 - Used with : --iterative_dropout --stage4_lasso \u00b6 Use LassoCV-based interactor significance testing in Stage 4. - Default : False (uses linear regression) - Alternative : More conservative approach with regularization --stage4_topn \u00b6 Perform Stage 4 evaluation on top-n data instead of all data. - Default : False (uses all data) - Effect : Focuses final analysis on high-performing subset Output Control \u00b6 --output_dir OUTPUT_DIR \u00b6 Base directory for saving results. - Default : \"./linear_perturbation_binding_modeling_results\" - Structure : Creates subdirectory per run with timestamp --output_suffix OUTPUT_SUFFIX \u00b6 Suffix to append to output subdirectory name. - Default : Empty string - Naming : {perturbed_tf}{suffix}_{timestamp} - Example : YPD1_custom_run_20240115_143022 System Options \u00b6 --n_cpus N_CPUS \u00b6 Number of CPU cores for parallel processing. - Default : 4 - Recommendation : Match your system's available cores - Impact : Each LassoCV call uses specified cores Output Structure \u00b6 Results are saved in a timestamped subdirectory: {output_dir}/{perturbed_tf}{output_suffix}_{timestamp}/ \u251c\u2500\u2500 all_data_results/ \u2502 \u251c\u2500\u2500 bootstrap_coefficients.csv # Coefficient estimates per bootstrap \u2502 \u251c\u2500\u2500 confidence_intervals.csv # CI and significance results \u2502 \u251c\u2500\u2500 model_statistics.csv # R\u00b2, CV scores, feature counts \u2502 \u251c\u2500\u2500 feature_importance.csv # Ranked feature importance \u2502 \u2514\u2500\u2500 diagnostic_plots/ # Visualization outputs \u251c\u2500\u2500 topn_results/ # Same structure for top-N analysis \u2502 \u251c\u2500\u2500 bootstrap_coefficients.csv \u2502 \u251c\u2500\u2500 confidence_intervals.csv \u2502 \u251c\u2500\u2500 model_statistics.csv \u2502 \u251c\u2500\u2500 feature_importance.csv \u2502 \u2514\u2500\u2500 diagnostic_plots/ \u251c\u2500\u2500 interactor_significance/ \u2502 \u251c\u2500\u2500 significance_results.csv # Interaction vs main effect tests \u2502 \u251c\u2500\u2500 comparison_statistics.csv # Statistical comparisons \u2502 \u251c\u2500\u2500 final_selection.csv # Selected significant interactions \u2502 \u2514\u2500\u2500 interaction_plots/ # Interaction visualizations \u251c\u2500\u2500 input_data/ \u2502 \u251c\u2500\u2500 processed_response.csv # Cleaned response data \u2502 \u251c\u2500\u2500 processed_predictors.csv # Cleaned predictor data \u2502 \u251c\u2500\u2500 bootstrap_indices.csv # Bootstrap sample indices \u2502 \u2514\u2500\u2500 data_summary.json # Data processing summary \u2514\u2500\u2500 tfbpmodeling_{timestamp}.log # Complete execution log Examples \u00b6 Basic Analysis \u00b6 python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data/expression.csv \\ --predictors_file data/binding.csv \\ --perturbed_tf YPD1 Reproducible High-Quality Analysis \u00b6 python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data/expression.csv \\ --predictors_file data/binding.csv \\ --perturbed_tf YPD1 \\ --n_bootstraps 2000 \\ --top_n 500 \\ --all_data_ci_level 95 .0 \\ --topn_ci_level 85 .0 \\ --random_state 42 \\ --output_dir ./results \\ --output_suffix _high_quality Advanced Feature Engineering \u00b6 python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data/expression.csv \\ --predictors_file data/binding.csv \\ --perturbed_tf YPD1 \\ --blacklist_file data/exclude_genes.txt \\ --row_max \\ --squared_pTF \\ --cubic_pTF \\ --ptf_main_effect \\ --add_model_variables \"red_median,green_median\" \\ --exclude_interactor_variables \"batch_effect\" \\ --normalize_sample_weights \\ --scale_by_std \\ --bins \"0,5,10,15,np.inf\" High-Performance Analysis \u00b6 python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data/expression.csv \\ --predictors_file data/binding.csv \\ --perturbed_tf YPD1 \\ --n_bootstraps 5000 \\ --n_cpus 16 \\ --max_iter 20000 \\ --iterative_dropout \\ --stage4_lasso \\ --stage4_topn Performance Considerations \u00b6 Memory Usage \u00b6 Scales with: features \u00d7 samples \u00d7 bootstrap samples Large datasets: reduce n_bootstraps or top_n Monitor with: htop or top during execution Runtime Estimation \u00b6 For typical datasets (1000 features, 100 samples): - 1000 bootstraps: 10-30 minutes - 2000 bootstraps: 20-60 minutes - 5000 bootstraps: 1-3 hours Optimization Tips \u00b6 Start with default parameters Use --random_state for reproducible development Increase --n_cpus to match available cores Use --log-level DEBUG for detailed progress tracking Consider --iterative_dropout for feature-rich datasets Error Handling \u00b6 Common Issues \u00b6 File Format Errors \u00b6 # Verify CSV format head -5 data/expression.csv head -5 data/binding.csv Missing Perturbed TF \u00b6 # Check column names head -1 data/expression.csv | tr ',' '\\n' | grep YPD1 Convergence Issues \u00b6 # Increase iterations --max_iter 20000 Memory Issues \u00b6 # Reduce computational load --n_bootstraps 500 --top_n 300 Related Commands \u00b6 CLI Overview : General CLI documentation Tutorials : Step-by-step examples API Reference : Programmatic usage","title":"linear_perturbation_binding_modeling"},{"location":"cli/linear-perturbation-binding-modeling/#linear_perturbation_binding_modeling","text":"The main command for running transcription factor binding and perturbation analysis.","title":"linear_perturbation_binding_modeling"},{"location":"cli/linear-perturbation-binding-modeling/#synopsis","text":"python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file RESPONSE_FILE \\ --predictors_file PREDICTORS_FILE \\ --perturbed_tf PERTURBED_TF \\ [ OPTIONS ]","title":"Synopsis"},{"location":"cli/linear-perturbation-binding-modeling/#description","text":"This command executes a sequential 4-stage workflow that models the relationship between transcription factor binding data and gene expression perturbation data: All Data Modeling : Bootstrap resampling with LassoCV on complete dataset Top-N Modeling : Secondary modeling on significant predictors from top-performing data subset Interactor Significance : Statistical evaluation of interaction terms vs corresponding main effects Results Generation : Comprehensive output with confidence intervals and diagnostic information The workflow uses bootstrap resampling to provide robust statistical inference and regularized regression (LassoCV) for feature selection and model fitting.","title":"Description"},{"location":"cli/linear-perturbation-binding-modeling/#required-arguments","text":"","title":"Required Arguments"},{"location":"cli/linear-perturbation-binding-modeling/#input-files","text":"","title":"Input Files"},{"location":"cli/linear-perturbation-binding-modeling/#-response_file-response_file","text":"Path to the response CSV file containing gene expression data. Format Requirements: - CSV format with genes/features as rows - First column: feature names or locus tags (e.g., gene symbols) - Subsequent columns: expression values for each sample - Must contain a column matching the --perturbed_tf parameter - Gene identifiers must match those in the predictors file Example: gene_id,sample1,sample2,sample3,sample4 YPD1,0.23,-1.45,0.87,-0.12 YBR123W,1.34,0.56,-0.23,0.78 YCR456X,-0.45,0.12,1.23,-0.56","title":"--response_file RESPONSE_FILE"},{"location":"cli/linear-perturbation-binding-modeling/#-predictors_file-predictors_file","text":"Path to the predictors CSV file containing transcription factor binding data. Format Requirements: - CSV format with genes/features as rows - First column: feature names or locus tags matching response file - Subsequent columns: binding measurements for different transcription factors - Numeric values representing binding strength/probability Example: gene_id,TF1,TF2,TF3,TF4 YPD1,0.34,0.12,0.78,0.01 YBR123W,0.89,0.45,0.23,0.67 YCR456X,0.12,0.78,0.34,0.90","title":"--predictors_file PREDICTORS_FILE"},{"location":"cli/linear-perturbation-binding-modeling/#-perturbed_tf-perturbed_tf","text":"Name of the perturbed transcription factor used as the response variable. Requirements: - Must exactly match a column name in the response file - Case-sensitive - Will be used as the dependent variable in modeling","title":"--perturbed_tf PERTURBED_TF"},{"location":"cli/linear-perturbation-binding-modeling/#optional-arguments","text":"","title":"Optional Arguments"},{"location":"cli/linear-perturbation-binding-modeling/#input-control","text":"","title":"Input Control"},{"location":"cli/linear-perturbation-binding-modeling/#-blacklist_file-blacklist_file","text":"Optional file containing features to exclude from analysis. Format : Plain text file with one feature identifier per line Example: YBR999W YCR888X control_gene batch_effect_gene","title":"--blacklist_file BLACKLIST_FILE"},{"location":"cli/linear-perturbation-binding-modeling/#-n_bootstraps-n_bootstraps","text":"Number of bootstrap samples for resampling analysis. - Default : 1000 - Range : 100-10000 recommended - Impact : Higher values provide more robust statistics but increase runtime","title":"--n_bootstraps N_BOOTSTRAPS"},{"location":"cli/linear-perturbation-binding-modeling/#-random_state-random_state","text":"Seed for reproducible bootstrap sampling. - Default : None (random seed each run) - Type : Integer - Note : If set, top-N modeling uses random_state + 10 for different sampling","title":"--random_state RANDOM_STATE"},{"location":"cli/linear-perturbation-binding-modeling/#-top_n-top_n","text":"Number of features to retain for second-round modeling. - Default : 600 - Impact : Higher values include more features but may reduce specificity","title":"--top_n TOP_N"},{"location":"cli/linear-perturbation-binding-modeling/#data-processing","text":"","title":"Data Processing"},{"location":"cli/linear-perturbation-binding-modeling/#-normalize_sample_weights","text":"Normalize bootstrap sample weights to sum to 1. - Default : False - Use case : When sample weights need explicit normalization","title":"--normalize_sample_weights"},{"location":"cli/linear-perturbation-binding-modeling/#-scale_by_std","text":"Center and scale the model matrix by standard deviation. - Default : False - Effect : Sets LassoCV fit_intercept parameter to False - Use case : When features have very different scales","title":"--scale_by_std"},{"location":"cli/linear-perturbation-binding-modeling/#feature-engineering","text":"","title":"Feature Engineering"},{"location":"cli/linear-perturbation-binding-modeling/#-row_max","text":"Include row maximum as an additional predictor in all-data modeling. - Default : False - Description : Adds the maximum binding value across all TFs for each gene","title":"--row_max"},{"location":"cli/linear-perturbation-binding-modeling/#-squared_ptf","text":"Include squared perturbed TF term in all-data modeling. - Default : False - Mathematical : Adds pTF\u00b2 term to capture non-linear relationships","title":"--squared_pTF"},{"location":"cli/linear-perturbation-binding-modeling/#-cubic_ptf","text":"Include cubic perturbed TF term in all-data modeling. - Default : False - Mathematical : Adds pTF\u00b3 term for higher-order non-linearities","title":"--cubic_pTF"},{"location":"cli/linear-perturbation-binding-modeling/#-ptf_main_effect","text":"Include perturbed transcription factor main effect in modeling formula. - Default : False - Description : Adds the pTF binding value as a direct predictor","title":"--ptf_main_effect"},{"location":"cli/linear-perturbation-binding-modeling/#-exclude_interactor_variables-exclude_interactor_variables","text":"Comma-separated list of variables to exclude from interaction terms. - Format : var1,var2,var3 or exclude_all - Example : --exclude_interactor_variables \"red_median,green_median\"","title":"--exclude_interactor_variables EXCLUDE_INTERACTOR_VARIABLES"},{"location":"cli/linear-perturbation-binding-modeling/#-add_model_variables-add_model_variables","text":"Comma-separated list of additional variables for all-data modeling. - Format : var1,var2,var3 - Example : --add_model_variables \"red_median,green_median\" - Effect : Adds ... + red_median + green_median to model formula","title":"--add_model_variables ADD_MODEL_VARIABLES"},{"location":"cli/linear-perturbation-binding-modeling/#binning-options","text":"","title":"Binning Options"},{"location":"cli/linear-perturbation-binding-modeling/#-bins-bins","text":"Comma-separated bin edges for data stratification. - Default : \"0,8,12,np.inf\" - Format : Numbers or np.inf for infinity - Example : --bins \"0,5,10,15,np.inf\" - Purpose : Stratifies data for cross-validation","title":"--bins BINS"},{"location":"cli/linear-perturbation-binding-modeling/#model-parameters","text":"","title":"Model Parameters"},{"location":"cli/linear-perturbation-binding-modeling/#-all_data_ci_level-all_data_ci_level","text":"Confidence interval threshold (%) for selecting significant coefficients in first stage. - Default : 98.0 - Range : 80.0-99.9 - Impact : Higher values are more stringent for feature selection","title":"--all_data_ci_level ALL_DATA_CI_LEVEL"},{"location":"cli/linear-perturbation-binding-modeling/#-topn_ci_level-topn_ci_level","text":"Confidence interval threshold for second round of modeling. - Default : 90.0 - Range : 80.0-99.9 - Typically : Lower than all_data_ci_level for refinement","title":"--topn_ci_level TOPN_CI_LEVEL"},{"location":"cli/linear-perturbation-binding-modeling/#-max_iter-max_iter","text":"Maximum iterations for LassoCV convergence. - Default : 10000 - Range : 1000-100000 - Increase if : Convergence warnings appear","title":"--max_iter MAX_ITER"},{"location":"cli/linear-perturbation-binding-modeling/#-iterative_dropout","text":"Enable iterative variable dropout based on confidence intervals. - Default : False - Description : Progressively removes non-significant variables during modeling","title":"--iterative_dropout"},{"location":"cli/linear-perturbation-binding-modeling/#-stabilization_ci_start-stabilization_ci_start","text":"Starting confidence interval for iterative dropout stabilization. - Default : 50.0 - Range : 50.0-95.0 - Used with : --iterative_dropout","title":"--stabilization_ci_start STABILIZATION_CI_START"},{"location":"cli/linear-perturbation-binding-modeling/#-stage4_lasso","text":"Use LassoCV-based interactor significance testing in Stage 4. - Default : False (uses linear regression) - Alternative : More conservative approach with regularization","title":"--stage4_lasso"},{"location":"cli/linear-perturbation-binding-modeling/#-stage4_topn","text":"Perform Stage 4 evaluation on top-n data instead of all data. - Default : False (uses all data) - Effect : Focuses final analysis on high-performing subset","title":"--stage4_topn"},{"location":"cli/linear-perturbation-binding-modeling/#output-control","text":"","title":"Output Control"},{"location":"cli/linear-perturbation-binding-modeling/#-output_dir-output_dir","text":"Base directory for saving results. - Default : \"./linear_perturbation_binding_modeling_results\" - Structure : Creates subdirectory per run with timestamp","title":"--output_dir OUTPUT_DIR"},{"location":"cli/linear-perturbation-binding-modeling/#-output_suffix-output_suffix","text":"Suffix to append to output subdirectory name. - Default : Empty string - Naming : {perturbed_tf}{suffix}_{timestamp} - Example : YPD1_custom_run_20240115_143022","title":"--output_suffix OUTPUT_SUFFIX"},{"location":"cli/linear-perturbation-binding-modeling/#system-options","text":"","title":"System Options"},{"location":"cli/linear-perturbation-binding-modeling/#-n_cpus-n_cpus","text":"Number of CPU cores for parallel processing. - Default : 4 - Recommendation : Match your system's available cores - Impact : Each LassoCV call uses specified cores","title":"--n_cpus N_CPUS"},{"location":"cli/linear-perturbation-binding-modeling/#output-structure","text":"Results are saved in a timestamped subdirectory: {output_dir}/{perturbed_tf}{output_suffix}_{timestamp}/ \u251c\u2500\u2500 all_data_results/ \u2502 \u251c\u2500\u2500 bootstrap_coefficients.csv # Coefficient estimates per bootstrap \u2502 \u251c\u2500\u2500 confidence_intervals.csv # CI and significance results \u2502 \u251c\u2500\u2500 model_statistics.csv # R\u00b2, CV scores, feature counts \u2502 \u251c\u2500\u2500 feature_importance.csv # Ranked feature importance \u2502 \u2514\u2500\u2500 diagnostic_plots/ # Visualization outputs \u251c\u2500\u2500 topn_results/ # Same structure for top-N analysis \u2502 \u251c\u2500\u2500 bootstrap_coefficients.csv \u2502 \u251c\u2500\u2500 confidence_intervals.csv \u2502 \u251c\u2500\u2500 model_statistics.csv \u2502 \u251c\u2500\u2500 feature_importance.csv \u2502 \u2514\u2500\u2500 diagnostic_plots/ \u251c\u2500\u2500 interactor_significance/ \u2502 \u251c\u2500\u2500 significance_results.csv # Interaction vs main effect tests \u2502 \u251c\u2500\u2500 comparison_statistics.csv # Statistical comparisons \u2502 \u251c\u2500\u2500 final_selection.csv # Selected significant interactions \u2502 \u2514\u2500\u2500 interaction_plots/ # Interaction visualizations \u251c\u2500\u2500 input_data/ \u2502 \u251c\u2500\u2500 processed_response.csv # Cleaned response data \u2502 \u251c\u2500\u2500 processed_predictors.csv # Cleaned predictor data \u2502 \u251c\u2500\u2500 bootstrap_indices.csv # Bootstrap sample indices \u2502 \u2514\u2500\u2500 data_summary.json # Data processing summary \u2514\u2500\u2500 tfbpmodeling_{timestamp}.log # Complete execution log","title":"Output Structure"},{"location":"cli/linear-perturbation-binding-modeling/#examples","text":"","title":"Examples"},{"location":"cli/linear-perturbation-binding-modeling/#basic-analysis","text":"python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data/expression.csv \\ --predictors_file data/binding.csv \\ --perturbed_tf YPD1","title":"Basic Analysis"},{"location":"cli/linear-perturbation-binding-modeling/#reproducible-high-quality-analysis","text":"python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data/expression.csv \\ --predictors_file data/binding.csv \\ --perturbed_tf YPD1 \\ --n_bootstraps 2000 \\ --top_n 500 \\ --all_data_ci_level 95 .0 \\ --topn_ci_level 85 .0 \\ --random_state 42 \\ --output_dir ./results \\ --output_suffix _high_quality","title":"Reproducible High-Quality Analysis"},{"location":"cli/linear-perturbation-binding-modeling/#advanced-feature-engineering","text":"python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data/expression.csv \\ --predictors_file data/binding.csv \\ --perturbed_tf YPD1 \\ --blacklist_file data/exclude_genes.txt \\ --row_max \\ --squared_pTF \\ --cubic_pTF \\ --ptf_main_effect \\ --add_model_variables \"red_median,green_median\" \\ --exclude_interactor_variables \"batch_effect\" \\ --normalize_sample_weights \\ --scale_by_std \\ --bins \"0,5,10,15,np.inf\"","title":"Advanced Feature Engineering"},{"location":"cli/linear-perturbation-binding-modeling/#high-performance-analysis","text":"python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data/expression.csv \\ --predictors_file data/binding.csv \\ --perturbed_tf YPD1 \\ --n_bootstraps 5000 \\ --n_cpus 16 \\ --max_iter 20000 \\ --iterative_dropout \\ --stage4_lasso \\ --stage4_topn","title":"High-Performance Analysis"},{"location":"cli/linear-perturbation-binding-modeling/#performance-considerations","text":"","title":"Performance Considerations"},{"location":"cli/linear-perturbation-binding-modeling/#memory-usage","text":"Scales with: features \u00d7 samples \u00d7 bootstrap samples Large datasets: reduce n_bootstraps or top_n Monitor with: htop or top during execution","title":"Memory Usage"},{"location":"cli/linear-perturbation-binding-modeling/#runtime-estimation","text":"For typical datasets (1000 features, 100 samples): - 1000 bootstraps: 10-30 minutes - 2000 bootstraps: 20-60 minutes - 5000 bootstraps: 1-3 hours","title":"Runtime Estimation"},{"location":"cli/linear-perturbation-binding-modeling/#optimization-tips","text":"Start with default parameters Use --random_state for reproducible development Increase --n_cpus to match available cores Use --log-level DEBUG for detailed progress tracking Consider --iterative_dropout for feature-rich datasets","title":"Optimization Tips"},{"location":"cli/linear-perturbation-binding-modeling/#error-handling","text":"","title":"Error Handling"},{"location":"cli/linear-perturbation-binding-modeling/#common-issues","text":"","title":"Common Issues"},{"location":"cli/linear-perturbation-binding-modeling/#file-format-errors","text":"# Verify CSV format head -5 data/expression.csv head -5 data/binding.csv","title":"File Format Errors"},{"location":"cli/linear-perturbation-binding-modeling/#missing-perturbed-tf","text":"# Check column names head -1 data/expression.csv | tr ',' '\\n' | grep YPD1","title":"Missing Perturbed TF"},{"location":"cli/linear-perturbation-binding-modeling/#convergence-issues","text":"# Increase iterations --max_iter 20000","title":"Convergence Issues"},{"location":"cli/linear-perturbation-binding-modeling/#memory-issues","text":"# Reduce computational load --n_bootstraps 500 --top_n 300","title":"Memory Issues"},{"location":"cli/linear-perturbation-binding-modeling/#related-commands","text":"CLI Overview : General CLI documentation Tutorials : Step-by-step examples API Reference : Programmatic usage","title":"Related Commands"},{"location":"cli/overview/","text":"CLI Reference Overview \u00b6 tfbpmodeling provides a comprehensive command-line interface for transcription factor binding and perturbation modeling. The CLI is designed to be user-friendly while offering advanced options for power users. Main Command Structure \u00b6 python -m tfbpmodeling [ GLOBAL_OPTIONS ] COMMAND [ COMMAND_OPTIONS ] Global Options \u00b6 Options that apply to all commands: Option Description Default Choices --log-level Set logging verbosity INFO DEBUG , INFO , WARNING , ERROR , CRITICAL --log-handler Log output destination console console , file Available Commands \u00b6 Currently, tfbpmodeling provides one main command: linear_perturbation_binding_modeling : Complete workflow for TFBP analysis Command Help System \u00b6 Getting Help \u00b6 Display main help: python -m tfbpmodeling --help Display command-specific help: python -m tfbpmodeling linear_perturbation_binding_modeling --help Help Output Format \u00b6 The CLI uses a custom help formatter that organizes options into logical groups: Input : Data files and basic parameters Feature Options : Feature engineering and selection Binning Options : Data stratification parameters Parameters : Model configuration and thresholds Output : Result directories and naming System : Performance and logging options Common Usage Patterns \u00b6 Basic Analysis \u00b6 python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data.csv \\ --predictors_file binding.csv \\ --perturbed_tf YPD1 Reproducible Analysis \u00b6 python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data.csv \\ --predictors_file binding.csv \\ --perturbed_tf YPD1 \\ --random_state 42 \\ --log-level DEBUG \\ --log-handler file High-Performance Analysis \u00b6 python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data.csv \\ --predictors_file binding.csv \\ --perturbed_tf YPD1 \\ --n_cpus 16 \\ --n_bootstraps 5000 Exit Codes \u00b6 The CLI uses standard exit codes: 0 : Success 1 : General error (invalid arguments, file not found, etc.) 2 : Modeling error (convergence failure, insufficient data, etc.) Logging \u00b6 Log Levels \u00b6 Level Description When to Use DEBUG Detailed diagnostic information Development, troubleshooting INFO General information about progress Normal operation WARNING Warning messages about potential issues Production monitoring ERROR Error messages for recoverable problems Error investigation CRITICAL Critical errors that stop execution System failures Log Handlers \u00b6 Console Handler (default) \u00b6 Outputs log messages to the terminal with color coding: --log-handler console File Handler \u00b6 Saves log messages to a timestamped file: --log-handler file Creates log files named: tfbpmodeling_YYYYMMDD-HHMMSS.log Example Log Output \u00b6 2024-01-15 14:30:22,123 - INFO - Starting linear perturbation binding modeling 2024-01-15 14:30:22,125 - INFO - Loading response data from: data/expression.csv 2024-01-15 14:30:22,234 - INFO - Loading predictor data from: data/binding.csv 2024-01-15 14:30:22,456 - INFO - Perturbed TF: YPD1 2024-01-15 14:30:22,458 - INFO - Starting Stage 1: Bootstrap modeling on all data 2024-01-15 14:30:22,459 - DEBUG - Bootstrap parameters: n_bootstraps=1000, random_state=None Configuration Files \u00b6 While tfbpmodeling doesn't currently support configuration files, you can create shell scripts or aliases for commonly used parameter combinations: Shell Script Example \u00b6 #!/bin/bash # run_analysis.sh RESPONSE_FILE = \" $1 \" PREDICTORS_FILE = \" $2 \" PERTURBED_TF = \" $3 \" OUTPUT_DIR = \" ${ 4 :- ./results } \" python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file \" $RESPONSE_FILE \" \\ --predictors_file \" $PREDICTORS_FILE \" \\ --perturbed_tf \" $PERTURBED_TF \" \\ --output_dir \" $OUTPUT_DIR \" \\ --n_bootstraps 2000 \\ --squared_pTF \\ --ptf_main_effect \\ --iterative_dropout \\ --random_state 42 \\ --log-level INFO \\ --log-handler file Usage: ./run_analysis.sh expression.csv binding.csv YPD1 my_results Bash Alias Example \u00b6 # Add to ~/.bashrc or ~/.bash_profile alias tfbp-basic = 'python -m tfbpmodeling linear_perturbation_binding_modeling' alias tfbp-advanced = 'python -m tfbpmodeling linear_perturbation_binding_modeling --n_bootstraps 2000 --squared_pTF --ptf_main_effect --iterative_dropout --random_state 42' Usage: tfbp-basic --response_file data.csv --predictors_file binding.csv --perturbed_tf YPD1 Error Handling \u00b6 Common Error Messages \u00b6 File Not Found \u00b6 ERROR: Response file not found: data/missing_file.csv Solution : Verify file paths and permissions Invalid Perturbed TF \u00b6 ERROR: Perturbed TF 'INVALID_TF' not found in response file columns Solution : Check TF name spelling and presence in response file Insufficient Data \u00b6 ERROR: Insufficient data after filtering. Found 5 samples, minimum required: 10 Solution : Check data quality, reduce filtering, or provide more samples Convergence Issues \u00b6 WARNING: LassoCV failed to converge for 15/1000 bootstrap samples Solution : Increase --max_iter or check data preprocessing Debugging Tips \u00b6 Start with defaults : Use minimal parameters first Enable debug logging : Add --log-level DEBUG Use file logging : Add --log-handler file to preserve logs Check input data : Verify file formats and content Reduce complexity : Lower bootstrap samples for initial testing Performance Considerations \u00b6 Memory Usage \u00b6 Memory usage scales with: number of features \u00d7 number of samples \u00d7 bootstrap samples For large datasets, consider reducing --n_bootstraps or --top_n CPU Usage \u00b6 Set --n_cpus to match your system capabilities Each LassoCV call uses the specified number of CPUs Default of 4 CPUs works well for most systems Runtime Estimation \u00b6 Approximate runtime factors: - Bootstrap samples : Linear scaling - Feature count : Quadratic scaling with regularization - Sample count : Linear scaling - CPU cores : Near-linear speedup For typical datasets (1000 features, 100 samples): - 1000 bootstraps: ~10-30 minutes - 2000 bootstraps: ~20-60 minutes - 5000 bootstraps: ~1-3 hours Next Steps \u00b6 Linear Perturbation Binding Modeling : Detailed documentation for the main command Tutorials : Step-by-step examples API Reference : Programmatic usage documentation","title":"Overview"},{"location":"cli/overview/#cli-reference-overview","text":"tfbpmodeling provides a comprehensive command-line interface for transcription factor binding and perturbation modeling. The CLI is designed to be user-friendly while offering advanced options for power users.","title":"CLI Reference Overview"},{"location":"cli/overview/#main-command-structure","text":"python -m tfbpmodeling [ GLOBAL_OPTIONS ] COMMAND [ COMMAND_OPTIONS ]","title":"Main Command Structure"},{"location":"cli/overview/#global-options","text":"Options that apply to all commands: Option Description Default Choices --log-level Set logging verbosity INFO DEBUG , INFO , WARNING , ERROR , CRITICAL --log-handler Log output destination console console , file","title":"Global Options"},{"location":"cli/overview/#available-commands","text":"Currently, tfbpmodeling provides one main command: linear_perturbation_binding_modeling : Complete workflow for TFBP analysis","title":"Available Commands"},{"location":"cli/overview/#command-help-system","text":"","title":"Command Help System"},{"location":"cli/overview/#getting-help","text":"Display main help: python -m tfbpmodeling --help Display command-specific help: python -m tfbpmodeling linear_perturbation_binding_modeling --help","title":"Getting Help"},{"location":"cli/overview/#help-output-format","text":"The CLI uses a custom help formatter that organizes options into logical groups: Input : Data files and basic parameters Feature Options : Feature engineering and selection Binning Options : Data stratification parameters Parameters : Model configuration and thresholds Output : Result directories and naming System : Performance and logging options","title":"Help Output Format"},{"location":"cli/overview/#common-usage-patterns","text":"","title":"Common Usage Patterns"},{"location":"cli/overview/#basic-analysis","text":"python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data.csv \\ --predictors_file binding.csv \\ --perturbed_tf YPD1","title":"Basic Analysis"},{"location":"cli/overview/#reproducible-analysis","text":"python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data.csv \\ --predictors_file binding.csv \\ --perturbed_tf YPD1 \\ --random_state 42 \\ --log-level DEBUG \\ --log-handler file","title":"Reproducible Analysis"},{"location":"cli/overview/#high-performance-analysis","text":"python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data.csv \\ --predictors_file binding.csv \\ --perturbed_tf YPD1 \\ --n_cpus 16 \\ --n_bootstraps 5000","title":"High-Performance Analysis"},{"location":"cli/overview/#exit-codes","text":"The CLI uses standard exit codes: 0 : Success 1 : General error (invalid arguments, file not found, etc.) 2 : Modeling error (convergence failure, insufficient data, etc.)","title":"Exit Codes"},{"location":"cli/overview/#logging","text":"","title":"Logging"},{"location":"cli/overview/#log-levels","text":"Level Description When to Use DEBUG Detailed diagnostic information Development, troubleshooting INFO General information about progress Normal operation WARNING Warning messages about potential issues Production monitoring ERROR Error messages for recoverable problems Error investigation CRITICAL Critical errors that stop execution System failures","title":"Log Levels"},{"location":"cli/overview/#log-handlers","text":"","title":"Log Handlers"},{"location":"cli/overview/#console-handler-default","text":"Outputs log messages to the terminal with color coding: --log-handler console","title":"Console Handler (default)"},{"location":"cli/overview/#file-handler","text":"Saves log messages to a timestamped file: --log-handler file Creates log files named: tfbpmodeling_YYYYMMDD-HHMMSS.log","title":"File Handler"},{"location":"cli/overview/#example-log-output","text":"2024-01-15 14:30:22,123 - INFO - Starting linear perturbation binding modeling 2024-01-15 14:30:22,125 - INFO - Loading response data from: data/expression.csv 2024-01-15 14:30:22,234 - INFO - Loading predictor data from: data/binding.csv 2024-01-15 14:30:22,456 - INFO - Perturbed TF: YPD1 2024-01-15 14:30:22,458 - INFO - Starting Stage 1: Bootstrap modeling on all data 2024-01-15 14:30:22,459 - DEBUG - Bootstrap parameters: n_bootstraps=1000, random_state=None","title":"Example Log Output"},{"location":"cli/overview/#configuration-files","text":"While tfbpmodeling doesn't currently support configuration files, you can create shell scripts or aliases for commonly used parameter combinations:","title":"Configuration Files"},{"location":"cli/overview/#shell-script-example","text":"#!/bin/bash # run_analysis.sh RESPONSE_FILE = \" $1 \" PREDICTORS_FILE = \" $2 \" PERTURBED_TF = \" $3 \" OUTPUT_DIR = \" ${ 4 :- ./results } \" python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file \" $RESPONSE_FILE \" \\ --predictors_file \" $PREDICTORS_FILE \" \\ --perturbed_tf \" $PERTURBED_TF \" \\ --output_dir \" $OUTPUT_DIR \" \\ --n_bootstraps 2000 \\ --squared_pTF \\ --ptf_main_effect \\ --iterative_dropout \\ --random_state 42 \\ --log-level INFO \\ --log-handler file Usage: ./run_analysis.sh expression.csv binding.csv YPD1 my_results","title":"Shell Script Example"},{"location":"cli/overview/#bash-alias-example","text":"# Add to ~/.bashrc or ~/.bash_profile alias tfbp-basic = 'python -m tfbpmodeling linear_perturbation_binding_modeling' alias tfbp-advanced = 'python -m tfbpmodeling linear_perturbation_binding_modeling --n_bootstraps 2000 --squared_pTF --ptf_main_effect --iterative_dropout --random_state 42' Usage: tfbp-basic --response_file data.csv --predictors_file binding.csv --perturbed_tf YPD1","title":"Bash Alias Example"},{"location":"cli/overview/#error-handling","text":"","title":"Error Handling"},{"location":"cli/overview/#common-error-messages","text":"","title":"Common Error Messages"},{"location":"cli/overview/#file-not-found","text":"ERROR: Response file not found: data/missing_file.csv Solution : Verify file paths and permissions","title":"File Not Found"},{"location":"cli/overview/#invalid-perturbed-tf","text":"ERROR: Perturbed TF 'INVALID_TF' not found in response file columns Solution : Check TF name spelling and presence in response file","title":"Invalid Perturbed TF"},{"location":"cli/overview/#insufficient-data","text":"ERROR: Insufficient data after filtering. Found 5 samples, minimum required: 10 Solution : Check data quality, reduce filtering, or provide more samples","title":"Insufficient Data"},{"location":"cli/overview/#convergence-issues","text":"WARNING: LassoCV failed to converge for 15/1000 bootstrap samples Solution : Increase --max_iter or check data preprocessing","title":"Convergence Issues"},{"location":"cli/overview/#debugging-tips","text":"Start with defaults : Use minimal parameters first Enable debug logging : Add --log-level DEBUG Use file logging : Add --log-handler file to preserve logs Check input data : Verify file formats and content Reduce complexity : Lower bootstrap samples for initial testing","title":"Debugging Tips"},{"location":"cli/overview/#performance-considerations","text":"","title":"Performance Considerations"},{"location":"cli/overview/#memory-usage","text":"Memory usage scales with: number of features \u00d7 number of samples \u00d7 bootstrap samples For large datasets, consider reducing --n_bootstraps or --top_n","title":"Memory Usage"},{"location":"cli/overview/#cpu-usage","text":"Set --n_cpus to match your system capabilities Each LassoCV call uses the specified number of CPUs Default of 4 CPUs works well for most systems","title":"CPU Usage"},{"location":"cli/overview/#runtime-estimation","text":"Approximate runtime factors: - Bootstrap samples : Linear scaling - Feature count : Quadratic scaling with regularization - Sample count : Linear scaling - CPU cores : Near-linear speedup For typical datasets (1000 features, 100 samples): - 1000 bootstraps: ~10-30 minutes - 2000 bootstraps: ~20-60 minutes - 5000 bootstraps: ~1-3 hours","title":"Runtime Estimation"},{"location":"cli/overview/#next-steps","text":"Linear Perturbation Binding Modeling : Detailed documentation for the main command Tutorials : Step-by-step examples API Reference : Programmatic usage documentation","title":"Next Steps"},{"location":"development/contributing/","text":"Contributing to tfbpmodeling \u00b6 We welcome contributions to tfbpmodeling! This guide will help you get started with contributing code, documentation, or bug reports. Getting Started \u00b6 Development Setup \u00b6 Fork and Clone # Fork the repository on GitHub git clone https://github.com/YOUR_USERNAME/tfbpmodeling.git cd tfbpmodeling # Add upstream remote git remote add upstream https://github.com/BrentLab/tfbpmodeling.git Install Dependencies # Install Poetry if you haven't already pip install poetry # Configure Poetry (recommended) poetry config virtualenvs.in-project true # Install all dependencies including dev tools poetry install Set Up Pre-commit Hooks poetry run pre-commit install Development Workflow \u00b6 Create Feature Branch # Start from dev branch git checkout dev git pull upstream dev # Create feature branch git checkout -b feature/your-feature-name Make Changes Write code following our style guidelines Add tests for new functionality Update documentation as needed Test Your Changes # Run tests poetry run pytest # Run with coverage poetry run pytest --cov --cov-branch --cov-report = xml # Check code style poetry run black . poetry run flake8 poetry run mypy tfbpmodeling Commit and Push git add . git commit -m \"Add feature: description of your changes\" git push origin feature/your-feature-name Create Pull Request Open a pull request against the dev branch Provide clear description of changes Reference any related issues Project Structure \u00b6 tfbpmodeling/ \u251c\u2500\u2500 tfbpmodeling/ # Main package \u2502 \u251c\u2500\u2500 __main__.py # CLI entry point \u2502 \u251c\u2500\u2500 interface.py # Main workflow functions \u2502 \u251c\u2500\u2500 modeling_input_data.py \u2502 \u251c\u2500\u2500 bootstrapped_input_data.py \u2502 \u251c\u2500\u2500 bootstrap_model_results.py \u2502 \u251c\u2500\u2500 evaluate_interactor_significance_*.py \u2502 \u251c\u2500\u2500 stratified_cv*.py \u2502 \u251c\u2500\u2500 utils/ # Utility functions \u2502 \u2514\u2500\u2500 tests/ # Test suite \u251c\u2500\u2500 docs/ # Documentation (MkDocs) \u251c\u2500\u2500 tmp/ # Exploratory development \u251c\u2500\u2500 pyproject.toml # Poetry configuration \u251c\u2500\u2500 mkdocs.yml # Documentation configuration \u251c\u2500\u2500 CLAUDE.md # Claude Code instructions \u2514\u2500\u2500 README.md Core Modules \u00b6 __main__.py : CLI entry point with argparse setup interface.py : Main workflow orchestration and CLI functions modeling_input_data.py : Data loading and preprocessing bootstrapped_input_data.py : Bootstrap resampling functionality bootstrap_model_results.py : Results aggregation and statistics evaluate_interactor_significance_*.py : Statistical significance testing stratified_cv*.py : Cross-validation with stratification utils/ : Helper functions for data manipulation Exploratory Development \u00b6 The tmp/ directory is set up for exploratory data analysis and interactive development: Jupyter notebooks : Can be run from tmp/ in the virtual environment iPython kernel : Installed in the development environment Version control : Files in tmp/ are excluded from git tracking Testing : tmp/ directory is ignored by pytest Experimentation : Safe space for prototyping and data exploration See tmp/README.md for more information about using this directory. Code Standards \u00b6 Style Guidelines \u00b6 We use automated tools to maintain consistent code style: Black : Code formatting (88 character line length) Flake8 : Style checking and linting MyPy : Type checking isort : Import sorting Code Quality \u00b6 Type Hints : All functions should have type hints Docstrings : Use Sphinx-style docstrings for all public functions Tests : Write tests for all new functionality No Secrets : Never commit API keys, passwords, or other secrets Example Code Style \u00b6 from typing import List , Optional , Tuple import numpy as np import pandas as pd def process_data ( data : pd . DataFrame , threshold : float = 0.05 , normalize : bool = True ) -> Tuple [ pd . DataFrame , List [ str ]]: \\ \" \\\"\\\" Process input data with filtering and normalization . : param data : Input dataframe with features as columns : param threshold : Minimum value threshold for filtering : param normalize : Whether to normalize data to unit variance : return : Processed dataframe and list of excluded features \\ \" \\\"\\\" excluded_features : List [ str ] = [] # Filter low-variance features for col in data . columns : if data [ col ] . var () < threshold : excluded_features . append ( col ) processed_data = data . drop ( columns = excluded_features ) if normalize : processed_data = ( processed_data - processed_data . mean ()) / processed_data . std () return processed_data , excluded_features Testing \u00b6 Test Structure \u00b6 Tests are located in tfbpmodeling/tests/ : tests/ \u251c\u2500\u2500 test_interface.py \u251c\u2500\u2500 test_modeling_input_data.py \u251c\u2500\u2500 test_bootstrapped_input_data.py \u251c\u2500\u2500 test_utils.py \u2514\u2500\u2500 fixtures/ \u251c\u2500\u2500 sample_expression.csv \u2514\u2500\u2500 sample_binding.csv Writing Tests \u00b6 Use pytest for all tests: import pytest import pandas as pd from tfbpmodeling.modeling_input_data import ModelingInputData class TestModelingInputData : \\ \" \\\"\\\" Test suite for ModelingInputData class. \\\"\\\"\\\" def test_initialization ( self , sample_data_files ): \\ \" \\\"\\\" Test basic initialization. \\\"\\\"\\\" data = ModelingInputData ( response_file = sample_data_files [ 'response' ], predictors_file = sample_data_files [ 'predictors' ], perturbed_tf = 'YPD1' ) assert data is not None assert len ( data . get_feature_names ()) > 0 def test_invalid_perturbed_tf ( self , sample_data_files ): \\ \" \\\"\\\" Test error handling for invalid perturbed TF. \\\"\\\"\\\" with pytest . raises ( KeyError , match = \"not found in response\" ): ModelingInputData ( response_file = sample_data_files [ 'response' ], predictors_file = sample_data_files [ 'predictors' ], perturbed_tf = 'INVALID_TF' ) @pytest . mark . parametrize ( \"normalize\" , [ True , False ]) def test_normalization_options ( self , sample_data_files , normalize ): \\ \" \\\"\\\" Test normalization parameter. \\\"\\\"\\\" data = ModelingInputData ( response_file = sample_data_files [ 'response' ], predictors_file = sample_data_files [ 'predictors' ], perturbed_tf = 'YPD1' , normalize_weights = normalize ) # Test that normalization was applied correctly assert data . normalize_weights == normalize @pytest . fixture def sample_data_files ( tmp_path ): \\ \" \\\"\\\" Create sample data files for testing. \\\"\\\"\\\" # Create sample response data response_data = pd . DataFrame ({ 'sample1' : [ 0.1 , 0.2 , 0.3 ], 'sample2' : [ 0.4 , 0.5 , 0.6 ], 'YPD1' : [ 0.7 , 0.8 , 0.9 ] }, index = [ 'gene1' , 'gene2' , 'gene3' ]) # Create sample predictor data predictor_data = pd . DataFrame ({ 'TF1' : [ 0.1 , 0.2 , 0.3 ], 'TF2' : [ 0.4 , 0.5 , 0.6 ] }, index = [ 'gene1' , 'gene2' , 'gene3' ]) # Save to temporary files response_file = tmp_path / \"response.csv\" predictor_file = tmp_path / \"predictors.csv\" response_data . to_csv ( response_file , index_label = 'gene_id' ) predictor_data . to_csv ( predictor_file , index_label = 'gene_id' ) return { 'response' : str ( response_file ), 'predictors' : str ( predictor_file ) } Running Tests \u00b6 # Run all tests poetry run pytest # Run specific test file poetry run pytest tfbpmodeling/tests/test_interface.py # Run with coverage poetry run pytest --cov = tfbpmodeling --cov-report = html # Run tests matching pattern poetry run pytest -k \"test_modeling\" # Run tests with verbose output poetry run pytest -v Documentation \u00b6 Documentation Structure \u00b6 Documentation is built with MkDocs and uses the Material theme: docs/ \u251c\u2500\u2500 index.md \u251c\u2500\u2500 getting-started/ \u2502 \u251c\u2500\u2500 installation.md \u2502 \u2514\u2500\u2500 quickstart.md \u251c\u2500\u2500 cli/ \u2502 \u251c\u2500\u2500 overview.md \u2502 \u2514\u2500\u2500 linear-perturbation-binding-modeling.md \u251c\u2500\u2500 tutorials/ \u2502 \u251c\u2500\u2500 basic-workflow.md \u2502 \u2514\u2500\u2500 advanced-features.md \u251c\u2500\u2500 api/ \u2502 \u251c\u2500\u2500 interface.md \u2502 \u2514\u2500\u2500 modeling_input_data.md \u2514\u2500\u2500 development/ \u251c\u2500\u2500 contributing.md \u2514\u2500\u2500 testing.md Writing Documentation \u00b6 Use clear, concise language Include code examples for all features Provide both basic and advanced usage examples Link between related sections Building Documentation \u00b6 # Serve documentation locally with live reload mkdocs serve # Build documentation mkdocs build # Deploy to GitHub Pages (maintainers only) poetry run mkdocs gh-deploy Issue Reporting \u00b6 Bug Reports \u00b6 When reporting bugs, please include: Environment Information Python version tfbpmodeling version Operating system Reproduction Steps Minimal code example Input data characteristics Expected vs actual behavior Error Messages Complete error traceback Log output if available Feature Requests \u00b6 For feature requests, provide: Use Case : Describe the problem you're trying to solve Proposed Solution : How you envision the feature working Alternatives : Other approaches you've considered Examples : Code examples of how it would be used Development Guidelines \u00b6 Branch Management \u00b6 main : Stable release branch dev : Development branch for integration feature/ *: Feature development branches hotfix/ *: Critical bug fixes Commit Messages \u00b6 Use clear, descriptive commit messages: Add bootstrap confidence interval calculation - Implement percentile method for CI estimation - Add support for custom confidence levels - Include tests for edge cases - Update documentation with examples Code Review Process \u00b6 All changes require pull request review CI tests must pass Code coverage should not decrease Documentation must be updated for new features At least one maintainer approval required Release Process \u00b6 Features merged to dev branch Testing and integration on dev Release candidate created Final testing and documentation review Merge to main and tag release Update changelog and documentation Getting Help \u00b6 Communication Channels \u00b6 GitHub Issues : Bug reports and feature requests GitHub Discussions : Questions and general discussion Pull Request Reviews : Code-specific feedback Maintainer Contact \u00b6 Chase Mateusiak : Lead developer Michael Brent : Principal investigator Resources \u00b6 Project Documentation GitHub Repository Issue Tracker Troubleshooting \u00b6 Development Environment Issues \u00b6 Poetry Installation Problems \u00b6 If Poetry installation fails, try the alternative installation method: pip install poetry Virtual Environment Issues \u00b6 If you encounter virtual environment problems: # Remove existing environment poetry env remove python # Reinstall dependencies poetry install Pre-commit Hook Failures \u00b6 If pre-commit hooks fail during commits: # Run pre-commit manually to see specific issues poetry run pre-commit run --all-files # Fix any reported issues and commit again Documentation Build Issues \u00b6 If mkdocs build fails: # Check for missing dependencies poetry install # Try building with verbose output mkdocs build --verbose # Check configuration mkdocs config Test Failures \u00b6 If tests fail unexpectedly: # Run tests with verbose output poetry run pytest -v # Run specific failing test poetry run pytest path/to/failing_test.py::test_name -v # Check test dependencies poetry run pytest --collect-only Common Development Issues \u00b6 Import Errors \u00b6 # Ensure package is installed in development mode poetry install # Check Python path python -c \"import tfbpmodeling; print(tfbpmodeling.__file__)\" Module Not Found \u00b6 # Verify virtual environment is activated which python poetry env info # Reinstall in development mode poetry install --no-deps Permission Errors \u00b6 # Check file permissions ls -la # Fix permissions if needed chmod +x scripts/your_script.sh Getting Help with Development \u00b6 If you encounter issues not covered here: Search existing issues : Check if someone else has faced the same problem Create a detailed issue : Include error messages, environment info, and steps to reproduce Join discussions : Use GitHub Discussions for questions and help Contact maintainers : Reach out directly for urgent issues Recognition \u00b6 Contributors are recognized in: CHANGELOG.md : Feature and bug fix credits AUTHORS.md : Comprehensive contributor list Release Notes : Major contribution highlights Documentation : Author attributions for significant additions Thank you for contributing to tfbpmodeling!","title":"Contributing"},{"location":"development/contributing/#contributing-to-tfbpmodeling","text":"We welcome contributions to tfbpmodeling! This guide will help you get started with contributing code, documentation, or bug reports.","title":"Contributing to tfbpmodeling"},{"location":"development/contributing/#getting-started","text":"","title":"Getting Started"},{"location":"development/contributing/#development-setup","text":"Fork and Clone # Fork the repository on GitHub git clone https://github.com/YOUR_USERNAME/tfbpmodeling.git cd tfbpmodeling # Add upstream remote git remote add upstream https://github.com/BrentLab/tfbpmodeling.git Install Dependencies # Install Poetry if you haven't already pip install poetry # Configure Poetry (recommended) poetry config virtualenvs.in-project true # Install all dependencies including dev tools poetry install Set Up Pre-commit Hooks poetry run pre-commit install","title":"Development Setup"},{"location":"development/contributing/#development-workflow","text":"Create Feature Branch # Start from dev branch git checkout dev git pull upstream dev # Create feature branch git checkout -b feature/your-feature-name Make Changes Write code following our style guidelines Add tests for new functionality Update documentation as needed Test Your Changes # Run tests poetry run pytest # Run with coverage poetry run pytest --cov --cov-branch --cov-report = xml # Check code style poetry run black . poetry run flake8 poetry run mypy tfbpmodeling Commit and Push git add . git commit -m \"Add feature: description of your changes\" git push origin feature/your-feature-name Create Pull Request Open a pull request against the dev branch Provide clear description of changes Reference any related issues","title":"Development Workflow"},{"location":"development/contributing/#project-structure","text":"tfbpmodeling/ \u251c\u2500\u2500 tfbpmodeling/ # Main package \u2502 \u251c\u2500\u2500 __main__.py # CLI entry point \u2502 \u251c\u2500\u2500 interface.py # Main workflow functions \u2502 \u251c\u2500\u2500 modeling_input_data.py \u2502 \u251c\u2500\u2500 bootstrapped_input_data.py \u2502 \u251c\u2500\u2500 bootstrap_model_results.py \u2502 \u251c\u2500\u2500 evaluate_interactor_significance_*.py \u2502 \u251c\u2500\u2500 stratified_cv*.py \u2502 \u251c\u2500\u2500 utils/ # Utility functions \u2502 \u2514\u2500\u2500 tests/ # Test suite \u251c\u2500\u2500 docs/ # Documentation (MkDocs) \u251c\u2500\u2500 tmp/ # Exploratory development \u251c\u2500\u2500 pyproject.toml # Poetry configuration \u251c\u2500\u2500 mkdocs.yml # Documentation configuration \u251c\u2500\u2500 CLAUDE.md # Claude Code instructions \u2514\u2500\u2500 README.md","title":"Project Structure"},{"location":"development/contributing/#core-modules","text":"__main__.py : CLI entry point with argparse setup interface.py : Main workflow orchestration and CLI functions modeling_input_data.py : Data loading and preprocessing bootstrapped_input_data.py : Bootstrap resampling functionality bootstrap_model_results.py : Results aggregation and statistics evaluate_interactor_significance_*.py : Statistical significance testing stratified_cv*.py : Cross-validation with stratification utils/ : Helper functions for data manipulation","title":"Core Modules"},{"location":"development/contributing/#exploratory-development","text":"The tmp/ directory is set up for exploratory data analysis and interactive development: Jupyter notebooks : Can be run from tmp/ in the virtual environment iPython kernel : Installed in the development environment Version control : Files in tmp/ are excluded from git tracking Testing : tmp/ directory is ignored by pytest Experimentation : Safe space for prototyping and data exploration See tmp/README.md for more information about using this directory.","title":"Exploratory Development"},{"location":"development/contributing/#code-standards","text":"","title":"Code Standards"},{"location":"development/contributing/#style-guidelines","text":"We use automated tools to maintain consistent code style: Black : Code formatting (88 character line length) Flake8 : Style checking and linting MyPy : Type checking isort : Import sorting","title":"Style Guidelines"},{"location":"development/contributing/#code-quality","text":"Type Hints : All functions should have type hints Docstrings : Use Sphinx-style docstrings for all public functions Tests : Write tests for all new functionality No Secrets : Never commit API keys, passwords, or other secrets","title":"Code Quality"},{"location":"development/contributing/#example-code-style","text":"from typing import List , Optional , Tuple import numpy as np import pandas as pd def process_data ( data : pd . DataFrame , threshold : float = 0.05 , normalize : bool = True ) -> Tuple [ pd . DataFrame , List [ str ]]: \\ \" \\\"\\\" Process input data with filtering and normalization . : param data : Input dataframe with features as columns : param threshold : Minimum value threshold for filtering : param normalize : Whether to normalize data to unit variance : return : Processed dataframe and list of excluded features \\ \" \\\"\\\" excluded_features : List [ str ] = [] # Filter low-variance features for col in data . columns : if data [ col ] . var () < threshold : excluded_features . append ( col ) processed_data = data . drop ( columns = excluded_features ) if normalize : processed_data = ( processed_data - processed_data . mean ()) / processed_data . std () return processed_data , excluded_features","title":"Example Code Style"},{"location":"development/contributing/#testing","text":"","title":"Testing"},{"location":"development/contributing/#test-structure","text":"Tests are located in tfbpmodeling/tests/ : tests/ \u251c\u2500\u2500 test_interface.py \u251c\u2500\u2500 test_modeling_input_data.py \u251c\u2500\u2500 test_bootstrapped_input_data.py \u251c\u2500\u2500 test_utils.py \u2514\u2500\u2500 fixtures/ \u251c\u2500\u2500 sample_expression.csv \u2514\u2500\u2500 sample_binding.csv","title":"Test Structure"},{"location":"development/contributing/#writing-tests","text":"Use pytest for all tests: import pytest import pandas as pd from tfbpmodeling.modeling_input_data import ModelingInputData class TestModelingInputData : \\ \" \\\"\\\" Test suite for ModelingInputData class. \\\"\\\"\\\" def test_initialization ( self , sample_data_files ): \\ \" \\\"\\\" Test basic initialization. \\\"\\\"\\\" data = ModelingInputData ( response_file = sample_data_files [ 'response' ], predictors_file = sample_data_files [ 'predictors' ], perturbed_tf = 'YPD1' ) assert data is not None assert len ( data . get_feature_names ()) > 0 def test_invalid_perturbed_tf ( self , sample_data_files ): \\ \" \\\"\\\" Test error handling for invalid perturbed TF. \\\"\\\"\\\" with pytest . raises ( KeyError , match = \"not found in response\" ): ModelingInputData ( response_file = sample_data_files [ 'response' ], predictors_file = sample_data_files [ 'predictors' ], perturbed_tf = 'INVALID_TF' ) @pytest . mark . parametrize ( \"normalize\" , [ True , False ]) def test_normalization_options ( self , sample_data_files , normalize ): \\ \" \\\"\\\" Test normalization parameter. \\\"\\\"\\\" data = ModelingInputData ( response_file = sample_data_files [ 'response' ], predictors_file = sample_data_files [ 'predictors' ], perturbed_tf = 'YPD1' , normalize_weights = normalize ) # Test that normalization was applied correctly assert data . normalize_weights == normalize @pytest . fixture def sample_data_files ( tmp_path ): \\ \" \\\"\\\" Create sample data files for testing. \\\"\\\"\\\" # Create sample response data response_data = pd . DataFrame ({ 'sample1' : [ 0.1 , 0.2 , 0.3 ], 'sample2' : [ 0.4 , 0.5 , 0.6 ], 'YPD1' : [ 0.7 , 0.8 , 0.9 ] }, index = [ 'gene1' , 'gene2' , 'gene3' ]) # Create sample predictor data predictor_data = pd . DataFrame ({ 'TF1' : [ 0.1 , 0.2 , 0.3 ], 'TF2' : [ 0.4 , 0.5 , 0.6 ] }, index = [ 'gene1' , 'gene2' , 'gene3' ]) # Save to temporary files response_file = tmp_path / \"response.csv\" predictor_file = tmp_path / \"predictors.csv\" response_data . to_csv ( response_file , index_label = 'gene_id' ) predictor_data . to_csv ( predictor_file , index_label = 'gene_id' ) return { 'response' : str ( response_file ), 'predictors' : str ( predictor_file ) }","title":"Writing Tests"},{"location":"development/contributing/#running-tests","text":"# Run all tests poetry run pytest # Run specific test file poetry run pytest tfbpmodeling/tests/test_interface.py # Run with coverage poetry run pytest --cov = tfbpmodeling --cov-report = html # Run tests matching pattern poetry run pytest -k \"test_modeling\" # Run tests with verbose output poetry run pytest -v","title":"Running Tests"},{"location":"development/contributing/#documentation","text":"","title":"Documentation"},{"location":"development/contributing/#documentation-structure","text":"Documentation is built with MkDocs and uses the Material theme: docs/ \u251c\u2500\u2500 index.md \u251c\u2500\u2500 getting-started/ \u2502 \u251c\u2500\u2500 installation.md \u2502 \u2514\u2500\u2500 quickstart.md \u251c\u2500\u2500 cli/ \u2502 \u251c\u2500\u2500 overview.md \u2502 \u2514\u2500\u2500 linear-perturbation-binding-modeling.md \u251c\u2500\u2500 tutorials/ \u2502 \u251c\u2500\u2500 basic-workflow.md \u2502 \u2514\u2500\u2500 advanced-features.md \u251c\u2500\u2500 api/ \u2502 \u251c\u2500\u2500 interface.md \u2502 \u2514\u2500\u2500 modeling_input_data.md \u2514\u2500\u2500 development/ \u251c\u2500\u2500 contributing.md \u2514\u2500\u2500 testing.md","title":"Documentation Structure"},{"location":"development/contributing/#writing-documentation","text":"Use clear, concise language Include code examples for all features Provide both basic and advanced usage examples Link between related sections","title":"Writing Documentation"},{"location":"development/contributing/#building-documentation","text":"# Serve documentation locally with live reload mkdocs serve # Build documentation mkdocs build # Deploy to GitHub Pages (maintainers only) poetry run mkdocs gh-deploy","title":"Building Documentation"},{"location":"development/contributing/#issue-reporting","text":"","title":"Issue Reporting"},{"location":"development/contributing/#bug-reports","text":"When reporting bugs, please include: Environment Information Python version tfbpmodeling version Operating system Reproduction Steps Minimal code example Input data characteristics Expected vs actual behavior Error Messages Complete error traceback Log output if available","title":"Bug Reports"},{"location":"development/contributing/#feature-requests","text":"For feature requests, provide: Use Case : Describe the problem you're trying to solve Proposed Solution : How you envision the feature working Alternatives : Other approaches you've considered Examples : Code examples of how it would be used","title":"Feature Requests"},{"location":"development/contributing/#development-guidelines","text":"","title":"Development Guidelines"},{"location":"development/contributing/#branch-management","text":"main : Stable release branch dev : Development branch for integration feature/ *: Feature development branches hotfix/ *: Critical bug fixes","title":"Branch Management"},{"location":"development/contributing/#commit-messages","text":"Use clear, descriptive commit messages: Add bootstrap confidence interval calculation - Implement percentile method for CI estimation - Add support for custom confidence levels - Include tests for edge cases - Update documentation with examples","title":"Commit Messages"},{"location":"development/contributing/#code-review-process","text":"All changes require pull request review CI tests must pass Code coverage should not decrease Documentation must be updated for new features At least one maintainer approval required","title":"Code Review Process"},{"location":"development/contributing/#release-process","text":"Features merged to dev branch Testing and integration on dev Release candidate created Final testing and documentation review Merge to main and tag release Update changelog and documentation","title":"Release Process"},{"location":"development/contributing/#getting-help","text":"","title":"Getting Help"},{"location":"development/contributing/#communication-channels","text":"GitHub Issues : Bug reports and feature requests GitHub Discussions : Questions and general discussion Pull Request Reviews : Code-specific feedback","title":"Communication Channels"},{"location":"development/contributing/#maintainer-contact","text":"Chase Mateusiak : Lead developer Michael Brent : Principal investigator","title":"Maintainer Contact"},{"location":"development/contributing/#resources","text":"Project Documentation GitHub Repository Issue Tracker","title":"Resources"},{"location":"development/contributing/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"development/contributing/#development-environment-issues","text":"","title":"Development Environment Issues"},{"location":"development/contributing/#poetry-installation-problems","text":"If Poetry installation fails, try the alternative installation method: pip install poetry","title":"Poetry Installation Problems"},{"location":"development/contributing/#virtual-environment-issues","text":"If you encounter virtual environment problems: # Remove existing environment poetry env remove python # Reinstall dependencies poetry install","title":"Virtual Environment Issues"},{"location":"development/contributing/#pre-commit-hook-failures","text":"If pre-commit hooks fail during commits: # Run pre-commit manually to see specific issues poetry run pre-commit run --all-files # Fix any reported issues and commit again","title":"Pre-commit Hook Failures"},{"location":"development/contributing/#documentation-build-issues","text":"If mkdocs build fails: # Check for missing dependencies poetry install # Try building with verbose output mkdocs build --verbose # Check configuration mkdocs config","title":"Documentation Build Issues"},{"location":"development/contributing/#test-failures","text":"If tests fail unexpectedly: # Run tests with verbose output poetry run pytest -v # Run specific failing test poetry run pytest path/to/failing_test.py::test_name -v # Check test dependencies poetry run pytest --collect-only","title":"Test Failures"},{"location":"development/contributing/#common-development-issues","text":"","title":"Common Development Issues"},{"location":"development/contributing/#import-errors","text":"# Ensure package is installed in development mode poetry install # Check Python path python -c \"import tfbpmodeling; print(tfbpmodeling.__file__)\"","title":"Import Errors"},{"location":"development/contributing/#module-not-found","text":"# Verify virtual environment is activated which python poetry env info # Reinstall in development mode poetry install --no-deps","title":"Module Not Found"},{"location":"development/contributing/#permission-errors","text":"# Check file permissions ls -la # Fix permissions if needed chmod +x scripts/your_script.sh","title":"Permission Errors"},{"location":"development/contributing/#getting-help-with-development","text":"If you encounter issues not covered here: Search existing issues : Check if someone else has faced the same problem Create a detailed issue : Include error messages, environment info, and steps to reproduce Join discussions : Use GitHub Discussions for questions and help Contact maintainers : Reach out directly for urgent issues","title":"Getting Help with Development"},{"location":"development/contributing/#recognition","text":"Contributors are recognized in: CHANGELOG.md : Feature and bug fix credits AUTHORS.md : Comprehensive contributor list Release Notes : Major contribution highlights Documentation : Author attributions for significant additions Thank you for contributing to tfbpmodeling!","title":"Recognition"},{"location":"development/testing/","text":"Testing Guide \u00b6 This guide covers testing practices, running tests, and writing new tests for tfbpmodeling. Overview \u00b6 tfbpmodeling uses pytest as the testing framework with the following testing practices: Unit tests : Test individual functions and classes Integration tests : Test complete workflows Coverage tracking : Monitor test coverage with codecov Automated testing : CI/CD pipeline runs tests on all PRs Running Tests \u00b6 Basic Test Execution \u00b6 # Run all tests poetry run pytest # Run specific test file poetry run pytest tfbpmodeling/tests/test_interface.py # Run specific test method poetry run pytest tfbpmodeling/tests/test_interface.py::test_linear_perturbation_binding_modeling # Run tests matching pattern poetry run pytest -k \"test_modeling\" Coverage Testing \u00b6 # Run with coverage poetry run pytest --cov --cov-branch --cov-report = xml # Generate HTML coverage report poetry run pytest --cov = tfbpmodeling --cov-report = html # View coverage report open htmlcov/index.html Verbose Testing \u00b6 # Show detailed output poetry run pytest -v # Show print statements poetry run pytest -s # Stop on first failure poetry run pytest -x # Run in parallel (if pytest-xdist installed) poetry run pytest -n auto Test Structure \u00b6 Directory Layout \u00b6 tfbpmodeling/tests/ \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 test_interface.py # Main workflow tests \u251c\u2500\u2500 test_modeling_input_data.py # Data handling tests \u251c\u2500\u2500 test_bootstrapped_input_data.py # Bootstrap tests \u251c\u2500\u2500 test_bootstrap_model_results.py # Results tests \u251c\u2500\u2500 test_evaluation_modules.py # Significance testing \u251c\u2500\u2500 test_utils.py # Utility function tests \u251c\u2500\u2500 fixtures/ # Test data files \u2502 \u251c\u2500\u2500 sample_expression.csv \u2502 \u251c\u2500\u2500 sample_binding.csv \u2502 \u2514\u2500\u2500 sample_blacklist.txt \u2514\u2500\u2500 conftest.py # Shared fixtures Test Configuration \u00b6 Tests are configured in pyproject.toml : [tool.pytest.ini_options] testpaths = [ \"tfbpmodeling/tests\" ] python_files = [ \"test_*.py\" ] python_classes = [ \"Test*\" ] python_functions = [ \"test_*\" ] addopts = [ \"--strict-markers\" , \"--strict-config\" , \"--cov=tfbpmodeling\" , \"--cov-branch\" , \"--cov-report=term-missing\" , ] markers = [ \"slow: marks tests as slow (deselect with '-m \\\" not slow \\\" ')\" , \"integration: marks tests as integration tests\" , ] Writing Tests \u00b6 Test Class Structure \u00b6 import pytest import pandas as pd from tfbpmodeling.modeling_input_data import ModelingInputData class TestModelingInputData : \\ \" \\\"\\\" Test suite for ModelingInputData class. \\\"\\\"\\\" def test_basic_initialization ( self , sample_data_files ): \\ \" \\\"\\\" Test basic object creation. \\\"\\\"\\\" data = ModelingInputData ( response_file = sample_data_files [ 'response' ], predictors_file = sample_data_files [ 'predictors' ], perturbed_tf = 'YPD1' ) assert data is not None assert len ( data . get_feature_names ()) > 0 assert data . perturbed_tf == 'YPD1' def test_file_validation ( self , sample_data_files ): \\ \" \\\"\\\" Test input file validation. \\\"\\\"\\\" # Test missing response file with pytest . raises ( FileNotFoundError ): ModelingInputData ( response_file = 'nonexistent.csv' , predictors_file = sample_data_files [ 'predictors' ], perturbed_tf = 'YPD1' ) @pytest . mark . parametrize ( \"normalize\" , [ True , False ]) def test_normalization_options ( self , sample_data_files , normalize ): \\ \" \\\"\\\" Test different normalization settings. \\\"\\\"\\\" data = ModelingInputData ( response_file = sample_data_files [ 'response' ], predictors_file = sample_data_files [ 'predictors' ], perturbed_tf = 'YPD1' , normalize_weights = normalize ) assert data . normalize_weights == normalize @pytest . mark . slow def test_large_dataset_handling ( self , large_sample_data ): \\ \" \\\"\\\" Test performance with large datasets. \\\"\\\"\\\" # This test is marked as slow and can be skipped data = ModelingInputData ( ** large_sample_data ) assert len ( data . get_feature_names ()) > 1000 Fixtures \u00b6 Create reusable test data with fixtures: # conftest.py import pytest import pandas as pd import numpy as np from pathlib import Path @pytest . fixture def sample_data_files ( tmp_path ): \\ \" \\\"\\\" Create sample CSV files for testing. \\\"\\\"\\\" # Generate sample data np . random . seed ( 42 ) genes = [ f \"gene_ { i } \" for i in range ( 100 )] samples = [ f \"sample_ { i } \" for i in range ( 20 )] tfs = [ f \"TF_ { i } \" for i in range ( 10 )] # Response data response_data = pd . DataFrame ( np . random . normal ( 0 , 1 , ( 100 , 20 )), index = genes , columns = samples ) response_data [ 'YPD1' ] = np . random . normal ( - 0.5 , 0.8 , 100 ) response_data . index . name = 'gene_id' # Predictor data predictor_data = pd . DataFrame ( np . random . beta ( 0.5 , 2 , ( 100 , 10 )), index = genes , columns = tfs ) predictor_data . index . name = 'gene_id' # Save files response_file = tmp_path / \"response.csv\" predictor_file = tmp_path / \"predictors.csv\" response_data . to_csv ( response_file ) predictor_data . to_csv ( predictor_file ) return { 'response' : str ( response_file ), 'predictors' : str ( predictor_file ) } @pytest . fixture def sample_blacklist_file ( tmp_path ): \\ \" \\\"\\\" Create sample blacklist file. \\\"\\\"\\\" blacklist_file = tmp_path / \"blacklist.txt\" blacklist_file . write_text ( \"gene_1 \\\\ ngene_2 \\\\ ngene_3 \\\\ n\" ) return str ( blacklist_file ) @pytest . fixture ( scope = \"session\" ) def large_sample_data (): \\ \" \\\"\\\" Create large dataset for performance testing. \\\"\\\"\\\" # Only create once per test session # Implementation for large test data pass Testing Async and Complex Operations \u00b6 import pytest from unittest.mock import patch , MagicMock class TestBootstrapModeling : \\ \" \\\"\\\" Test bootstrap modeling functionality. \\\"\\\"\\\" def test_bootstrap_sampling ( self , sample_data_files ): \\ \" \\\"\\\" Test bootstrap sample generation. \\\"\\\"\\\" data = BootstrappedModelingInputData ( base_data = ModelingInputData ( ** sample_data_files , perturbed_tf = 'YPD1' ), n_bootstraps = 100 , random_state = 42 ) # Test reproducibility indices1 = data . get_bootstrap_indices () data_copy = BootstrappedModelingInputData ( base_data = ModelingInputData ( ** sample_data_files , perturbed_tf = 'YPD1' ), n_bootstraps = 100 , random_state = 42 ) indices2 = data_copy . get_bootstrap_indices () assert np . array_equal ( indices1 , indices2 ) @patch ( 'tfbpmodeling.interface.LassoCV' ) def test_lasso_cv_integration ( self , mock_lasso , sample_data_files ): \\ \" \\\"\\\" Test LassoCV integration with mocking. \\\"\\\"\\\" # Mock LassoCV behavior mock_estimator = MagicMock () mock_estimator . fit . return_value = mock_estimator mock_estimator . coef_ = np . random . normal ( 0 , 1 , 10 ) mock_lasso . return_value = mock_estimator # Test the integration args = create_test_args ( sample_data_files ) result = linear_perturbation_binding_modeling ( args ) # Verify LassoCV was called mock_lasso . assert_called () mock_estimator . fit . assert_called () def test_error_handling ( self , sample_data_files ): \\ \" \\\"\\\" Test error handling in edge cases. \\\"\\\"\\\" # Test with insufficient data minimal_data = create_minimal_data ( n_features = 5 , n_samples = 3 ) with pytest . raises ( ValueError , match = \"Insufficient data\" ): ModelingInputData ( ** minimal_data , perturbed_tf = 'YPD1' ) Integration Tests \u00b6 End-to-End Workflow Tests \u00b6 class TestCompleteWorkflow : \\ \" \\\"\\\" Test complete analysis workflow. \\\"\\\"\\\" def test_full_pipeline ( self , sample_data_files , tmp_path ): \\ \" \\\"\\\" Test complete analysis from start to finish. \\\"\\\"\\\" args = argparse . Namespace ( response_file = sample_data_files [ 'response' ], predictors_file = sample_data_files [ 'predictors' ], perturbed_tf = 'YPD1' , n_bootstraps = 50 , # Reduced for testing top_n = 30 , all_data_ci_level = 90.0 , topn_ci_level = 80.0 , max_iter = 1000 , output_dir = str ( tmp_path ), output_suffix = '_test' , n_cpus = 1 , # ... other required args ) # Run complete analysis linear_perturbation_binding_modeling ( args ) # Verify output files exist output_dirs = list ( tmp_path . glob ( \"YPD1_test_*\" )) assert len ( output_dirs ) == 1 output_dir = output_dirs [ 0 ] assert ( output_dir / \"all_data_results\" / \"confidence_intervals.csv\" ) . exists () assert ( output_dir / \"topn_results\" / \"confidence_intervals.csv\" ) . exists () assert ( output_dir / \"interactor_significance\" / \"final_selection.csv\" ) . exists () def test_reproducible_results ( self , sample_data_files , tmp_path ): \\ \" \\\"\\\" Test that results are reproducible with fixed seed. \\\"\\\"\\\" args = create_test_args ( sample_data_files , tmp_path , random_state = 42 ) # Run twice with same seed linear_perturbation_binding_modeling ( args ) result1_files = list ( tmp_path . glob ( \"YPD1_*\" )) args . output_suffix = '_run2' linear_perturbation_binding_modeling ( args ) result2_files = list ( tmp_path . glob ( \"YPD1_*run2*\" )) # Compare key results ci1 = pd . read_csv ( result1_files [ 0 ] / \"all_data_results\" / \"confidence_intervals.csv\" ) ci2 = pd . read_csv ( result2_files [ 0 ] / \"all_data_results\" / \"confidence_intervals.csv\" ) pd . testing . assert_frame_equal ( ci1 , ci2 ) Performance Testing \u00b6 Benchmarking \u00b6 import time import pytest class TestPerformance : \\ \" \\\"\\\" Performance benchmarks for key operations. \\\"\\\"\\\" @pytest . mark . slow def test_bootstrap_performance ( self , large_sample_data ): \\ \" \\\"\\\" Benchmark bootstrap modeling performance. \\\"\\\"\\\" start_time = time . time () data = BootstrappedModelingInputData ( base_data = large_sample_data , n_bootstraps = 1000 ) elapsed = time . time () - start_time # Performance assertion (adjust thresholds as needed) assert elapsed < 60 , f \"Bootstrap creation took { elapsed : .2f } s, expected < 60s\" def test_memory_usage ( self , sample_data_files ): \\ \" \\\"\\\" Test memory usage during analysis. \\\"\\\"\\\" import psutil import os process = psutil . Process ( os . getpid ()) initial_memory = process . memory_info () . rss / 1024 / 1024 # MB # Run analysis args = create_test_args ( sample_data_files , n_bootstraps = 1000 ) linear_perturbation_binding_modeling ( args ) peak_memory = process . memory_info () . rss / 1024 / 1024 # MB memory_increase = peak_memory - initial_memory # Memory assertion (adjust threshold as needed) assert memory_increase < 1000 , f \"Memory usage increased by { memory_increase : .2f } MB\" Continuous Integration \u00b6 GitHub Actions \u00b6 Tests run automatically on: - Pull requests to main and dev branches - Pushes to main and dev branches - Scheduled runs (daily) Test Matrix \u00b6 Tests run on multiple environments: - Python versions : 3.11, 3.12 - Operating systems : Ubuntu, macOS, Windows - Dependencies : Latest and pinned versions Coverage Requirements \u00b6 Minimum coverage : 80% Coverage reporting : codecov.io Coverage enforcement : CI fails if coverage drops Debugging Tests \u00b6 Running Specific Tests \u00b6 # Debug specific test with verbose output poetry run pytest -v -s tfbpmodeling/tests/test_interface.py::test_specific_function # Run with debugger poetry run pytest --pdb tfbpmodeling/tests/test_interface.py::test_specific_function # Run last failed tests poetry run pytest --lf Test Data Inspection \u00b6 def test_debug_data_inspection ( sample_data_files ): \\ \" \\\"\\\" Template for debugging test data. \\\"\\\"\\\" response_df = pd . read_csv ( sample_data_files [ 'response' ], index_col = 0 ) predictor_df = pd . read_csv ( sample_data_files [ 'predictors' ], index_col = 0 ) print ( f \"Response shape: { response_df . shape } \" ) print ( f \"Predictor shape: { predictor_df . shape } \" ) print ( f \"Response columns: { response_df . columns . tolist () } \" ) print ( f \"Predictor columns: { predictor_df . columns . tolist () } \" ) # Add your debugging code here assert False # Fail test to see output Best Practices \u00b6 Test Organization \u00b6 One concept per test : Each test should verify one specific behavior Clear test names : Use descriptive names that explain what is being tested Arrange-Act-Assert : Structure tests with clear setup, execution, and verification Independent tests : Tests should not depend on each other Test Data \u00b6 Use fixtures : Create reusable test data with pytest fixtures Minimal data : Use smallest datasets that demonstrate the behavior Reproducible data : Use fixed seeds for random data generation Clean up : Use temporary directories that are automatically cleaned Assertions \u00b6 Specific assertions : Use specific assertion methods ( assert_frame_equal vs assert ) Meaningful messages : Include helpful error messages in assertions Expected exceptions : Test error conditions with pytest.raises Floating point comparisons : Use appropriate tolerance for numeric comparisons Mock and Patch \u00b6 External dependencies : Mock external API calls, file system operations Expensive operations : Mock slow computations during unit tests Isolation : Use mocks to isolate the unit being tested Verification : Assert that mocked methods were called correctly Common Testing Patterns \u00b6 Testing File I/O \u00b6 def test_file_loading ( tmp_path ): \\ \" \\\"\\\" Test file loading with temporary files. \\\"\\\"\\\" # Create test file test_file = tmp_path / \"test.csv\" test_data = pd . DataFrame ({ 'col1' : [ 1 , 2 , 3 ], 'col2' : [ 4 , 5 , 6 ]}) test_data . to_csv ( test_file , index = False ) # Test loading result = load_data_function ( str ( test_file )) # Verify pd . testing . assert_frame_equal ( result , test_data ) Testing Statistical Functions \u00b6 def test_confidence_interval_calculation (): \\ \" \\\"\\\" Test confidence interval calculation. \\\"\\\"\\\" # Known data with expected results data = np . array ([ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ]) ci_lower , ci_upper = calculate_confidence_interval ( data , confidence = 95 ) # Assert approximate equality for floating point assert abs ( ci_lower - 2.5 ) < 0.1 assert abs ( ci_upper - 7.5 ) < 0.1 Testing Error Conditions \u00b6 def test_invalid_input_handling (): \\ \" \\\"\\\" Test that invalid inputs raise appropriate errors. \\\"\\\"\\\" with pytest . raises ( ValueError , match = \"must be positive\" ): some_function ( negative_parameter =- 1 ) with pytest . raises ( FileNotFoundError ): load_data_function ( \"nonexistent_file.csv\" ) This testing guide provides comprehensive coverage of testing practices in tfbpmodeling. Regular testing ensures code quality, prevents regressions, and facilitates confident refactoring.","title":"Testing"},{"location":"development/testing/#testing-guide","text":"This guide covers testing practices, running tests, and writing new tests for tfbpmodeling.","title":"Testing Guide"},{"location":"development/testing/#overview","text":"tfbpmodeling uses pytest as the testing framework with the following testing practices: Unit tests : Test individual functions and classes Integration tests : Test complete workflows Coverage tracking : Monitor test coverage with codecov Automated testing : CI/CD pipeline runs tests on all PRs","title":"Overview"},{"location":"development/testing/#running-tests","text":"","title":"Running Tests"},{"location":"development/testing/#basic-test-execution","text":"# Run all tests poetry run pytest # Run specific test file poetry run pytest tfbpmodeling/tests/test_interface.py # Run specific test method poetry run pytest tfbpmodeling/tests/test_interface.py::test_linear_perturbation_binding_modeling # Run tests matching pattern poetry run pytest -k \"test_modeling\"","title":"Basic Test Execution"},{"location":"development/testing/#coverage-testing","text":"# Run with coverage poetry run pytest --cov --cov-branch --cov-report = xml # Generate HTML coverage report poetry run pytest --cov = tfbpmodeling --cov-report = html # View coverage report open htmlcov/index.html","title":"Coverage Testing"},{"location":"development/testing/#verbose-testing","text":"# Show detailed output poetry run pytest -v # Show print statements poetry run pytest -s # Stop on first failure poetry run pytest -x # Run in parallel (if pytest-xdist installed) poetry run pytest -n auto","title":"Verbose Testing"},{"location":"development/testing/#test-structure","text":"","title":"Test Structure"},{"location":"development/testing/#directory-layout","text":"tfbpmodeling/tests/ \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 test_interface.py # Main workflow tests \u251c\u2500\u2500 test_modeling_input_data.py # Data handling tests \u251c\u2500\u2500 test_bootstrapped_input_data.py # Bootstrap tests \u251c\u2500\u2500 test_bootstrap_model_results.py # Results tests \u251c\u2500\u2500 test_evaluation_modules.py # Significance testing \u251c\u2500\u2500 test_utils.py # Utility function tests \u251c\u2500\u2500 fixtures/ # Test data files \u2502 \u251c\u2500\u2500 sample_expression.csv \u2502 \u251c\u2500\u2500 sample_binding.csv \u2502 \u2514\u2500\u2500 sample_blacklist.txt \u2514\u2500\u2500 conftest.py # Shared fixtures","title":"Directory Layout"},{"location":"development/testing/#test-configuration","text":"Tests are configured in pyproject.toml : [tool.pytest.ini_options] testpaths = [ \"tfbpmodeling/tests\" ] python_files = [ \"test_*.py\" ] python_classes = [ \"Test*\" ] python_functions = [ \"test_*\" ] addopts = [ \"--strict-markers\" , \"--strict-config\" , \"--cov=tfbpmodeling\" , \"--cov-branch\" , \"--cov-report=term-missing\" , ] markers = [ \"slow: marks tests as slow (deselect with '-m \\\" not slow \\\" ')\" , \"integration: marks tests as integration tests\" , ]","title":"Test Configuration"},{"location":"development/testing/#writing-tests","text":"","title":"Writing Tests"},{"location":"development/testing/#test-class-structure","text":"import pytest import pandas as pd from tfbpmodeling.modeling_input_data import ModelingInputData class TestModelingInputData : \\ \" \\\"\\\" Test suite for ModelingInputData class. \\\"\\\"\\\" def test_basic_initialization ( self , sample_data_files ): \\ \" \\\"\\\" Test basic object creation. \\\"\\\"\\\" data = ModelingInputData ( response_file = sample_data_files [ 'response' ], predictors_file = sample_data_files [ 'predictors' ], perturbed_tf = 'YPD1' ) assert data is not None assert len ( data . get_feature_names ()) > 0 assert data . perturbed_tf == 'YPD1' def test_file_validation ( self , sample_data_files ): \\ \" \\\"\\\" Test input file validation. \\\"\\\"\\\" # Test missing response file with pytest . raises ( FileNotFoundError ): ModelingInputData ( response_file = 'nonexistent.csv' , predictors_file = sample_data_files [ 'predictors' ], perturbed_tf = 'YPD1' ) @pytest . mark . parametrize ( \"normalize\" , [ True , False ]) def test_normalization_options ( self , sample_data_files , normalize ): \\ \" \\\"\\\" Test different normalization settings. \\\"\\\"\\\" data = ModelingInputData ( response_file = sample_data_files [ 'response' ], predictors_file = sample_data_files [ 'predictors' ], perturbed_tf = 'YPD1' , normalize_weights = normalize ) assert data . normalize_weights == normalize @pytest . mark . slow def test_large_dataset_handling ( self , large_sample_data ): \\ \" \\\"\\\" Test performance with large datasets. \\\"\\\"\\\" # This test is marked as slow and can be skipped data = ModelingInputData ( ** large_sample_data ) assert len ( data . get_feature_names ()) > 1000","title":"Test Class Structure"},{"location":"development/testing/#fixtures","text":"Create reusable test data with fixtures: # conftest.py import pytest import pandas as pd import numpy as np from pathlib import Path @pytest . fixture def sample_data_files ( tmp_path ): \\ \" \\\"\\\" Create sample CSV files for testing. \\\"\\\"\\\" # Generate sample data np . random . seed ( 42 ) genes = [ f \"gene_ { i } \" for i in range ( 100 )] samples = [ f \"sample_ { i } \" for i in range ( 20 )] tfs = [ f \"TF_ { i } \" for i in range ( 10 )] # Response data response_data = pd . DataFrame ( np . random . normal ( 0 , 1 , ( 100 , 20 )), index = genes , columns = samples ) response_data [ 'YPD1' ] = np . random . normal ( - 0.5 , 0.8 , 100 ) response_data . index . name = 'gene_id' # Predictor data predictor_data = pd . DataFrame ( np . random . beta ( 0.5 , 2 , ( 100 , 10 )), index = genes , columns = tfs ) predictor_data . index . name = 'gene_id' # Save files response_file = tmp_path / \"response.csv\" predictor_file = tmp_path / \"predictors.csv\" response_data . to_csv ( response_file ) predictor_data . to_csv ( predictor_file ) return { 'response' : str ( response_file ), 'predictors' : str ( predictor_file ) } @pytest . fixture def sample_blacklist_file ( tmp_path ): \\ \" \\\"\\\" Create sample blacklist file. \\\"\\\"\\\" blacklist_file = tmp_path / \"blacklist.txt\" blacklist_file . write_text ( \"gene_1 \\\\ ngene_2 \\\\ ngene_3 \\\\ n\" ) return str ( blacklist_file ) @pytest . fixture ( scope = \"session\" ) def large_sample_data (): \\ \" \\\"\\\" Create large dataset for performance testing. \\\"\\\"\\\" # Only create once per test session # Implementation for large test data pass","title":"Fixtures"},{"location":"development/testing/#testing-async-and-complex-operations","text":"import pytest from unittest.mock import patch , MagicMock class TestBootstrapModeling : \\ \" \\\"\\\" Test bootstrap modeling functionality. \\\"\\\"\\\" def test_bootstrap_sampling ( self , sample_data_files ): \\ \" \\\"\\\" Test bootstrap sample generation. \\\"\\\"\\\" data = BootstrappedModelingInputData ( base_data = ModelingInputData ( ** sample_data_files , perturbed_tf = 'YPD1' ), n_bootstraps = 100 , random_state = 42 ) # Test reproducibility indices1 = data . get_bootstrap_indices () data_copy = BootstrappedModelingInputData ( base_data = ModelingInputData ( ** sample_data_files , perturbed_tf = 'YPD1' ), n_bootstraps = 100 , random_state = 42 ) indices2 = data_copy . get_bootstrap_indices () assert np . array_equal ( indices1 , indices2 ) @patch ( 'tfbpmodeling.interface.LassoCV' ) def test_lasso_cv_integration ( self , mock_lasso , sample_data_files ): \\ \" \\\"\\\" Test LassoCV integration with mocking. \\\"\\\"\\\" # Mock LassoCV behavior mock_estimator = MagicMock () mock_estimator . fit . return_value = mock_estimator mock_estimator . coef_ = np . random . normal ( 0 , 1 , 10 ) mock_lasso . return_value = mock_estimator # Test the integration args = create_test_args ( sample_data_files ) result = linear_perturbation_binding_modeling ( args ) # Verify LassoCV was called mock_lasso . assert_called () mock_estimator . fit . assert_called () def test_error_handling ( self , sample_data_files ): \\ \" \\\"\\\" Test error handling in edge cases. \\\"\\\"\\\" # Test with insufficient data minimal_data = create_minimal_data ( n_features = 5 , n_samples = 3 ) with pytest . raises ( ValueError , match = \"Insufficient data\" ): ModelingInputData ( ** minimal_data , perturbed_tf = 'YPD1' )","title":"Testing Async and Complex Operations"},{"location":"development/testing/#integration-tests","text":"","title":"Integration Tests"},{"location":"development/testing/#end-to-end-workflow-tests","text":"class TestCompleteWorkflow : \\ \" \\\"\\\" Test complete analysis workflow. \\\"\\\"\\\" def test_full_pipeline ( self , sample_data_files , tmp_path ): \\ \" \\\"\\\" Test complete analysis from start to finish. \\\"\\\"\\\" args = argparse . Namespace ( response_file = sample_data_files [ 'response' ], predictors_file = sample_data_files [ 'predictors' ], perturbed_tf = 'YPD1' , n_bootstraps = 50 , # Reduced for testing top_n = 30 , all_data_ci_level = 90.0 , topn_ci_level = 80.0 , max_iter = 1000 , output_dir = str ( tmp_path ), output_suffix = '_test' , n_cpus = 1 , # ... other required args ) # Run complete analysis linear_perturbation_binding_modeling ( args ) # Verify output files exist output_dirs = list ( tmp_path . glob ( \"YPD1_test_*\" )) assert len ( output_dirs ) == 1 output_dir = output_dirs [ 0 ] assert ( output_dir / \"all_data_results\" / \"confidence_intervals.csv\" ) . exists () assert ( output_dir / \"topn_results\" / \"confidence_intervals.csv\" ) . exists () assert ( output_dir / \"interactor_significance\" / \"final_selection.csv\" ) . exists () def test_reproducible_results ( self , sample_data_files , tmp_path ): \\ \" \\\"\\\" Test that results are reproducible with fixed seed. \\\"\\\"\\\" args = create_test_args ( sample_data_files , tmp_path , random_state = 42 ) # Run twice with same seed linear_perturbation_binding_modeling ( args ) result1_files = list ( tmp_path . glob ( \"YPD1_*\" )) args . output_suffix = '_run2' linear_perturbation_binding_modeling ( args ) result2_files = list ( tmp_path . glob ( \"YPD1_*run2*\" )) # Compare key results ci1 = pd . read_csv ( result1_files [ 0 ] / \"all_data_results\" / \"confidence_intervals.csv\" ) ci2 = pd . read_csv ( result2_files [ 0 ] / \"all_data_results\" / \"confidence_intervals.csv\" ) pd . testing . assert_frame_equal ( ci1 , ci2 )","title":"End-to-End Workflow Tests"},{"location":"development/testing/#performance-testing","text":"","title":"Performance Testing"},{"location":"development/testing/#benchmarking","text":"import time import pytest class TestPerformance : \\ \" \\\"\\\" Performance benchmarks for key operations. \\\"\\\"\\\" @pytest . mark . slow def test_bootstrap_performance ( self , large_sample_data ): \\ \" \\\"\\\" Benchmark bootstrap modeling performance. \\\"\\\"\\\" start_time = time . time () data = BootstrappedModelingInputData ( base_data = large_sample_data , n_bootstraps = 1000 ) elapsed = time . time () - start_time # Performance assertion (adjust thresholds as needed) assert elapsed < 60 , f \"Bootstrap creation took { elapsed : .2f } s, expected < 60s\" def test_memory_usage ( self , sample_data_files ): \\ \" \\\"\\\" Test memory usage during analysis. \\\"\\\"\\\" import psutil import os process = psutil . Process ( os . getpid ()) initial_memory = process . memory_info () . rss / 1024 / 1024 # MB # Run analysis args = create_test_args ( sample_data_files , n_bootstraps = 1000 ) linear_perturbation_binding_modeling ( args ) peak_memory = process . memory_info () . rss / 1024 / 1024 # MB memory_increase = peak_memory - initial_memory # Memory assertion (adjust threshold as needed) assert memory_increase < 1000 , f \"Memory usage increased by { memory_increase : .2f } MB\"","title":"Benchmarking"},{"location":"development/testing/#continuous-integration","text":"","title":"Continuous Integration"},{"location":"development/testing/#github-actions","text":"Tests run automatically on: - Pull requests to main and dev branches - Pushes to main and dev branches - Scheduled runs (daily)","title":"GitHub Actions"},{"location":"development/testing/#test-matrix","text":"Tests run on multiple environments: - Python versions : 3.11, 3.12 - Operating systems : Ubuntu, macOS, Windows - Dependencies : Latest and pinned versions","title":"Test Matrix"},{"location":"development/testing/#coverage-requirements","text":"Minimum coverage : 80% Coverage reporting : codecov.io Coverage enforcement : CI fails if coverage drops","title":"Coverage Requirements"},{"location":"development/testing/#debugging-tests","text":"","title":"Debugging Tests"},{"location":"development/testing/#running-specific-tests","text":"# Debug specific test with verbose output poetry run pytest -v -s tfbpmodeling/tests/test_interface.py::test_specific_function # Run with debugger poetry run pytest --pdb tfbpmodeling/tests/test_interface.py::test_specific_function # Run last failed tests poetry run pytest --lf","title":"Running Specific Tests"},{"location":"development/testing/#test-data-inspection","text":"def test_debug_data_inspection ( sample_data_files ): \\ \" \\\"\\\" Template for debugging test data. \\\"\\\"\\\" response_df = pd . read_csv ( sample_data_files [ 'response' ], index_col = 0 ) predictor_df = pd . read_csv ( sample_data_files [ 'predictors' ], index_col = 0 ) print ( f \"Response shape: { response_df . shape } \" ) print ( f \"Predictor shape: { predictor_df . shape } \" ) print ( f \"Response columns: { response_df . columns . tolist () } \" ) print ( f \"Predictor columns: { predictor_df . columns . tolist () } \" ) # Add your debugging code here assert False # Fail test to see output","title":"Test Data Inspection"},{"location":"development/testing/#best-practices","text":"","title":"Best Practices"},{"location":"development/testing/#test-organization","text":"One concept per test : Each test should verify one specific behavior Clear test names : Use descriptive names that explain what is being tested Arrange-Act-Assert : Structure tests with clear setup, execution, and verification Independent tests : Tests should not depend on each other","title":"Test Organization"},{"location":"development/testing/#test-data","text":"Use fixtures : Create reusable test data with pytest fixtures Minimal data : Use smallest datasets that demonstrate the behavior Reproducible data : Use fixed seeds for random data generation Clean up : Use temporary directories that are automatically cleaned","title":"Test Data"},{"location":"development/testing/#assertions","text":"Specific assertions : Use specific assertion methods ( assert_frame_equal vs assert ) Meaningful messages : Include helpful error messages in assertions Expected exceptions : Test error conditions with pytest.raises Floating point comparisons : Use appropriate tolerance for numeric comparisons","title":"Assertions"},{"location":"development/testing/#mock-and-patch","text":"External dependencies : Mock external API calls, file system operations Expensive operations : Mock slow computations during unit tests Isolation : Use mocks to isolate the unit being tested Verification : Assert that mocked methods were called correctly","title":"Mock and Patch"},{"location":"development/testing/#common-testing-patterns","text":"","title":"Common Testing Patterns"},{"location":"development/testing/#testing-file-io","text":"def test_file_loading ( tmp_path ): \\ \" \\\"\\\" Test file loading with temporary files. \\\"\\\"\\\" # Create test file test_file = tmp_path / \"test.csv\" test_data = pd . DataFrame ({ 'col1' : [ 1 , 2 , 3 ], 'col2' : [ 4 , 5 , 6 ]}) test_data . to_csv ( test_file , index = False ) # Test loading result = load_data_function ( str ( test_file )) # Verify pd . testing . assert_frame_equal ( result , test_data )","title":"Testing File I/O"},{"location":"development/testing/#testing-statistical-functions","text":"def test_confidence_interval_calculation (): \\ \" \\\"\\\" Test confidence interval calculation. \\\"\\\"\\\" # Known data with expected results data = np . array ([ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ]) ci_lower , ci_upper = calculate_confidence_interval ( data , confidence = 95 ) # Assert approximate equality for floating point assert abs ( ci_lower - 2.5 ) < 0.1 assert abs ( ci_upper - 7.5 ) < 0.1","title":"Testing Statistical Functions"},{"location":"development/testing/#testing-error-conditions","text":"def test_invalid_input_handling (): \\ \" \\\"\\\" Test that invalid inputs raise appropriate errors. \\\"\\\"\\\" with pytest . raises ( ValueError , match = \"must be positive\" ): some_function ( negative_parameter =- 1 ) with pytest . raises ( FileNotFoundError ): load_data_function ( \"nonexistent_file.csv\" ) This testing guide provides comprehensive coverage of testing practices in tfbpmodeling. Regular testing ensures code quality, prevents regressions, and facilitates confident refactoring.","title":"Testing Error Conditions"},{"location":"getting-started/installation/","text":"Installation \u00b6 Requirements \u00b6 Python 3.11 or higher Git for version control Standard Installation \u00b6 tfbpmodeling is available for installation directly from GitHub using pip. PyPI distribution is planned for future releases. Install from GitHub \u00b6 === \"Latest Stable (main branch)\" ```bash python -m pip install git+https://github.com/BrentLab/tfbpmodeling.git ``` === \"Development Version (dev branch)\" ``` bash python - m pip install git + https : // github . com / BrentLab / tfbpmodeling . git @dev ``` === \"Specific Version (by tag)\" ``` bash python - m pip install git + https : // github . com / BrentLab / tfbpmodeling . git @v1 .0.0 ``` Virtual Environment (Recommended) \u00b6 It's recommended to install tfbpmodeling in a virtual environment: # Create virtual environment python -m venv tfbp-env # Activate virtual environment # On Linux/macOS: source tfbp-env/bin/activate # On Windows: tfbp-env \\S cripts \\a ctivate # Install tfbpmodeling python -m pip install git+https://github.com/BrentLab/tfbpmodeling.git # Upgrade existing installation python -m pip install --upgrade git+https://github.com/BrentLab/tfbpmodeling.git Verify Installation \u00b6 Test that the installation was successful: # Show help for the main command python -m tfbpmodeling --help # Show help for the modeling command python -m tfbpmodeling linear_perturbation_binding_modeling --help Development Installation \u00b6 For development work, you'll need to clone the repository and install with Poetry. 1. Clone the Repository \u00b6 === \"Fork for Development\" If you plan to contribute: 1. Fork the repository on GitHub 2. Clone your fork: ```bash git clone https://github.com/YOUR_USERNAME/tfbpmodeling.git cd tfbpmodeling ``` 3. Add the upstream remote: ```bash git remote add upstream https://github.com/BrentLab/tfbpmodeling.git ``` === \"Direct Clone\" ```bash git clone https://github.com/BrentLab/tfbpmodeling.git cd tfbpmodeling ``` 2. Install Poetry \u00b6 If you don't have Poetry installed, follow the official installation instructions . Poetry Configuration We recommend configuring Poetry to create virtual environments in the project directory: poetry config virtualenvs.in-project true This creates the virtual environment as .venv in the project directory. 3. Install Dependencies \u00b6 poetry install This installs all dependencies, including development tools for testing and documentation. 4. Install Pre-commit Hooks \u00b6 For development work, install pre-commit hooks to ensure code quality: poetry run pre-commit install You can run pre-commit manually at any time: poetry run pre-commit run --all-files Development Tools \u00b6 Running Tests \u00b6 # Run all tests poetry run pytest # Run with coverage poetry run pytest --cov --cov-branch --cov-report = xml # Run specific test file poetry run pytest tfbpmodeling/tests/test_interface.py # Run specific test by name poetry run pytest -k \"test_function_name\" Code Quality \u00b6 The project uses several tools to maintain code quality: # Format code with Black poetry run black . # Check code style with Flake8 poetry run flake8 # Type checking with MyPy poetry run mypy tfbpmodeling Documentation \u00b6 Build and serve documentation locally: # Serve documentation with live reload mkdocs serve # Build documentation mkdocs build # Deploy to GitHub Pages (maintainers only) poetry run mkdocs gh-deploy Getting Help \u00b6 GitHub Issues : Report bugs or request features GitHub Discussions : Ask questions or discuss usage Documentation : Browse the complete documentation for detailed usage examples Development Guide : See Contributing for development setup and troubleshooting","title":"Installation"},{"location":"getting-started/installation/#installation","text":"","title":"Installation"},{"location":"getting-started/installation/#requirements","text":"Python 3.11 or higher Git for version control","title":"Requirements"},{"location":"getting-started/installation/#standard-installation","text":"tfbpmodeling is available for installation directly from GitHub using pip. PyPI distribution is planned for future releases.","title":"Standard Installation"},{"location":"getting-started/installation/#install-from-github","text":"=== \"Latest Stable (main branch)\" ```bash python -m pip install git+https://github.com/BrentLab/tfbpmodeling.git ``` === \"Development Version (dev branch)\" ``` bash python - m pip install git + https : // github . com / BrentLab / tfbpmodeling . git @dev ``` === \"Specific Version (by tag)\" ``` bash python - m pip install git + https : // github . com / BrentLab / tfbpmodeling . git @v1 .0.0 ```","title":"Install from GitHub"},{"location":"getting-started/installation/#virtual-environment-recommended","text":"It's recommended to install tfbpmodeling in a virtual environment: # Create virtual environment python -m venv tfbp-env # Activate virtual environment # On Linux/macOS: source tfbp-env/bin/activate # On Windows: tfbp-env \\S cripts \\a ctivate # Install tfbpmodeling python -m pip install git+https://github.com/BrentLab/tfbpmodeling.git # Upgrade existing installation python -m pip install --upgrade git+https://github.com/BrentLab/tfbpmodeling.git","title":"Virtual Environment (Recommended)"},{"location":"getting-started/installation/#verify-installation","text":"Test that the installation was successful: # Show help for the main command python -m tfbpmodeling --help # Show help for the modeling command python -m tfbpmodeling linear_perturbation_binding_modeling --help","title":"Verify Installation"},{"location":"getting-started/installation/#development-installation","text":"For development work, you'll need to clone the repository and install with Poetry.","title":"Development Installation"},{"location":"getting-started/installation/#1-clone-the-repository","text":"=== \"Fork for Development\" If you plan to contribute: 1. Fork the repository on GitHub 2. Clone your fork: ```bash git clone https://github.com/YOUR_USERNAME/tfbpmodeling.git cd tfbpmodeling ``` 3. Add the upstream remote: ```bash git remote add upstream https://github.com/BrentLab/tfbpmodeling.git ``` === \"Direct Clone\" ```bash git clone https://github.com/BrentLab/tfbpmodeling.git cd tfbpmodeling ```","title":"1. Clone the Repository"},{"location":"getting-started/installation/#2-install-poetry","text":"If you don't have Poetry installed, follow the official installation instructions . Poetry Configuration We recommend configuring Poetry to create virtual environments in the project directory: poetry config virtualenvs.in-project true This creates the virtual environment as .venv in the project directory.","title":"2. Install Poetry"},{"location":"getting-started/installation/#3-install-dependencies","text":"poetry install This installs all dependencies, including development tools for testing and documentation.","title":"3. Install Dependencies"},{"location":"getting-started/installation/#4-install-pre-commit-hooks","text":"For development work, install pre-commit hooks to ensure code quality: poetry run pre-commit install You can run pre-commit manually at any time: poetry run pre-commit run --all-files","title":"4. Install Pre-commit Hooks"},{"location":"getting-started/installation/#development-tools","text":"","title":"Development Tools"},{"location":"getting-started/installation/#running-tests","text":"# Run all tests poetry run pytest # Run with coverage poetry run pytest --cov --cov-branch --cov-report = xml # Run specific test file poetry run pytest tfbpmodeling/tests/test_interface.py # Run specific test by name poetry run pytest -k \"test_function_name\"","title":"Running Tests"},{"location":"getting-started/installation/#code-quality","text":"The project uses several tools to maintain code quality: # Format code with Black poetry run black . # Check code style with Flake8 poetry run flake8 # Type checking with MyPy poetry run mypy tfbpmodeling","title":"Code Quality"},{"location":"getting-started/installation/#documentation","text":"Build and serve documentation locally: # Serve documentation with live reload mkdocs serve # Build documentation mkdocs build # Deploy to GitHub Pages (maintainers only) poetry run mkdocs gh-deploy","title":"Documentation"},{"location":"getting-started/installation/#getting-help","text":"GitHub Issues : Report bugs or request features GitHub Discussions : Ask questions or discuss usage Documentation : Browse the complete documentation for detailed usage examples Development Guide : See Contributing for development setup and troubleshooting","title":"Getting Help"},{"location":"getting-started/quickstart/","text":"Quick Start Guide \u00b6 This guide will walk you through your first analysis with tfbpmodeling using example data. Overview \u00b6 tfbpmodeling analyzes the relationship between transcription factor binding data and gene expression perturbation data through a 4-stage workflow: Bootstrap modeling on all data to identify significant binding-expression relationships Top-N modeling on the most significant predictors from high-performing data Interaction analysis to evaluate interaction terms vs main effects Results generation with comprehensive statistics and confidence intervals Prepare Your Data \u00b6 tfbpmodeling requires two main input files: Response File (Gene Expression Data) \u00b6 CSV format with genes as rows and samples as columns: gene_id,sample1,sample2,sample3,sample4 YPD1,0.23,-1.45,0.87,-0.12 YBR123W,1.34,0.56,-0.23,0.78 YCR456X,-0.45,0.12,1.23,-0.56 First column: Gene identifiers Subsequent columns: Expression values for each sample Must contain a column matching your --perturbed_tf parameter Predictors File (Binding Data) \u00b6 CSV format with genes as rows and transcription factors as columns: gene_id,TF1,TF2,TF3,TF4 YPD1,0.34,0.12,0.78,0.01 YBR123W,0.89,0.45,0.23,0.67 YCR456X,0.12,0.78,0.34,0.90 First column: Gene identifiers (must match response file) Subsequent columns: Binding measurements for different TFs Basic Analysis \u00b6 Minimal Command \u00b6 Run a basic analysis with default parameters: python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data/expression.csv \\ --predictors_file data/binding.csv \\ --perturbed_tf YPD1 This command will: Use 1000 bootstrap samples Apply 98% confidence interval for initial feature selection Use 90% confidence interval for second-round modeling Process top 600 features in the second round Save results to ./linear_perturbation_binding_modeling_results/YPD1/ With Custom Parameters \u00b6 python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data/expression.csv \\ --predictors_file data/binding.csv \\ --perturbed_tf YPD1 \\ --n_bootstraps 2000 \\ --top_n 500 \\ --all_data_ci_level 95 .0 \\ --topn_ci_level 85 .0 \\ --output_dir ./my_results \\ --output_suffix _custom_analysis \\ --random_state 42 Understanding the Output \u00b6 Results are saved in a timestamped subdirectory within your specified output directory: my_results/YPD1_custom_analysis_20240115_143022/ \u251c\u2500\u2500 all_data_results/ \u2502 \u251c\u2500\u2500 bootstrap_coefficients.csv \u2502 \u251c\u2500\u2500 confidence_intervals.csv \u2502 \u251c\u2500\u2500 model_statistics.csv \u2502 \u2514\u2500\u2500 diagnostic_plots/ \u251c\u2500\u2500 topn_results/ \u2502 \u251c\u2500\u2500 bootstrap_coefficients.csv \u2502 \u251c\u2500\u2500 confidence_intervals.csv \u2502 \u251c\u2500\u2500 model_statistics.csv \u2502 \u2514\u2500\u2500 diagnostic_plots/ \u251c\u2500\u2500 interactor_significance/ \u2502 \u251c\u2500\u2500 significance_results.csv \u2502 \u251c\u2500\u2500 comparison_statistics.csv \u2502 \u2514\u2500\u2500 final_selection.csv \u2514\u2500\u2500 tfbpmodeling_20240115_143022.log Key Output Files \u00b6 Bootstrap Coefficients \u00b6 Contains coefficient estimates from each bootstrap sample: - Rows: Bootstrap samples - Columns: Model features - Values: Coefficient estimates Confidence Intervals \u00b6 Statistical significance of each feature: - feature : Feature name - mean_coef : Mean coefficient across bootstrap samples - ci_lower : Lower confidence interval bound - ci_upper : Upper confidence interval bound - significant : Boolean indicating statistical significance Model Statistics \u00b6 Overall model performance metrics: - R\u00b2 scores across bootstrap samples - Cross-validation performance - Feature selection statistics Advanced Features \u00b6 Feature Engineering \u00b6 Add polynomial terms and custom variables: python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data/expression.csv \\ --predictors_file data/binding.csv \\ --perturbed_tf YPD1 \\ --squared_pTF \\ --cubic_pTF \\ --row_max \\ --ptf_main_effect \\ --add_model_variables \"red_median,green_median\" Data Processing Options \u00b6 Control data preprocessing: python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data/expression.csv \\ --predictors_file data/binding.csv \\ --perturbed_tf YPD1 \\ --normalize_sample_weights \\ --scale_by_std \\ --bins \"0,5,10,15,np.inf\" Excluding Features \u00b6 Exclude specific genes or features: # Create blacklist file echo -e \"YBR999W\\nYCR888X\\ncontrol_gene\" > blacklist.txt # Run analysis with exclusions python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data/expression.csv \\ --predictors_file data/binding.csv \\ --perturbed_tf YPD1 \\ --blacklist_file blacklist.txt \\ --exclude_interactor_variables \"batch_effect,technical_replicate\" Example Workflow \u00b6 Here's a complete example workflow: 1. Prepare Data \u00b6 # Create example data directory mkdir -p example_data # Your data preparation steps here # (load and format your actual expression and binding data) 2. Run Basic Analysis \u00b6 python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file example_data/expression.csv \\ --predictors_file example_data/binding.csv \\ --perturbed_tf YPD1 \\ --random_state 12345 \\ --output_dir ./results \\ --output_suffix _basic_analysis 3. Run Advanced Analysis \u00b6 python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file example_data/expression.csv \\ --predictors_file example_data/binding.csv \\ --perturbed_tf YPD1 \\ --n_bootstraps 2000 \\ --squared_pTF \\ --ptf_main_effect \\ --iterative_dropout \\ --stage4_lasso \\ --random_state 12345 \\ --output_dir ./results \\ --output_suffix _advanced_analysis 4. Compare Results \u00b6 # Compare the two analyses ls -la results/YPD1_*_analysis_*/ Next Steps \u00b6 CLI Reference : Complete documentation of all command-line options Tutorials : Detailed tutorials with real examples API Reference : Documentation for programmatic usage Input Formats : Detailed specifications for input data Troubleshooting \u00b6 Common Issues \u00b6 File Not Found \u00b6 # Verify your files exist and paths are correct ls -la data/expression.csv data/binding.csv Memory Issues \u00b6 # Reduce bootstrap samples or top_n for large datasets --n_bootstraps 500 --top_n 300 Convergence Issues \u00b6 # Increase iteration limit --max_iter 20000 No Significant Features \u00b6 # Lower confidence intervals or check data quality --all_data_ci_level 90 .0 --topn_ci_level 80 .0 For more help, see the troubleshooting section or open an issue on GitHub.","title":"Quick Start"},{"location":"getting-started/quickstart/#quick-start-guide","text":"This guide will walk you through your first analysis with tfbpmodeling using example data.","title":"Quick Start Guide"},{"location":"getting-started/quickstart/#overview","text":"tfbpmodeling analyzes the relationship between transcription factor binding data and gene expression perturbation data through a 4-stage workflow: Bootstrap modeling on all data to identify significant binding-expression relationships Top-N modeling on the most significant predictors from high-performing data Interaction analysis to evaluate interaction terms vs main effects Results generation with comprehensive statistics and confidence intervals","title":"Overview"},{"location":"getting-started/quickstart/#prepare-your-data","text":"tfbpmodeling requires two main input files:","title":"Prepare Your Data"},{"location":"getting-started/quickstart/#response-file-gene-expression-data","text":"CSV format with genes as rows and samples as columns: gene_id,sample1,sample2,sample3,sample4 YPD1,0.23,-1.45,0.87,-0.12 YBR123W,1.34,0.56,-0.23,0.78 YCR456X,-0.45,0.12,1.23,-0.56 First column: Gene identifiers Subsequent columns: Expression values for each sample Must contain a column matching your --perturbed_tf parameter","title":"Response File (Gene Expression Data)"},{"location":"getting-started/quickstart/#predictors-file-binding-data","text":"CSV format with genes as rows and transcription factors as columns: gene_id,TF1,TF2,TF3,TF4 YPD1,0.34,0.12,0.78,0.01 YBR123W,0.89,0.45,0.23,0.67 YCR456X,0.12,0.78,0.34,0.90 First column: Gene identifiers (must match response file) Subsequent columns: Binding measurements for different TFs","title":"Predictors File (Binding Data)"},{"location":"getting-started/quickstart/#basic-analysis","text":"","title":"Basic Analysis"},{"location":"getting-started/quickstart/#minimal-command","text":"Run a basic analysis with default parameters: python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data/expression.csv \\ --predictors_file data/binding.csv \\ --perturbed_tf YPD1 This command will: Use 1000 bootstrap samples Apply 98% confidence interval for initial feature selection Use 90% confidence interval for second-round modeling Process top 600 features in the second round Save results to ./linear_perturbation_binding_modeling_results/YPD1/","title":"Minimal Command"},{"location":"getting-started/quickstart/#with-custom-parameters","text":"python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data/expression.csv \\ --predictors_file data/binding.csv \\ --perturbed_tf YPD1 \\ --n_bootstraps 2000 \\ --top_n 500 \\ --all_data_ci_level 95 .0 \\ --topn_ci_level 85 .0 \\ --output_dir ./my_results \\ --output_suffix _custom_analysis \\ --random_state 42","title":"With Custom Parameters"},{"location":"getting-started/quickstart/#understanding-the-output","text":"Results are saved in a timestamped subdirectory within your specified output directory: my_results/YPD1_custom_analysis_20240115_143022/ \u251c\u2500\u2500 all_data_results/ \u2502 \u251c\u2500\u2500 bootstrap_coefficients.csv \u2502 \u251c\u2500\u2500 confidence_intervals.csv \u2502 \u251c\u2500\u2500 model_statistics.csv \u2502 \u2514\u2500\u2500 diagnostic_plots/ \u251c\u2500\u2500 topn_results/ \u2502 \u251c\u2500\u2500 bootstrap_coefficients.csv \u2502 \u251c\u2500\u2500 confidence_intervals.csv \u2502 \u251c\u2500\u2500 model_statistics.csv \u2502 \u2514\u2500\u2500 diagnostic_plots/ \u251c\u2500\u2500 interactor_significance/ \u2502 \u251c\u2500\u2500 significance_results.csv \u2502 \u251c\u2500\u2500 comparison_statistics.csv \u2502 \u2514\u2500\u2500 final_selection.csv \u2514\u2500\u2500 tfbpmodeling_20240115_143022.log","title":"Understanding the Output"},{"location":"getting-started/quickstart/#key-output-files","text":"","title":"Key Output Files"},{"location":"getting-started/quickstart/#bootstrap-coefficients","text":"Contains coefficient estimates from each bootstrap sample: - Rows: Bootstrap samples - Columns: Model features - Values: Coefficient estimates","title":"Bootstrap Coefficients"},{"location":"getting-started/quickstart/#confidence-intervals","text":"Statistical significance of each feature: - feature : Feature name - mean_coef : Mean coefficient across bootstrap samples - ci_lower : Lower confidence interval bound - ci_upper : Upper confidence interval bound - significant : Boolean indicating statistical significance","title":"Confidence Intervals"},{"location":"getting-started/quickstart/#model-statistics","text":"Overall model performance metrics: - R\u00b2 scores across bootstrap samples - Cross-validation performance - Feature selection statistics","title":"Model Statistics"},{"location":"getting-started/quickstart/#advanced-features","text":"","title":"Advanced Features"},{"location":"getting-started/quickstart/#feature-engineering","text":"Add polynomial terms and custom variables: python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data/expression.csv \\ --predictors_file data/binding.csv \\ --perturbed_tf YPD1 \\ --squared_pTF \\ --cubic_pTF \\ --row_max \\ --ptf_main_effect \\ --add_model_variables \"red_median,green_median\"","title":"Feature Engineering"},{"location":"getting-started/quickstart/#data-processing-options","text":"Control data preprocessing: python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data/expression.csv \\ --predictors_file data/binding.csv \\ --perturbed_tf YPD1 \\ --normalize_sample_weights \\ --scale_by_std \\ --bins \"0,5,10,15,np.inf\"","title":"Data Processing Options"},{"location":"getting-started/quickstart/#excluding-features","text":"Exclude specific genes or features: # Create blacklist file echo -e \"YBR999W\\nYCR888X\\ncontrol_gene\" > blacklist.txt # Run analysis with exclusions python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data/expression.csv \\ --predictors_file data/binding.csv \\ --perturbed_tf YPD1 \\ --blacklist_file blacklist.txt \\ --exclude_interactor_variables \"batch_effect,technical_replicate\"","title":"Excluding Features"},{"location":"getting-started/quickstart/#example-workflow","text":"Here's a complete example workflow:","title":"Example Workflow"},{"location":"getting-started/quickstart/#1-prepare-data","text":"# Create example data directory mkdir -p example_data # Your data preparation steps here # (load and format your actual expression and binding data)","title":"1. Prepare Data"},{"location":"getting-started/quickstart/#2-run-basic-analysis","text":"python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file example_data/expression.csv \\ --predictors_file example_data/binding.csv \\ --perturbed_tf YPD1 \\ --random_state 12345 \\ --output_dir ./results \\ --output_suffix _basic_analysis","title":"2. Run Basic Analysis"},{"location":"getting-started/quickstart/#3-run-advanced-analysis","text":"python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file example_data/expression.csv \\ --predictors_file example_data/binding.csv \\ --perturbed_tf YPD1 \\ --n_bootstraps 2000 \\ --squared_pTF \\ --ptf_main_effect \\ --iterative_dropout \\ --stage4_lasso \\ --random_state 12345 \\ --output_dir ./results \\ --output_suffix _advanced_analysis","title":"3. Run Advanced Analysis"},{"location":"getting-started/quickstart/#4-compare-results","text":"# Compare the two analyses ls -la results/YPD1_*_analysis_*/","title":"4. Compare Results"},{"location":"getting-started/quickstart/#next-steps","text":"CLI Reference : Complete documentation of all command-line options Tutorials : Detailed tutorials with real examples API Reference : Documentation for programmatic usage Input Formats : Detailed specifications for input data","title":"Next Steps"},{"location":"getting-started/quickstart/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"getting-started/quickstart/#common-issues","text":"","title":"Common Issues"},{"location":"getting-started/quickstart/#file-not-found","text":"# Verify your files exist and paths are correct ls -la data/expression.csv data/binding.csv","title":"File Not Found"},{"location":"getting-started/quickstart/#memory-issues","text":"# Reduce bootstrap samples or top_n for large datasets --n_bootstraps 500 --top_n 300","title":"Memory Issues"},{"location":"getting-started/quickstart/#convergence-issues","text":"# Increase iteration limit --max_iter 20000","title":"Convergence Issues"},{"location":"getting-started/quickstart/#no-significant-features","text":"# Lower confidence intervals or check data quality --all_data_ci_level 90 .0 --topn_ci_level 80 .0 For more help, see the troubleshooting section or open an issue on GitHub.","title":"No Significant Features"},{"location":"tutorials/advanced-features/","text":"Advanced Features Tutorial \u00b6 This tutorial covers advanced features and configuration options in tfbpmodeling for power users and specialized analyses. Overview \u00b6 Beyond the basic workflow, tfbpmodeling offers advanced features for: Feature Engineering : Polynomial terms and custom variables Model Tuning : Advanced parameter optimization Statistical Methods : Alternative significance testing approaches Performance Optimization : High-performance computing configurations Custom Workflows : Programmatic usage and customization Feature Engineering \u00b6 Polynomial Terms \u00b6 Add non-linear relationships to capture complex binding-expression dynamics: # Add squared and cubic terms for the perturbed TF python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data/expression.csv \\ --predictors_file data/binding.csv \\ --perturbed_tf YPD1 \\ --squared_pTF \\ --cubic_pTF \\ --ptf_main_effect When to use polynomial terms : - Non-linear dose-response relationships - Saturation effects in binding - Threshold behaviors in gene expression Custom Variables \u00b6 Include additional experimental variables in the model: # Add batch effects and technical variables python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data/expression.csv \\ --predictors_file data/binding.csv \\ --perturbed_tf YPD1 \\ --add_model_variables \"batch_id,plate_position,extraction_date\" \\ --exclude_interactor_variables \"batch_id,technical_replicate\" Row-wise Features \u00b6 Include summary statistics as predictors: # Add maximum binding strength across all TFs python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data/expression.csv \\ --predictors_file data/binding.csv \\ --perturbed_tf YPD1 \\ --row_max \\ --normalize_sample_weights Advanced Model Configuration \u00b6 Iterative Dropout \u00b6 Use iterative variable selection for feature-rich datasets: python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data/expression.csv \\ --predictors_file data/binding.csv \\ --perturbed_tf YPD1 \\ --iterative_dropout \\ --stabilization_ci_start 50 .0 \\ --all_data_ci_level 95 .0 How iterative dropout works : 1. Start with low confidence threshold (50%) 2. Remove non-significant features 3. Gradually increase threshold 4. Stabilize at final confidence level Stage 4 Configuration \u00b6 Choose between linear and LassoCV approaches for final significance testing: # Conservative LassoCV approach python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data/expression.csv \\ --predictors_file data/binding.csv \\ --perturbed_tf YPD1 \\ --stage4_lasso \\ --stage4_topn # Sensitive linear regression approach (default) python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data/expression.csv \\ --predictors_file data/binding.csv \\ --perturbed_tf YPD1 Data Preprocessing \u00b6 Advanced data handling options: python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data/expression.csv \\ --predictors_file data/binding.csv \\ --perturbed_tf YPD1 \\ --scale_by_std \\ --normalize_sample_weights \\ --bins \"0,5,10,15,20,np.inf\" High-Performance Computing \u00b6 Parallel Processing \u00b6 Optimize for multi-core systems: # Use all available cores python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data/expression.csv \\ --predictors_file data/binding.csv \\ --perturbed_tf YPD1 \\ --n_cpus 16 \\ --n_bootstraps 5000 \\ --max_iter 20000 Memory Management \u00b6 For large datasets: # Reduce memory usage python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data/expression.csv \\ --predictors_file data/binding.csv \\ --perturbed_tf YPD1 \\ --n_bootstraps 500 \\ --top_n 300 \\ --max_iter 5000 Cluster Computing \u00b6 Example SLURM script for HPC environments: #!/bin/bash #SBATCH --job-name=tfbp_analysis #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=32 #SBATCH --mem=64GB #SBATCH --time=24:00:00 module load python/3.11 source venv/bin/activate python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file $SCRATCH /data/expression.csv \\ --predictors_file $SCRATCH /data/binding.csv \\ --perturbed_tf $1 \\ --n_cpus 32 \\ --n_bootstraps 10000 \\ --output_dir $SCRATCH /results \\ --log-handler file Programmatic Usage \u00b6 Python API \u00b6 Use tfbpmodeling programmatically: import argparse from tfbpmodeling.interface import linear_perturbation_binding_modeling # Create arguments programmatically args = argparse . Namespace ( response_file = 'data/expression.csv' , predictors_file = 'data/binding.csv' , perturbed_tf = 'YPD1' , n_bootstraps = 2000 , top_n = 500 , all_data_ci_level = 95.0 , topn_ci_level = 85.0 , max_iter = 15000 , iterative_dropout = True , stage4_lasso = True , squared_pTF = True , ptf_main_effect = True , output_dir = './results' , output_suffix = '_programmatic' , n_cpus = 8 , random_state = 42 , # Set all other required parameters... ) # Run analysis linear_perturbation_binding_modeling ( args ) Batch Processing \u00b6 Process multiple transcription factors: import os from pathlib import Path tfs_to_analyze = [ 'YPD1' , 'YBR123W' , 'YCR456X' ] base_args = { 'response_file' : 'data/expression.csv' , 'predictors_file' : 'data/binding.csv' , 'n_bootstraps' : 2000 , 'random_state' : 42 , # ... other common parameters } for tf in tfs_to_analyze : print ( f \"Analyzing { tf } ...\" ) args = argparse . Namespace ( perturbed_tf = tf , output_suffix = f '_batch_ { tf } ' , ** base_args ) try : linear_perturbation_binding_modeling ( args ) print ( f \"\u2713 { tf } completed successfully\" ) except Exception as e : print ( f \"\u2717 { tf } failed: { e } \" ) Advanced Analysis Patterns \u00b6 Comparative Analysis \u00b6 Compare different parameter settings: # Conservative analysis python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data/expression.csv \\ --predictors_file data/binding.csv \\ --perturbed_tf YPD1 \\ --all_data_ci_level 99 .0 \\ --topn_ci_level 95 .0 \\ --stage4_lasso \\ --output_suffix _conservative \\ --random_state 42 # Sensitive analysis python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data/expression.csv \\ --predictors_file data/binding.csv \\ --perturbed_tf YPD1 \\ --all_data_ci_level 90 .0 \\ --topn_ci_level 80 .0 \\ --output_suffix _sensitive \\ --random_state 42 Cross-Validation \u00b6 Validate results across different data subsets: from sklearn.model_selection import KFold import pandas as pd # Load data response_df = pd . read_csv ( 'data/expression.csv' , index_col = 0 ) predictor_df = pd . read_csv ( 'data/binding.csv' , index_col = 0 ) # Cross-validation across samples kf = KFold ( n_splits = 5 , shuffle = True , random_state = 42 ) for fold , ( train_idx , test_idx ) in enumerate ( kf . split ( response_df . columns [: - 1 ])): # Create train/test splits train_samples = response_df . columns [ train_idx ] test_samples = response_df . columns [ test_idx ] train_response = response_df [ list ( train_samples ) + [ 'YPD1' ]] test_response = response_df [ list ( test_samples ) + [ 'YPD1' ]] # Save fold data train_response . to_csv ( f 'data/fold_ { fold } _train_response.csv' ) test_response . to_csv ( f 'data/fold_ { fold } _test_response.csv' ) # Run analysis on training data # ... (analysis code) Troubleshooting Advanced Features \u00b6 Convergence Issues \u00b6 # Increase iterations and adjust tolerance python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data/expression.csv \\ --predictors_file data/binding.csv \\ --perturbed_tf YPD1 \\ --max_iter 50000 \\ --n_bootstraps 500 # Reduce for testing Memory Issues \u00b6 # Reduce computational load python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data/expression.csv \\ --predictors_file data/binding.csv \\ --perturbed_tf YPD1 \\ --n_bootstraps 100 \\ --top_n 100 \\ --n_cpus 2 Feature Selection Issues \u00b6 # More lenient thresholds python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data/expression.csv \\ --predictors_file data/binding.csv \\ --perturbed_tf YPD1 \\ --all_data_ci_level 80 .0 \\ --topn_ci_level 70 .0 Next Steps \u00b6 Input Formats : Detailed data preparation CLI Reference : Complete parameter documentation API Reference : Programmatic usage details","title":"Advanced Features"},{"location":"tutorials/advanced-features/#advanced-features-tutorial","text":"This tutorial covers advanced features and configuration options in tfbpmodeling for power users and specialized analyses.","title":"Advanced Features Tutorial"},{"location":"tutorials/advanced-features/#overview","text":"Beyond the basic workflow, tfbpmodeling offers advanced features for: Feature Engineering : Polynomial terms and custom variables Model Tuning : Advanced parameter optimization Statistical Methods : Alternative significance testing approaches Performance Optimization : High-performance computing configurations Custom Workflows : Programmatic usage and customization","title":"Overview"},{"location":"tutorials/advanced-features/#feature-engineering","text":"","title":"Feature Engineering"},{"location":"tutorials/advanced-features/#polynomial-terms","text":"Add non-linear relationships to capture complex binding-expression dynamics: # Add squared and cubic terms for the perturbed TF python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data/expression.csv \\ --predictors_file data/binding.csv \\ --perturbed_tf YPD1 \\ --squared_pTF \\ --cubic_pTF \\ --ptf_main_effect When to use polynomial terms : - Non-linear dose-response relationships - Saturation effects in binding - Threshold behaviors in gene expression","title":"Polynomial Terms"},{"location":"tutorials/advanced-features/#custom-variables","text":"Include additional experimental variables in the model: # Add batch effects and technical variables python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data/expression.csv \\ --predictors_file data/binding.csv \\ --perturbed_tf YPD1 \\ --add_model_variables \"batch_id,plate_position,extraction_date\" \\ --exclude_interactor_variables \"batch_id,technical_replicate\"","title":"Custom Variables"},{"location":"tutorials/advanced-features/#row-wise-features","text":"Include summary statistics as predictors: # Add maximum binding strength across all TFs python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data/expression.csv \\ --predictors_file data/binding.csv \\ --perturbed_tf YPD1 \\ --row_max \\ --normalize_sample_weights","title":"Row-wise Features"},{"location":"tutorials/advanced-features/#advanced-model-configuration","text":"","title":"Advanced Model Configuration"},{"location":"tutorials/advanced-features/#iterative-dropout","text":"Use iterative variable selection for feature-rich datasets: python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data/expression.csv \\ --predictors_file data/binding.csv \\ --perturbed_tf YPD1 \\ --iterative_dropout \\ --stabilization_ci_start 50 .0 \\ --all_data_ci_level 95 .0 How iterative dropout works : 1. Start with low confidence threshold (50%) 2. Remove non-significant features 3. Gradually increase threshold 4. Stabilize at final confidence level","title":"Iterative Dropout"},{"location":"tutorials/advanced-features/#stage-4-configuration","text":"Choose between linear and LassoCV approaches for final significance testing: # Conservative LassoCV approach python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data/expression.csv \\ --predictors_file data/binding.csv \\ --perturbed_tf YPD1 \\ --stage4_lasso \\ --stage4_topn # Sensitive linear regression approach (default) python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data/expression.csv \\ --predictors_file data/binding.csv \\ --perturbed_tf YPD1","title":"Stage 4 Configuration"},{"location":"tutorials/advanced-features/#data-preprocessing","text":"Advanced data handling options: python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data/expression.csv \\ --predictors_file data/binding.csv \\ --perturbed_tf YPD1 \\ --scale_by_std \\ --normalize_sample_weights \\ --bins \"0,5,10,15,20,np.inf\"","title":"Data Preprocessing"},{"location":"tutorials/advanced-features/#high-performance-computing","text":"","title":"High-Performance Computing"},{"location":"tutorials/advanced-features/#parallel-processing","text":"Optimize for multi-core systems: # Use all available cores python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data/expression.csv \\ --predictors_file data/binding.csv \\ --perturbed_tf YPD1 \\ --n_cpus 16 \\ --n_bootstraps 5000 \\ --max_iter 20000","title":"Parallel Processing"},{"location":"tutorials/advanced-features/#memory-management","text":"For large datasets: # Reduce memory usage python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data/expression.csv \\ --predictors_file data/binding.csv \\ --perturbed_tf YPD1 \\ --n_bootstraps 500 \\ --top_n 300 \\ --max_iter 5000","title":"Memory Management"},{"location":"tutorials/advanced-features/#cluster-computing","text":"Example SLURM script for HPC environments: #!/bin/bash #SBATCH --job-name=tfbp_analysis #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=32 #SBATCH --mem=64GB #SBATCH --time=24:00:00 module load python/3.11 source venv/bin/activate python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file $SCRATCH /data/expression.csv \\ --predictors_file $SCRATCH /data/binding.csv \\ --perturbed_tf $1 \\ --n_cpus 32 \\ --n_bootstraps 10000 \\ --output_dir $SCRATCH /results \\ --log-handler file","title":"Cluster Computing"},{"location":"tutorials/advanced-features/#programmatic-usage","text":"","title":"Programmatic Usage"},{"location":"tutorials/advanced-features/#python-api","text":"Use tfbpmodeling programmatically: import argparse from tfbpmodeling.interface import linear_perturbation_binding_modeling # Create arguments programmatically args = argparse . Namespace ( response_file = 'data/expression.csv' , predictors_file = 'data/binding.csv' , perturbed_tf = 'YPD1' , n_bootstraps = 2000 , top_n = 500 , all_data_ci_level = 95.0 , topn_ci_level = 85.0 , max_iter = 15000 , iterative_dropout = True , stage4_lasso = True , squared_pTF = True , ptf_main_effect = True , output_dir = './results' , output_suffix = '_programmatic' , n_cpus = 8 , random_state = 42 , # Set all other required parameters... ) # Run analysis linear_perturbation_binding_modeling ( args )","title":"Python API"},{"location":"tutorials/advanced-features/#batch-processing","text":"Process multiple transcription factors: import os from pathlib import Path tfs_to_analyze = [ 'YPD1' , 'YBR123W' , 'YCR456X' ] base_args = { 'response_file' : 'data/expression.csv' , 'predictors_file' : 'data/binding.csv' , 'n_bootstraps' : 2000 , 'random_state' : 42 , # ... other common parameters } for tf in tfs_to_analyze : print ( f \"Analyzing { tf } ...\" ) args = argparse . Namespace ( perturbed_tf = tf , output_suffix = f '_batch_ { tf } ' , ** base_args ) try : linear_perturbation_binding_modeling ( args ) print ( f \"\u2713 { tf } completed successfully\" ) except Exception as e : print ( f \"\u2717 { tf } failed: { e } \" )","title":"Batch Processing"},{"location":"tutorials/advanced-features/#advanced-analysis-patterns","text":"","title":"Advanced Analysis Patterns"},{"location":"tutorials/advanced-features/#comparative-analysis","text":"Compare different parameter settings: # Conservative analysis python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data/expression.csv \\ --predictors_file data/binding.csv \\ --perturbed_tf YPD1 \\ --all_data_ci_level 99 .0 \\ --topn_ci_level 95 .0 \\ --stage4_lasso \\ --output_suffix _conservative \\ --random_state 42 # Sensitive analysis python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data/expression.csv \\ --predictors_file data/binding.csv \\ --perturbed_tf YPD1 \\ --all_data_ci_level 90 .0 \\ --topn_ci_level 80 .0 \\ --output_suffix _sensitive \\ --random_state 42","title":"Comparative Analysis"},{"location":"tutorials/advanced-features/#cross-validation","text":"Validate results across different data subsets: from sklearn.model_selection import KFold import pandas as pd # Load data response_df = pd . read_csv ( 'data/expression.csv' , index_col = 0 ) predictor_df = pd . read_csv ( 'data/binding.csv' , index_col = 0 ) # Cross-validation across samples kf = KFold ( n_splits = 5 , shuffle = True , random_state = 42 ) for fold , ( train_idx , test_idx ) in enumerate ( kf . split ( response_df . columns [: - 1 ])): # Create train/test splits train_samples = response_df . columns [ train_idx ] test_samples = response_df . columns [ test_idx ] train_response = response_df [ list ( train_samples ) + [ 'YPD1' ]] test_response = response_df [ list ( test_samples ) + [ 'YPD1' ]] # Save fold data train_response . to_csv ( f 'data/fold_ { fold } _train_response.csv' ) test_response . to_csv ( f 'data/fold_ { fold } _test_response.csv' ) # Run analysis on training data # ... (analysis code)","title":"Cross-Validation"},{"location":"tutorials/advanced-features/#troubleshooting-advanced-features","text":"","title":"Troubleshooting Advanced Features"},{"location":"tutorials/advanced-features/#convergence-issues","text":"# Increase iterations and adjust tolerance python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data/expression.csv \\ --predictors_file data/binding.csv \\ --perturbed_tf YPD1 \\ --max_iter 50000 \\ --n_bootstraps 500 # Reduce for testing","title":"Convergence Issues"},{"location":"tutorials/advanced-features/#memory-issues","text":"# Reduce computational load python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data/expression.csv \\ --predictors_file data/binding.csv \\ --perturbed_tf YPD1 \\ --n_bootstraps 100 \\ --top_n 100 \\ --n_cpus 2","title":"Memory Issues"},{"location":"tutorials/advanced-features/#feature-selection-issues","text":"# More lenient thresholds python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file data/expression.csv \\ --predictors_file data/binding.csv \\ --perturbed_tf YPD1 \\ --all_data_ci_level 80 .0 \\ --topn_ci_level 70 .0","title":"Feature Selection Issues"},{"location":"tutorials/advanced-features/#next-steps","text":"Input Formats : Detailed data preparation CLI Reference : Complete parameter documentation API Reference : Programmatic usage details","title":"Next Steps"},{"location":"tutorials/basic-workflow/","text":"Basic Workflow Tutorial \u00b6 This tutorial walks through a complete tfbpmodeling analysis from data preparation to result interpretation. Overview \u00b6 We'll analyze the relationship between transcription factor binding and gene expression perturbation using a sample dataset. The workflow demonstrates: Data preparation : Formatting input files Basic analysis : Running with default parameters Result interpretation : Understanding output files Parameter tuning : Optimizing for your data Prerequisites \u00b6 tfbpmodeling installed and configured Basic familiarity with CSV files and command-line interfaces Understanding of transcription factor biology (helpful but not required) Sample Data \u00b6 For this tutorial, we'll use example data representing: - Response data : Gene expression changes after YPD1 knockout - Predictor data : Transcription factor binding probabilities from ChIP-seq Creating Sample Data \u00b6 import pandas as pd import numpy as np # Set random seed for reproducibility np . random . seed ( 42 ) # Create sample gene list genes = [ f \"YBR { str ( i ) . zfill ( 3 ) } W\" for i in range ( 1 , 1001 )] samples = [ f \"sample_ { i } \" for i in range ( 1 , 101 )] tfs = [ f \"TF_ { i } \" for i in range ( 1 , 51 )] # Generate response data (expression changes) response_data = pd . DataFrame ( np . random . normal ( 0 , 1 , ( 1000 , 100 )), index = genes , columns = samples ) response_data . index . name = 'gene_id' # Add YPD1 column (our perturbed TF) response_data [ 'YPD1' ] = np . random . normal ( - 0.5 , 0.8 , 1000 ) # Generate predictor data (binding probabilities) predictor_data = pd . DataFrame ( np . random . beta ( 0.5 , 2 , ( 1000 , 50 )), index = genes , columns = tfs ) predictor_data . index . name = 'gene_id' # Save to CSV response_data . to_csv ( 'tutorial_expression.csv' ) predictor_data . to_csv ( 'tutorial_binding.csv' ) print ( \"Sample data created:\" ) print ( f \"Response data: { response_data . shape } \" ) print ( f \"Predictor data: { predictor_data . shape } \" ) Step 1: Basic Analysis \u00b6 Run Default Analysis \u00b6 Start with the simplest possible command: python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file tutorial_expression.csv \\ --predictors_file tutorial_binding.csv \\ --perturbed_tf YPD1 This command will: - Use 1000 bootstrap samples - Apply 98% confidence interval for feature selection - Process top 600 features in second round - Save results to ./linear_perturbation_binding_modeling_results/YPD1_{timestamp}/ Monitor Progress \u00b6 The command provides real-time progress information: 2024-01-15 14:30:22 - INFO - Starting linear perturbation binding modeling 2024-01-15 14:30:22 - INFO - Loading response data from: tutorial_expression.csv 2024-01-15 14:30:23 - INFO - Loading predictor data from: tutorial_binding.csv 2024-01-15 14:30:23 - INFO - Perturbed TF: YPD1 2024-01-15 14:30:23 - INFO - Data preprocessing complete 2024-01-15 14:30:23 - INFO - Features: 1000, Samples: 100 2024-01-15 14:30:24 - INFO - Starting Stage 1: Bootstrap modeling on all data 2024-01-15 14:30:24 - INFO - Bootstrap parameters: n_bootstraps=1000, random_state=None 2024-01-15 14:32:15 - INFO - Stage 1 complete. Significant features: 156 2024-01-15 14:32:15 - INFO - Starting Stage 2: Top-N modeling 2024-01-15 14:33:45 - INFO - Stage 2 complete. Refined features: 78 2024-01-15 14:33:45 - INFO - Starting Stage 3: Interactor significance testing 2024-01-15 14:34:20 - INFO - Analysis complete. Results saved to: ./linear_perturbation_binding_modeling_results/YPD1_20240115_143022/ Step 2: Understanding Results \u00b6 Output Directory Structure \u00b6 After completion, examine the results directory: ls -la linear_perturbation_binding_modeling_results/YPD1_*/ YPD1_20240115_143022/ \u251c\u2500\u2500 all_data_results/ \u2502 \u251c\u2500\u2500 bootstrap_coefficients.csv \u2502 \u251c\u2500\u2500 confidence_intervals.csv \u2502 \u251c\u2500\u2500 model_statistics.csv \u2502 \u2514\u2500\u2500 diagnostic_plots/ \u251c\u2500\u2500 topn_results/ \u2502 \u251c\u2500\u2500 bootstrap_coefficients.csv \u2502 \u251c\u2500\u2500 confidence_intervals.csv \u2502 \u251c\u2500\u2500 model_statistics.csv \u2502 \u2514\u2500\u2500 diagnostic_plots/ \u251c\u2500\u2500 interactor_significance/ \u2502 \u251c\u2500\u2500 significance_results.csv \u2502 \u251c\u2500\u2500 comparison_statistics.csv \u2502 \u2514\u2500\u2500 final_selection.csv \u251c\u2500\u2500 input_data/ \u2502 \u251c\u2500\u2500 processed_response.csv \u2502 \u251c\u2500\u2500 processed_predictors.csv \u2502 \u2514\u2500\u2500 data_summary.json \u2514\u2500\u2500 tfbpmodeling_20240115_143022.log Key Result Files \u00b6 1. Confidence Intervals (most important) \u00b6 head -10 YPD1_*/all_data_results/confidence_intervals.csv feature,mean_coef,std_coef,ci_lower,ci_upper,significant,abs_mean_coef TF_1,0.023,0.008,0.007,0.039,True,0.023 TF_2,-0.045,0.012,-0.069,-0.021,True,0.045 TF_3,0.001,0.006,-0.011,0.013,False,0.001 Key columns : - feature : Transcription factor name - mean_coef : Average effect size across bootstrap samples - ci_lower/ci_upper : Confidence interval bounds - significant : Whether the effect is statistically significant 2. Model Statistics \u00b6 cat YPD1_*/all_data_results/model_statistics.csv metric,value mean_r2,0.234 std_r2,0.023 mean_cv_score,0.198 n_significant_features,156 total_features,1000 Key metrics : - mean_r2 : Model explanatory power - mean_cv_score : Cross-validation performance - n_significant_features : Count of statistically significant predictors 3. Final Significant Interactions \u00b6 head -10 YPD1_*/interactor_significance/final_selection.csv feature,interaction_coef,main_effect_coef,p_value,significant,effect_size TF_1:binding_strength,0.034,0.012,0.003,True,0.022 TF_2:binding_strength,-0.028,-0.008,0.012,True,0.020 This shows transcription factors with significant interaction effects beyond their main effects. Step 3: Interpreting Results \u00b6 Biological Interpretation \u00b6 Significant Features : TFs with non-zero confidence intervals affect YPD1 expression Effect Direction : Positive coefficients indicate binding increases expression Effect Size : Larger absolute coefficients indicate stronger effects Interactions : Features in final selection have context-dependent effects Statistical Interpretation \u00b6 Confidence Intervals : 98% CIs that exclude zero are statistically significant Bootstrap Stability : Lower standard deviations indicate more stable effects Cross-Validation : CV scores show generalization performance Multiple Testing : Built-in correction through bootstrap resampling Example Interpretation \u00b6 From our results: - TF_1 (coef: 0.023): Binding increases YPD1 expression - TF_2 (coef: -0.045): Binding decreases YPD1 expression - TF_3 (coef: 0.001, not significant): No detectable effect Step 4: Parameter Optimization \u00b6 Increasing Statistical Power \u00b6 For more robust results, increase bootstrap samples: python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file tutorial_expression.csv \\ --predictors_file tutorial_binding.csv \\ --perturbed_tf YPD1 \\ --n_bootstraps 2000 \\ --output_suffix _high_power Adjusting Sensitivity \u00b6 For more sensitive detection, lower confidence thresholds: python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file tutorial_expression.csv \\ --predictors_file tutorial_binding.csv \\ --perturbed_tf YPD1 \\ --all_data_ci_level 95 .0 \\ --topn_ci_level 85 .0 \\ --output_suffix _sensitive Adding Feature Engineering \u00b6 Include polynomial terms for non-linear relationships: python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file tutorial_expression.csv \\ --predictors_file tutorial_binding.csv \\ --perturbed_tf YPD1 \\ --squared_pTF \\ --ptf_main_effect \\ --row_max \\ --output_suffix _engineered Reproducible Analysis \u00b6 For reproducible results, set random seed: python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file tutorial_expression.csv \\ --predictors_file tutorial_binding.csv \\ --perturbed_tf YPD1 \\ --random_state 42 \\ --output_suffix _reproducible Step 5: Comparing Results \u00b6 Compare Different Analyses \u00b6 # List all result directories ls -d YPD1_*/ # Compare significant feature counts echo \"Analysis,Significant_Features\" for dir in YPD1_*/ ; do count = $( tail -n +2 \" $dir /all_data_results/confidence_intervals.csv\" | awk -F ',' '$6==\"True\"' | wc -l ) echo \" $dir , $count \" done Visualize Results \u00b6 import pandas as pd import matplotlib.pyplot as plt # Load confidence intervals from different analyses default_ci = pd . read_csv ( 'YPD1_20240115_143022/all_data_results/confidence_intervals.csv' ) sensitive_ci = pd . read_csv ( 'YPD1_sensitive_20240115_143522/all_data_results/confidence_intervals.csv' ) # Compare significant feature counts print ( f \"Default analysis: { default_ci [ 'significant' ] . sum () } significant features\" ) print ( f \"Sensitive analysis: { sensitive_ci [ 'significant' ] . sum () } significant features\" ) # Plot effect size distributions fig , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 , figsize = ( 12 , 5 )) ax1 . hist ( default_ci [ 'abs_mean_coef' ], bins = 30 , alpha = 0.7 , label = 'Default' ) ax1 . set_xlabel ( 'Absolute Effect Size' ) ax1 . set_ylabel ( 'Frequency' ) ax1 . set_title ( 'Default Analysis' ) ax2 . hist ( sensitive_ci [ 'abs_mean_coef' ], bins = 30 , alpha = 0.7 , label = 'Sensitive' , color = 'orange' ) ax2 . set_xlabel ( 'Absolute Effect Size' ) ax2 . set_ylabel ( 'Frequency' ) ax2 . set_title ( 'Sensitive Analysis' ) plt . tight_layout () plt . savefig ( 'effect_size_comparison.png' ) plt . show () Next Steps \u00b6 For Your Own Data \u00b6 Prepare your files : Follow the CSV format requirements Start simple : Use default parameters first Validate results : Check that results make biological sense Optimize parameters : Adjust based on data characteristics Document analysis : Save parameter choices and interpretations Advanced Techniques \u00b6 Advanced Features Tutorial : Feature engineering and model tuning Input Formats Guide : Detailed data preparation instructions CLI Reference : Complete parameter documentation Common Issues \u00b6 Low R\u00b2 Scores \u00b6 Cause : Weak signal, noisy data, or model misspecification Solutions : Increase sample size, add feature engineering, check data quality Few Significant Features \u00b6 Cause : Stringent thresholds or weak effects Solutions : Lower confidence levels, increase bootstrap samples, check effect sizes Long Runtime \u00b6 Cause : Large datasets or high bootstrap counts Solutions : Reduce parameters, increase --n_cpus , use smaller subset for testing Summary \u00b6 This tutorial demonstrated: Basic analysis with default parameters Result interpretation using key output files Parameter optimization for different analysis goals Comparison methods for evaluating different approaches The tfbpmodeling workflow provides a robust framework for analyzing transcription factor binding and perturbation relationships while controlling for multiple testing and providing interpretable results.","title":"Basic Workflow"},{"location":"tutorials/basic-workflow/#basic-workflow-tutorial","text":"This tutorial walks through a complete tfbpmodeling analysis from data preparation to result interpretation.","title":"Basic Workflow Tutorial"},{"location":"tutorials/basic-workflow/#overview","text":"We'll analyze the relationship between transcription factor binding and gene expression perturbation using a sample dataset. The workflow demonstrates: Data preparation : Formatting input files Basic analysis : Running with default parameters Result interpretation : Understanding output files Parameter tuning : Optimizing for your data","title":"Overview"},{"location":"tutorials/basic-workflow/#prerequisites","text":"tfbpmodeling installed and configured Basic familiarity with CSV files and command-line interfaces Understanding of transcription factor biology (helpful but not required)","title":"Prerequisites"},{"location":"tutorials/basic-workflow/#sample-data","text":"For this tutorial, we'll use example data representing: - Response data : Gene expression changes after YPD1 knockout - Predictor data : Transcription factor binding probabilities from ChIP-seq","title":"Sample Data"},{"location":"tutorials/basic-workflow/#creating-sample-data","text":"import pandas as pd import numpy as np # Set random seed for reproducibility np . random . seed ( 42 ) # Create sample gene list genes = [ f \"YBR { str ( i ) . zfill ( 3 ) } W\" for i in range ( 1 , 1001 )] samples = [ f \"sample_ { i } \" for i in range ( 1 , 101 )] tfs = [ f \"TF_ { i } \" for i in range ( 1 , 51 )] # Generate response data (expression changes) response_data = pd . DataFrame ( np . random . normal ( 0 , 1 , ( 1000 , 100 )), index = genes , columns = samples ) response_data . index . name = 'gene_id' # Add YPD1 column (our perturbed TF) response_data [ 'YPD1' ] = np . random . normal ( - 0.5 , 0.8 , 1000 ) # Generate predictor data (binding probabilities) predictor_data = pd . DataFrame ( np . random . beta ( 0.5 , 2 , ( 1000 , 50 )), index = genes , columns = tfs ) predictor_data . index . name = 'gene_id' # Save to CSV response_data . to_csv ( 'tutorial_expression.csv' ) predictor_data . to_csv ( 'tutorial_binding.csv' ) print ( \"Sample data created:\" ) print ( f \"Response data: { response_data . shape } \" ) print ( f \"Predictor data: { predictor_data . shape } \" )","title":"Creating Sample Data"},{"location":"tutorials/basic-workflow/#step-1-basic-analysis","text":"","title":"Step 1: Basic Analysis"},{"location":"tutorials/basic-workflow/#run-default-analysis","text":"Start with the simplest possible command: python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file tutorial_expression.csv \\ --predictors_file tutorial_binding.csv \\ --perturbed_tf YPD1 This command will: - Use 1000 bootstrap samples - Apply 98% confidence interval for feature selection - Process top 600 features in second round - Save results to ./linear_perturbation_binding_modeling_results/YPD1_{timestamp}/","title":"Run Default Analysis"},{"location":"tutorials/basic-workflow/#monitor-progress","text":"The command provides real-time progress information: 2024-01-15 14:30:22 - INFO - Starting linear perturbation binding modeling 2024-01-15 14:30:22 - INFO - Loading response data from: tutorial_expression.csv 2024-01-15 14:30:23 - INFO - Loading predictor data from: tutorial_binding.csv 2024-01-15 14:30:23 - INFO - Perturbed TF: YPD1 2024-01-15 14:30:23 - INFO - Data preprocessing complete 2024-01-15 14:30:23 - INFO - Features: 1000, Samples: 100 2024-01-15 14:30:24 - INFO - Starting Stage 1: Bootstrap modeling on all data 2024-01-15 14:30:24 - INFO - Bootstrap parameters: n_bootstraps=1000, random_state=None 2024-01-15 14:32:15 - INFO - Stage 1 complete. Significant features: 156 2024-01-15 14:32:15 - INFO - Starting Stage 2: Top-N modeling 2024-01-15 14:33:45 - INFO - Stage 2 complete. Refined features: 78 2024-01-15 14:33:45 - INFO - Starting Stage 3: Interactor significance testing 2024-01-15 14:34:20 - INFO - Analysis complete. Results saved to: ./linear_perturbation_binding_modeling_results/YPD1_20240115_143022/","title":"Monitor Progress"},{"location":"tutorials/basic-workflow/#step-2-understanding-results","text":"","title":"Step 2: Understanding Results"},{"location":"tutorials/basic-workflow/#output-directory-structure","text":"After completion, examine the results directory: ls -la linear_perturbation_binding_modeling_results/YPD1_*/ YPD1_20240115_143022/ \u251c\u2500\u2500 all_data_results/ \u2502 \u251c\u2500\u2500 bootstrap_coefficients.csv \u2502 \u251c\u2500\u2500 confidence_intervals.csv \u2502 \u251c\u2500\u2500 model_statistics.csv \u2502 \u2514\u2500\u2500 diagnostic_plots/ \u251c\u2500\u2500 topn_results/ \u2502 \u251c\u2500\u2500 bootstrap_coefficients.csv \u2502 \u251c\u2500\u2500 confidence_intervals.csv \u2502 \u251c\u2500\u2500 model_statistics.csv \u2502 \u2514\u2500\u2500 diagnostic_plots/ \u251c\u2500\u2500 interactor_significance/ \u2502 \u251c\u2500\u2500 significance_results.csv \u2502 \u251c\u2500\u2500 comparison_statistics.csv \u2502 \u2514\u2500\u2500 final_selection.csv \u251c\u2500\u2500 input_data/ \u2502 \u251c\u2500\u2500 processed_response.csv \u2502 \u251c\u2500\u2500 processed_predictors.csv \u2502 \u2514\u2500\u2500 data_summary.json \u2514\u2500\u2500 tfbpmodeling_20240115_143022.log","title":"Output Directory Structure"},{"location":"tutorials/basic-workflow/#key-result-files","text":"","title":"Key Result Files"},{"location":"tutorials/basic-workflow/#1-confidence-intervals-most-important","text":"head -10 YPD1_*/all_data_results/confidence_intervals.csv feature,mean_coef,std_coef,ci_lower,ci_upper,significant,abs_mean_coef TF_1,0.023,0.008,0.007,0.039,True,0.023 TF_2,-0.045,0.012,-0.069,-0.021,True,0.045 TF_3,0.001,0.006,-0.011,0.013,False,0.001 Key columns : - feature : Transcription factor name - mean_coef : Average effect size across bootstrap samples - ci_lower/ci_upper : Confidence interval bounds - significant : Whether the effect is statistically significant","title":"1. Confidence Intervals (most important)"},{"location":"tutorials/basic-workflow/#2-model-statistics","text":"cat YPD1_*/all_data_results/model_statistics.csv metric,value mean_r2,0.234 std_r2,0.023 mean_cv_score,0.198 n_significant_features,156 total_features,1000 Key metrics : - mean_r2 : Model explanatory power - mean_cv_score : Cross-validation performance - n_significant_features : Count of statistically significant predictors","title":"2. Model Statistics"},{"location":"tutorials/basic-workflow/#3-final-significant-interactions","text":"head -10 YPD1_*/interactor_significance/final_selection.csv feature,interaction_coef,main_effect_coef,p_value,significant,effect_size TF_1:binding_strength,0.034,0.012,0.003,True,0.022 TF_2:binding_strength,-0.028,-0.008,0.012,True,0.020 This shows transcription factors with significant interaction effects beyond their main effects.","title":"3. Final Significant Interactions"},{"location":"tutorials/basic-workflow/#step-3-interpreting-results","text":"","title":"Step 3: Interpreting Results"},{"location":"tutorials/basic-workflow/#biological-interpretation","text":"Significant Features : TFs with non-zero confidence intervals affect YPD1 expression Effect Direction : Positive coefficients indicate binding increases expression Effect Size : Larger absolute coefficients indicate stronger effects Interactions : Features in final selection have context-dependent effects","title":"Biological Interpretation"},{"location":"tutorials/basic-workflow/#statistical-interpretation","text":"Confidence Intervals : 98% CIs that exclude zero are statistically significant Bootstrap Stability : Lower standard deviations indicate more stable effects Cross-Validation : CV scores show generalization performance Multiple Testing : Built-in correction through bootstrap resampling","title":"Statistical Interpretation"},{"location":"tutorials/basic-workflow/#example-interpretation","text":"From our results: - TF_1 (coef: 0.023): Binding increases YPD1 expression - TF_2 (coef: -0.045): Binding decreases YPD1 expression - TF_3 (coef: 0.001, not significant): No detectable effect","title":"Example Interpretation"},{"location":"tutorials/basic-workflow/#step-4-parameter-optimization","text":"","title":"Step 4: Parameter Optimization"},{"location":"tutorials/basic-workflow/#increasing-statistical-power","text":"For more robust results, increase bootstrap samples: python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file tutorial_expression.csv \\ --predictors_file tutorial_binding.csv \\ --perturbed_tf YPD1 \\ --n_bootstraps 2000 \\ --output_suffix _high_power","title":"Increasing Statistical Power"},{"location":"tutorials/basic-workflow/#adjusting-sensitivity","text":"For more sensitive detection, lower confidence thresholds: python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file tutorial_expression.csv \\ --predictors_file tutorial_binding.csv \\ --perturbed_tf YPD1 \\ --all_data_ci_level 95 .0 \\ --topn_ci_level 85 .0 \\ --output_suffix _sensitive","title":"Adjusting Sensitivity"},{"location":"tutorials/basic-workflow/#adding-feature-engineering","text":"Include polynomial terms for non-linear relationships: python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file tutorial_expression.csv \\ --predictors_file tutorial_binding.csv \\ --perturbed_tf YPD1 \\ --squared_pTF \\ --ptf_main_effect \\ --row_max \\ --output_suffix _engineered","title":"Adding Feature Engineering"},{"location":"tutorials/basic-workflow/#reproducible-analysis","text":"For reproducible results, set random seed: python -m tfbpmodeling linear_perturbation_binding_modeling \\ --response_file tutorial_expression.csv \\ --predictors_file tutorial_binding.csv \\ --perturbed_tf YPD1 \\ --random_state 42 \\ --output_suffix _reproducible","title":"Reproducible Analysis"},{"location":"tutorials/basic-workflow/#step-5-comparing-results","text":"","title":"Step 5: Comparing Results"},{"location":"tutorials/basic-workflow/#compare-different-analyses","text":"# List all result directories ls -d YPD1_*/ # Compare significant feature counts echo \"Analysis,Significant_Features\" for dir in YPD1_*/ ; do count = $( tail -n +2 \" $dir /all_data_results/confidence_intervals.csv\" | awk -F ',' '$6==\"True\"' | wc -l ) echo \" $dir , $count \" done","title":"Compare Different Analyses"},{"location":"tutorials/basic-workflow/#visualize-results","text":"import pandas as pd import matplotlib.pyplot as plt # Load confidence intervals from different analyses default_ci = pd . read_csv ( 'YPD1_20240115_143022/all_data_results/confidence_intervals.csv' ) sensitive_ci = pd . read_csv ( 'YPD1_sensitive_20240115_143522/all_data_results/confidence_intervals.csv' ) # Compare significant feature counts print ( f \"Default analysis: { default_ci [ 'significant' ] . sum () } significant features\" ) print ( f \"Sensitive analysis: { sensitive_ci [ 'significant' ] . sum () } significant features\" ) # Plot effect size distributions fig , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 , figsize = ( 12 , 5 )) ax1 . hist ( default_ci [ 'abs_mean_coef' ], bins = 30 , alpha = 0.7 , label = 'Default' ) ax1 . set_xlabel ( 'Absolute Effect Size' ) ax1 . set_ylabel ( 'Frequency' ) ax1 . set_title ( 'Default Analysis' ) ax2 . hist ( sensitive_ci [ 'abs_mean_coef' ], bins = 30 , alpha = 0.7 , label = 'Sensitive' , color = 'orange' ) ax2 . set_xlabel ( 'Absolute Effect Size' ) ax2 . set_ylabel ( 'Frequency' ) ax2 . set_title ( 'Sensitive Analysis' ) plt . tight_layout () plt . savefig ( 'effect_size_comparison.png' ) plt . show ()","title":"Visualize Results"},{"location":"tutorials/basic-workflow/#next-steps","text":"","title":"Next Steps"},{"location":"tutorials/basic-workflow/#for-your-own-data","text":"Prepare your files : Follow the CSV format requirements Start simple : Use default parameters first Validate results : Check that results make biological sense Optimize parameters : Adjust based on data characteristics Document analysis : Save parameter choices and interpretations","title":"For Your Own Data"},{"location":"tutorials/basic-workflow/#advanced-techniques","text":"Advanced Features Tutorial : Feature engineering and model tuning Input Formats Guide : Detailed data preparation instructions CLI Reference : Complete parameter documentation","title":"Advanced Techniques"},{"location":"tutorials/basic-workflow/#common-issues","text":"","title":"Common Issues"},{"location":"tutorials/basic-workflow/#low-r2-scores","text":"Cause : Weak signal, noisy data, or model misspecification Solutions : Increase sample size, add feature engineering, check data quality","title":"Low R\u00b2 Scores"},{"location":"tutorials/basic-workflow/#few-significant-features","text":"Cause : Stringent thresholds or weak effects Solutions : Lower confidence levels, increase bootstrap samples, check effect sizes","title":"Few Significant Features"},{"location":"tutorials/basic-workflow/#long-runtime","text":"Cause : Large datasets or high bootstrap counts Solutions : Reduce parameters, increase --n_cpus , use smaller subset for testing","title":"Long Runtime"},{"location":"tutorials/basic-workflow/#summary","text":"This tutorial demonstrated: Basic analysis with default parameters Result interpretation using key output files Parameter optimization for different analysis goals Comparison methods for evaluating different approaches The tfbpmodeling workflow provides a robust framework for analyzing transcription factor binding and perturbation relationships while controlling for multiple testing and providing interpretable results.","title":"Summary"},{"location":"tutorials/input-formats/","text":"Input Data Formats \u00b6 This guide provides detailed specifications for preparing input data for tfbpmodeling analysis. Overview \u00b6 tfbpmodeling requires two main input files plus optional supplementary files: Response File : Gene expression data (dependent variable) Predictors File : Transcription factor binding data (independent variables) Blacklist File (optional): Features to exclude from analysis Response File Format \u00b6 Structure \u00b6 The response file contains gene expression measurements: gene_id,sample_1,sample_2,sample_3,sample_4,YPD1 YBR001C,0.234,-1.456,0.876,-0.123,0.543 YBR002W,-0.456,0.123,1.234,-0.567,-0.234 YBR003W,1.234,0.567,-0.234,0.876,0.123 YBR004C,-0.123,-0.876,0.456,0.234,-0.456 Requirements \u00b6 Element Requirement Description Format CSV with comma separators Standard comma-separated values First Column Gene identifiers Must match predictor file exactly Header Row Sample names + perturbed TF Column names for each measurement Data Cells Numeric values Expression measurements (log2 fold-change, z-scores, etc.) Perturbed TF Column Must be present Column name must match --perturbed_tf parameter Data Types \u00b6 Supported expression data types : - Log2 fold-change values - Z-scores or standardized values - Raw expression values (will be normalized if needed) - Differential expression statistics Example expression data preparation : import pandas as pd import numpy as np # Load raw expression data raw_expr = pd . read_csv ( 'raw_expression.csv' , index_col = 0 ) # Calculate log2 fold-change vs control control_samples = [ 'ctrl_1' , 'ctrl_2' , 'ctrl_3' ] treatment_samples = [ 'treat_1' , 'treat_2' , 'treat_3' ] control_mean = raw_expr [ control_samples ] . mean ( axis = 1 ) treatment_mean = raw_expr [ treatment_samples ] . mean ( axis = 1 ) log2fc = np . log2 ( treatment_mean + 1 ) - np . log2 ( control_mean + 1 ) # Create response file response_df = pd . DataFrame ({ 'sample_1' : log2fc , 'sample_2' : log2fc + np . random . normal ( 0 , 0.1 , len ( log2fc )), 'YPD1' : log2fc # Perturbed TF response }) response_df . index . name = 'gene_id' response_df . to_csv ( 'response_data.csv' ) Predictors File Format \u00b6 Structure \u00b6 The predictors file contains transcription factor binding data: gene_id,TF_1,TF_2,TF_3,TF_4,YPD1_binding YBR001C,0.123,0.456,0.789,0.012,0.345 YBR002W,0.234,0.567,0.890,0.123,0.456 YBR003W,0.345,0.678,0.901,0.234,0.567 YBR004C,0.456,0.789,0.012,0.345,0.678 Requirements \u00b6 Element Requirement Description Format CSV with comma separators Standard comma-separated values First Column Gene identifiers Must match response file exactly Header Row TF names Transcription factor identifiers Data Cells Numeric values 0-1 Binding probabilities or normalized scores No Missing Values Complete data required All cells must contain numeric values Data Types \u00b6 Supported binding data types : - ChIP-seq binding probabilities (0-1) - Normalized binding scores (0-1) - Peak presence indicators (0/1) - Binding strength quantiles (0-1) Example binding data preparation : import pandas as pd import numpy as np from sklearn.preprocessing import MinMaxScaler # Load ChIP-seq peak scores chip_data = pd . read_csv ( 'chip_peaks.csv' , index_col = 0 ) # Normalize to 0-1 range scaler = MinMaxScaler () normalized_binding = pd . DataFrame ( scaler . fit_transform ( chip_data ), index = chip_data . index , columns = chip_data . columns ) # Save as predictors file normalized_binding . index . name = 'gene_id' normalized_binding . to_csv ( 'binding_data.csv' ) Gene Identifier Consistency \u00b6 Critical Requirements \u00b6 Both files must use identical gene identifiers: # Verify gene ID consistency response_df = pd . read_csv ( 'response.csv' , index_col = 0 ) predictor_df = pd . read_csv ( 'predictors.csv' , index_col = 0 ) # Check for exact matches common_genes = set ( response_df . index ) & set ( predictor_df . index ) response_only = set ( response_df . index ) - set ( predictor_df . index ) predictor_only = set ( predictor_df . index ) - set ( response_df . index ) print ( f \"Common genes: { len ( common_genes ) } \" ) print ( f \"Response only: { len ( response_only ) } \" ) print ( f \"Predictor only: { len ( predictor_only ) } \" ) Common Gene ID Formats \u00b6 Format Example Notes Systematic names YBR001C S. cerevisiae standard Gene symbols CDC42 Human/mouse standard Ensembl IDs ENSG00000123456 Cross-species compatible RefSeq IDs NM_001234567 NCBI standard Handling ID Mismatches \u00b6 # Align gene IDs between files def align_gene_ids ( response_df , predictor_df ): # Find common genes common_genes = list ( set ( response_df . index ) & set ( predictor_df . index )) # Subset both dataframes aligned_response = response_df . loc [ common_genes ] aligned_predictor = predictor_df . loc [ common_genes ] return aligned_response , aligned_predictor # Apply alignment aligned_response , aligned_predictor = align_gene_ids ( response_df , predictor_df ) # Save aligned files aligned_response . to_csv ( 'aligned_response.csv' ) aligned_predictor . to_csv ( 'aligned_predictors.csv' ) Blacklist File Format \u00b6 Structure \u00b6 Simple text file with one gene identifier per line: YBR999W YCR888X ribosomal_protein_L1 housekeeping_gene_1 batch_effect_gene Use Cases \u00b6 Common genes to blacklist : - Housekeeping genes with stable expression - Ribosomal protein genes - Mitochondrial genes - Known batch effect genes - Genes with technical artifacts Creating Blacklist Files \u00b6 # Identify housekeeping genes housekeeping_genes = [ 'ACT1' , 'TUB1' , 'TUB2' , 'RDN18-1' , 'RDN25-1' ] # Identify low-variance genes low_variance_genes = response_df . var ( axis = 1 ) . sort_values () . head ( 50 ) . index . tolist () # Combine blacklists blacklist_genes = list ( set ( housekeeping_genes + low_variance_genes )) # Save blacklist with open ( 'blacklist.txt' , 'w' ) as f : for gene in sorted ( blacklist_genes ): f . write ( f \" { gene } \\n \" ) Data Quality Checks \u00b6 Automated Validation \u00b6 def validate_input_data ( response_file , predictor_file , perturbed_tf ): \"\"\"Validate input data format and consistency.\"\"\" # Load data response_df = pd . read_csv ( response_file , index_col = 0 ) predictor_df = pd . read_csv ( predictor_file , index_col = 0 ) # Check 1: File formats assert response_df . index . name == 'gene_id' , \"Response file must have 'gene_id' as index\" assert predictor_df . index . name == 'gene_id' , \"Predictor file must have 'gene_id' as index\" # Check 2: Perturbed TF presence assert perturbed_tf in response_df . columns , f \"Perturbed TF ' { perturbed_tf } ' not found in response\" # Check 3: Gene ID overlap common_genes = set ( response_df . index ) & set ( predictor_df . index ) assert len ( common_genes ) > 100 , f \"Too few common genes: { len ( common_genes ) } \" # Check 4: Data types assert response_df . dtypes . apply ( lambda x : np . issubdtype ( x , np . number )) . all () assert predictor_df . dtypes . apply ( lambda x : np . issubdtype ( x , np . number )) . all () # Check 5: Missing values assert not response_df . isnull () . any () . any (), \"Response data contains missing values\" assert not predictor_df . isnull () . any () . any (), \"Predictor data contains missing values\" # Check 6: Value ranges predictor_ranges = predictor_df . describe () if ( predictor_ranges . loc [ 'min' ] < 0 ) . any (): print ( \"Warning: Some predictor values are negative\" ) if ( predictor_ranges . loc [ 'max' ] > 1 ) . any (): print ( \"Warning: Some predictor values exceed 1\" ) print ( \"\u2713 All validation checks passed\" ) # Run validation validate_input_data ( 'response.csv' , 'predictors.csv' , 'YPD1' ) Manual Quality Assessment \u00b6 # Data exploration def explore_data ( response_file , predictor_file ): response_df = pd . read_csv ( response_file , index_col = 0 ) predictor_df = pd . read_csv ( predictor_file , index_col = 0 ) print ( \"=== Response Data ===\" ) print ( f \"Shape: { response_df . shape } \" ) print ( f \"Columns: { list ( response_df . columns ) } \" ) print ( f \"Value range: [ { response_df . min () . min () : .3f } , { response_df . max () . max () : .3f } ]\" ) print ( f \"Missing values: { response_df . isnull () . sum () . sum () } \" ) print ( \" \\n === Predictor Data ===\" ) print ( f \"Shape: { predictor_df . shape } \" ) print ( f \"Columns: { list ( predictor_df . columns [: 5 ]) } ...\" if len ( predictor_df . columns ) > 5 else list ( predictor_df . columns )) print ( f \"Value range: [ { predictor_df . min () . min () : .3f } , { predictor_df . max () . max () : .3f } ]\" ) print ( f \"Missing values: { predictor_df . isnull () . sum () . sum () } \" ) # Plot distributions import matplotlib.pyplot as plt fig , axes = plt . subplots ( 1 , 2 , figsize = ( 12 , 4 )) # Response distribution response_df . iloc [:, 0 ] . hist ( bins = 50 , ax = axes [ 0 ]) axes [ 0 ] . set_title ( 'Response Data Distribution' ) axes [ 0 ] . set_xlabel ( 'Expression Values' ) # Predictor distribution predictor_df . iloc [:, 0 ] . hist ( bins = 50 , ax = axes [ 1 ]) axes [ 1 ] . set_title ( 'Predictor Data Distribution' ) axes [ 1 ] . set_xlabel ( 'Binding Values' ) plt . tight_layout () plt . savefig ( 'data_distributions.png' ) plt . show () # Explore your data explore_data ( 'response.csv' , 'predictors.csv' ) Example Data Preparation Workflow \u00b6 Complete Pipeline \u00b6 import pandas as pd import numpy as np from sklearn.preprocessing import StandardScaler , MinMaxScaler def prepare_tfbp_data ( raw_expression_file , raw_binding_file , perturbed_tf , output_prefix ): \"\"\"Complete data preparation pipeline.\"\"\" # 1. Load raw data print ( \"Loading raw data...\" ) expr_raw = pd . read_csv ( raw_expression_file , index_col = 0 ) binding_raw = pd . read_csv ( raw_binding_file , index_col = 0 ) # 2. Process expression data print ( \"Processing expression data...\" ) # Log-transform if needed if expr_raw . min () . min () >= 0 : expr_processed = np . log2 ( expr_raw + 1 ) else : expr_processed = expr_raw # Add perturbed TF column (example: mean of treatment samples) if perturbed_tf not in expr_processed . columns : # Calculate as mean expression change expr_processed [ perturbed_tf ] = expr_processed . mean ( axis = 1 ) # 3. Process binding data print ( \"Processing binding data...\" ) # Normalize to 0-1 range scaler = MinMaxScaler () binding_processed = pd . DataFrame ( scaler . fit_transform ( binding_raw ), index = binding_raw . index , columns = binding_raw . columns ) # 4. Align gene IDs print ( \"Aligning gene identifiers...\" ) common_genes = list ( set ( expr_processed . index ) & set ( binding_processed . index )) print ( f \"Common genes: { len ( common_genes ) } \" ) expr_aligned = expr_processed . loc [ common_genes ] binding_aligned = binding_processed . loc [ common_genes ] # 5. Quality checks print ( \"Running quality checks...\" ) assert len ( common_genes ) > 100 , \"Insufficient gene overlap\" assert perturbed_tf in expr_aligned . columns , \"Perturbed TF missing\" assert not expr_aligned . isnull () . any () . any (), \"Missing expression data\" assert not binding_aligned . isnull () . any () . any (), \"Missing binding data\" # 6. Save processed data print ( \"Saving processed data...\" ) expr_aligned . index . name = 'gene_id' binding_aligned . index . name = 'gene_id' expr_aligned . to_csv ( f ' { output_prefix } _response.csv' ) binding_aligned . to_csv ( f ' { output_prefix } _predictors.csv' ) print ( f \"\u2713 Data preparation complete!\" ) print ( f \" Response file: { output_prefix } _response.csv\" ) print ( f \" Predictors file: { output_prefix } _predictors.csv\" ) print ( f \" Genes: { len ( common_genes ) } \" ) print ( f \" Expression samples: { len ( expr_aligned . columns ) } \" ) print ( f \" TF predictors: { len ( binding_aligned . columns ) } \" ) # Run preparation prepare_tfbp_data ( raw_expression_file = 'raw_expression.csv' , raw_binding_file = 'raw_binding.csv' , perturbed_tf = 'YPD1' , output_prefix = 'processed' ) Common Issues and Solutions \u00b6 Issue 1: Gene ID Mismatches \u00b6 Problem : Gene IDs don't match between files Solution : Use gene ID mapping: # Load ID mapping id_mapping = pd . read_csv ( 'gene_id_mapping.csv' ) # old_id, new_id mapping_dict = dict ( zip ( id_mapping [ 'old_id' ], id_mapping [ 'new_id' ])) # Apply mapping response_df . index = response_df . index . map ( mapping_dict ) . fillna ( response_df . index ) Issue 2: Missing Values \u00b6 Problem : Missing data in binding matrix Solution : Impute or filter: # Option 1: Remove genes with missing binding data complete_genes = binding_df . dropna () . index response_df = response_df . loc [ complete_genes ] binding_df = binding_df . loc [ complete_genes ] # Option 2: Impute missing values from sklearn.impute import SimpleImputer imputer = SimpleImputer ( strategy = 'median' ) binding_imputed = pd . DataFrame ( imputer . fit_transform ( binding_df ), index = binding_df . index , columns = binding_df . columns ) Issue 3: Scale Differences \u00b6 Problem : Binding values not in 0-1 range Solution : Normalize appropriately: # For ChIP-seq peak heights binding_normalized = binding_df / binding_df . max () # For count data binding_normalized = ( binding_df - binding_df . min ()) / ( binding_df . max () - binding_df . min ()) # For already-processed scores binding_clipped = binding_df . clip ( 0 , 1 ) This comprehensive guide should help you prepare properly formatted input data for tfbpmodeling analysis.","title":"Input Data Formats"},{"location":"tutorials/input-formats/#input-data-formats","text":"This guide provides detailed specifications for preparing input data for tfbpmodeling analysis.","title":"Input Data Formats"},{"location":"tutorials/input-formats/#overview","text":"tfbpmodeling requires two main input files plus optional supplementary files: Response File : Gene expression data (dependent variable) Predictors File : Transcription factor binding data (independent variables) Blacklist File (optional): Features to exclude from analysis","title":"Overview"},{"location":"tutorials/input-formats/#response-file-format","text":"","title":"Response File Format"},{"location":"tutorials/input-formats/#structure","text":"The response file contains gene expression measurements: gene_id,sample_1,sample_2,sample_3,sample_4,YPD1 YBR001C,0.234,-1.456,0.876,-0.123,0.543 YBR002W,-0.456,0.123,1.234,-0.567,-0.234 YBR003W,1.234,0.567,-0.234,0.876,0.123 YBR004C,-0.123,-0.876,0.456,0.234,-0.456","title":"Structure"},{"location":"tutorials/input-formats/#requirements","text":"Element Requirement Description Format CSV with comma separators Standard comma-separated values First Column Gene identifiers Must match predictor file exactly Header Row Sample names + perturbed TF Column names for each measurement Data Cells Numeric values Expression measurements (log2 fold-change, z-scores, etc.) Perturbed TF Column Must be present Column name must match --perturbed_tf parameter","title":"Requirements"},{"location":"tutorials/input-formats/#data-types","text":"Supported expression data types : - Log2 fold-change values - Z-scores or standardized values - Raw expression values (will be normalized if needed) - Differential expression statistics Example expression data preparation : import pandas as pd import numpy as np # Load raw expression data raw_expr = pd . read_csv ( 'raw_expression.csv' , index_col = 0 ) # Calculate log2 fold-change vs control control_samples = [ 'ctrl_1' , 'ctrl_2' , 'ctrl_3' ] treatment_samples = [ 'treat_1' , 'treat_2' , 'treat_3' ] control_mean = raw_expr [ control_samples ] . mean ( axis = 1 ) treatment_mean = raw_expr [ treatment_samples ] . mean ( axis = 1 ) log2fc = np . log2 ( treatment_mean + 1 ) - np . log2 ( control_mean + 1 ) # Create response file response_df = pd . DataFrame ({ 'sample_1' : log2fc , 'sample_2' : log2fc + np . random . normal ( 0 , 0.1 , len ( log2fc )), 'YPD1' : log2fc # Perturbed TF response }) response_df . index . name = 'gene_id' response_df . to_csv ( 'response_data.csv' )","title":"Data Types"},{"location":"tutorials/input-formats/#predictors-file-format","text":"","title":"Predictors File Format"},{"location":"tutorials/input-formats/#structure_1","text":"The predictors file contains transcription factor binding data: gene_id,TF_1,TF_2,TF_3,TF_4,YPD1_binding YBR001C,0.123,0.456,0.789,0.012,0.345 YBR002W,0.234,0.567,0.890,0.123,0.456 YBR003W,0.345,0.678,0.901,0.234,0.567 YBR004C,0.456,0.789,0.012,0.345,0.678","title":"Structure"},{"location":"tutorials/input-formats/#requirements_1","text":"Element Requirement Description Format CSV with comma separators Standard comma-separated values First Column Gene identifiers Must match response file exactly Header Row TF names Transcription factor identifiers Data Cells Numeric values 0-1 Binding probabilities or normalized scores No Missing Values Complete data required All cells must contain numeric values","title":"Requirements"},{"location":"tutorials/input-formats/#data-types_1","text":"Supported binding data types : - ChIP-seq binding probabilities (0-1) - Normalized binding scores (0-1) - Peak presence indicators (0/1) - Binding strength quantiles (0-1) Example binding data preparation : import pandas as pd import numpy as np from sklearn.preprocessing import MinMaxScaler # Load ChIP-seq peak scores chip_data = pd . read_csv ( 'chip_peaks.csv' , index_col = 0 ) # Normalize to 0-1 range scaler = MinMaxScaler () normalized_binding = pd . DataFrame ( scaler . fit_transform ( chip_data ), index = chip_data . index , columns = chip_data . columns ) # Save as predictors file normalized_binding . index . name = 'gene_id' normalized_binding . to_csv ( 'binding_data.csv' )","title":"Data Types"},{"location":"tutorials/input-formats/#gene-identifier-consistency","text":"","title":"Gene Identifier Consistency"},{"location":"tutorials/input-formats/#critical-requirements","text":"Both files must use identical gene identifiers: # Verify gene ID consistency response_df = pd . read_csv ( 'response.csv' , index_col = 0 ) predictor_df = pd . read_csv ( 'predictors.csv' , index_col = 0 ) # Check for exact matches common_genes = set ( response_df . index ) & set ( predictor_df . index ) response_only = set ( response_df . index ) - set ( predictor_df . index ) predictor_only = set ( predictor_df . index ) - set ( response_df . index ) print ( f \"Common genes: { len ( common_genes ) } \" ) print ( f \"Response only: { len ( response_only ) } \" ) print ( f \"Predictor only: { len ( predictor_only ) } \" )","title":"Critical Requirements"},{"location":"tutorials/input-formats/#common-gene-id-formats","text":"Format Example Notes Systematic names YBR001C S. cerevisiae standard Gene symbols CDC42 Human/mouse standard Ensembl IDs ENSG00000123456 Cross-species compatible RefSeq IDs NM_001234567 NCBI standard","title":"Common Gene ID Formats"},{"location":"tutorials/input-formats/#handling-id-mismatches","text":"# Align gene IDs between files def align_gene_ids ( response_df , predictor_df ): # Find common genes common_genes = list ( set ( response_df . index ) & set ( predictor_df . index )) # Subset both dataframes aligned_response = response_df . loc [ common_genes ] aligned_predictor = predictor_df . loc [ common_genes ] return aligned_response , aligned_predictor # Apply alignment aligned_response , aligned_predictor = align_gene_ids ( response_df , predictor_df ) # Save aligned files aligned_response . to_csv ( 'aligned_response.csv' ) aligned_predictor . to_csv ( 'aligned_predictors.csv' )","title":"Handling ID Mismatches"},{"location":"tutorials/input-formats/#blacklist-file-format","text":"","title":"Blacklist File Format"},{"location":"tutorials/input-formats/#structure_2","text":"Simple text file with one gene identifier per line: YBR999W YCR888X ribosomal_protein_L1 housekeeping_gene_1 batch_effect_gene","title":"Structure"},{"location":"tutorials/input-formats/#use-cases","text":"Common genes to blacklist : - Housekeeping genes with stable expression - Ribosomal protein genes - Mitochondrial genes - Known batch effect genes - Genes with technical artifacts","title":"Use Cases"},{"location":"tutorials/input-formats/#creating-blacklist-files","text":"# Identify housekeeping genes housekeeping_genes = [ 'ACT1' , 'TUB1' , 'TUB2' , 'RDN18-1' , 'RDN25-1' ] # Identify low-variance genes low_variance_genes = response_df . var ( axis = 1 ) . sort_values () . head ( 50 ) . index . tolist () # Combine blacklists blacklist_genes = list ( set ( housekeeping_genes + low_variance_genes )) # Save blacklist with open ( 'blacklist.txt' , 'w' ) as f : for gene in sorted ( blacklist_genes ): f . write ( f \" { gene } \\n \" )","title":"Creating Blacklist Files"},{"location":"tutorials/input-formats/#data-quality-checks","text":"","title":"Data Quality Checks"},{"location":"tutorials/input-formats/#automated-validation","text":"def validate_input_data ( response_file , predictor_file , perturbed_tf ): \"\"\"Validate input data format and consistency.\"\"\" # Load data response_df = pd . read_csv ( response_file , index_col = 0 ) predictor_df = pd . read_csv ( predictor_file , index_col = 0 ) # Check 1: File formats assert response_df . index . name == 'gene_id' , \"Response file must have 'gene_id' as index\" assert predictor_df . index . name == 'gene_id' , \"Predictor file must have 'gene_id' as index\" # Check 2: Perturbed TF presence assert perturbed_tf in response_df . columns , f \"Perturbed TF ' { perturbed_tf } ' not found in response\" # Check 3: Gene ID overlap common_genes = set ( response_df . index ) & set ( predictor_df . index ) assert len ( common_genes ) > 100 , f \"Too few common genes: { len ( common_genes ) } \" # Check 4: Data types assert response_df . dtypes . apply ( lambda x : np . issubdtype ( x , np . number )) . all () assert predictor_df . dtypes . apply ( lambda x : np . issubdtype ( x , np . number )) . all () # Check 5: Missing values assert not response_df . isnull () . any () . any (), \"Response data contains missing values\" assert not predictor_df . isnull () . any () . any (), \"Predictor data contains missing values\" # Check 6: Value ranges predictor_ranges = predictor_df . describe () if ( predictor_ranges . loc [ 'min' ] < 0 ) . any (): print ( \"Warning: Some predictor values are negative\" ) if ( predictor_ranges . loc [ 'max' ] > 1 ) . any (): print ( \"Warning: Some predictor values exceed 1\" ) print ( \"\u2713 All validation checks passed\" ) # Run validation validate_input_data ( 'response.csv' , 'predictors.csv' , 'YPD1' )","title":"Automated Validation"},{"location":"tutorials/input-formats/#manual-quality-assessment","text":"# Data exploration def explore_data ( response_file , predictor_file ): response_df = pd . read_csv ( response_file , index_col = 0 ) predictor_df = pd . read_csv ( predictor_file , index_col = 0 ) print ( \"=== Response Data ===\" ) print ( f \"Shape: { response_df . shape } \" ) print ( f \"Columns: { list ( response_df . columns ) } \" ) print ( f \"Value range: [ { response_df . min () . min () : .3f } , { response_df . max () . max () : .3f } ]\" ) print ( f \"Missing values: { response_df . isnull () . sum () . sum () } \" ) print ( \" \\n === Predictor Data ===\" ) print ( f \"Shape: { predictor_df . shape } \" ) print ( f \"Columns: { list ( predictor_df . columns [: 5 ]) } ...\" if len ( predictor_df . columns ) > 5 else list ( predictor_df . columns )) print ( f \"Value range: [ { predictor_df . min () . min () : .3f } , { predictor_df . max () . max () : .3f } ]\" ) print ( f \"Missing values: { predictor_df . isnull () . sum () . sum () } \" ) # Plot distributions import matplotlib.pyplot as plt fig , axes = plt . subplots ( 1 , 2 , figsize = ( 12 , 4 )) # Response distribution response_df . iloc [:, 0 ] . hist ( bins = 50 , ax = axes [ 0 ]) axes [ 0 ] . set_title ( 'Response Data Distribution' ) axes [ 0 ] . set_xlabel ( 'Expression Values' ) # Predictor distribution predictor_df . iloc [:, 0 ] . hist ( bins = 50 , ax = axes [ 1 ]) axes [ 1 ] . set_title ( 'Predictor Data Distribution' ) axes [ 1 ] . set_xlabel ( 'Binding Values' ) plt . tight_layout () plt . savefig ( 'data_distributions.png' ) plt . show () # Explore your data explore_data ( 'response.csv' , 'predictors.csv' )","title":"Manual Quality Assessment"},{"location":"tutorials/input-formats/#example-data-preparation-workflow","text":"","title":"Example Data Preparation Workflow"},{"location":"tutorials/input-formats/#complete-pipeline","text":"import pandas as pd import numpy as np from sklearn.preprocessing import StandardScaler , MinMaxScaler def prepare_tfbp_data ( raw_expression_file , raw_binding_file , perturbed_tf , output_prefix ): \"\"\"Complete data preparation pipeline.\"\"\" # 1. Load raw data print ( \"Loading raw data...\" ) expr_raw = pd . read_csv ( raw_expression_file , index_col = 0 ) binding_raw = pd . read_csv ( raw_binding_file , index_col = 0 ) # 2. Process expression data print ( \"Processing expression data...\" ) # Log-transform if needed if expr_raw . min () . min () >= 0 : expr_processed = np . log2 ( expr_raw + 1 ) else : expr_processed = expr_raw # Add perturbed TF column (example: mean of treatment samples) if perturbed_tf not in expr_processed . columns : # Calculate as mean expression change expr_processed [ perturbed_tf ] = expr_processed . mean ( axis = 1 ) # 3. Process binding data print ( \"Processing binding data...\" ) # Normalize to 0-1 range scaler = MinMaxScaler () binding_processed = pd . DataFrame ( scaler . fit_transform ( binding_raw ), index = binding_raw . index , columns = binding_raw . columns ) # 4. Align gene IDs print ( \"Aligning gene identifiers...\" ) common_genes = list ( set ( expr_processed . index ) & set ( binding_processed . index )) print ( f \"Common genes: { len ( common_genes ) } \" ) expr_aligned = expr_processed . loc [ common_genes ] binding_aligned = binding_processed . loc [ common_genes ] # 5. Quality checks print ( \"Running quality checks...\" ) assert len ( common_genes ) > 100 , \"Insufficient gene overlap\" assert perturbed_tf in expr_aligned . columns , \"Perturbed TF missing\" assert not expr_aligned . isnull () . any () . any (), \"Missing expression data\" assert not binding_aligned . isnull () . any () . any (), \"Missing binding data\" # 6. Save processed data print ( \"Saving processed data...\" ) expr_aligned . index . name = 'gene_id' binding_aligned . index . name = 'gene_id' expr_aligned . to_csv ( f ' { output_prefix } _response.csv' ) binding_aligned . to_csv ( f ' { output_prefix } _predictors.csv' ) print ( f \"\u2713 Data preparation complete!\" ) print ( f \" Response file: { output_prefix } _response.csv\" ) print ( f \" Predictors file: { output_prefix } _predictors.csv\" ) print ( f \" Genes: { len ( common_genes ) } \" ) print ( f \" Expression samples: { len ( expr_aligned . columns ) } \" ) print ( f \" TF predictors: { len ( binding_aligned . columns ) } \" ) # Run preparation prepare_tfbp_data ( raw_expression_file = 'raw_expression.csv' , raw_binding_file = 'raw_binding.csv' , perturbed_tf = 'YPD1' , output_prefix = 'processed' )","title":"Complete Pipeline"},{"location":"tutorials/input-formats/#common-issues-and-solutions","text":"","title":"Common Issues and Solutions"},{"location":"tutorials/input-formats/#issue-1-gene-id-mismatches","text":"Problem : Gene IDs don't match between files Solution : Use gene ID mapping: # Load ID mapping id_mapping = pd . read_csv ( 'gene_id_mapping.csv' ) # old_id, new_id mapping_dict = dict ( zip ( id_mapping [ 'old_id' ], id_mapping [ 'new_id' ])) # Apply mapping response_df . index = response_df . index . map ( mapping_dict ) . fillna ( response_df . index )","title":"Issue 1: Gene ID Mismatches"},{"location":"tutorials/input-formats/#issue-2-missing-values","text":"Problem : Missing data in binding matrix Solution : Impute or filter: # Option 1: Remove genes with missing binding data complete_genes = binding_df . dropna () . index response_df = response_df . loc [ complete_genes ] binding_df = binding_df . loc [ complete_genes ] # Option 2: Impute missing values from sklearn.impute import SimpleImputer imputer = SimpleImputer ( strategy = 'median' ) binding_imputed = pd . DataFrame ( imputer . fit_transform ( binding_df ), index = binding_df . index , columns = binding_df . columns )","title":"Issue 2: Missing Values"},{"location":"tutorials/input-formats/#issue-3-scale-differences","text":"Problem : Binding values not in 0-1 range Solution : Normalize appropriately: # For ChIP-seq peak heights binding_normalized = binding_df / binding_df . max () # For count data binding_normalized = ( binding_df - binding_df . min ()) / ( binding_df . max () - binding_df . min ()) # For already-processed scores binding_clipped = binding_df . clip ( 0 , 1 ) This comprehensive guide should help you prepare properly formatted input data for tfbpmodeling analysis.","title":"Issue 3: Scale Differences"}]}